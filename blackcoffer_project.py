# -*- coding: utf-8 -*-
"""Blackcoffer_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16y-tSa5lLVWKFhhttwasPm3Tgad9oLom
"""

import requests
from bs4 import BeautifulSoup
import pandas as pd
import csv
import os
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
import nltk
import re
import string
nltk.download('punkt')  # Download the necessary tokenizer resource

from google.colab import drive
drive.mount('/content/drive')

# Read the Excel file
df = pd.read_excel('/content/drive/MyDrive/Blackcoffer/Input.xlsx')

# Specify the output directory in your Google Drive
output_dir = '/content/drive/MyDrive/Blackcoffer'

# Iterate through each row
for index, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    # Fetch the webpage content
    response = requests.get(url)
    response.raise_for_status()

    # Parse the HTML content
    soup = BeautifulSoup(response.content, 'html.parser')

    # Extract the article title
    title_tag = soup.find('h1', class_='entry-title')
    if title_tag:
        title = title_tag.text.strip()
    else:
        title = "Title not found"

    # Find the main content area
    main_content = soup.find('div', class_='td-post-content')

    # Remove unwanted elements within the main content
    for unwanted_tag in main_content.find_all(['header', 'footer', 'nav', 'aside', 'div', 'figure']):
        unwanted_tag.decompose()

    # Specifically target paragraphs within the main content
    paragraphs = main_content.find_all('p')

    # Filter out paragraphs that might contain extraneous information
    filtered_paragraphs = [p for p in paragraphs if not ('blackcoffer' in p.text.lower() or 'summarized' in p.text.lower())]

    # Extract the article text from the filtered paragraphs
    article_content = ""
    for p_tag in filtered_paragraphs:
        article_content += p_tag.text.strip() + "\n"

    # Construct the full file path for saving
    file_path = os.path.join(output_dir, f"{url_id}.txt")

    # Save the extracted text to the specified file path
    with open(file_path, "w", encoding="utf-8") as f:
        f.write(title + "\n\n")
        f.write(article_content)

    print(f"Extracted and saved article for {url_id} to {file_path}")

# Specify the directory where the text files are saved
data_dir = '/content/drive/MyDrive/Blackcoffer'

# Get a list of all the text files in the directory
text_files = [f for f in os.listdir(data_dir) if f.endswith('.txt')]

# Iterate through each text file and read its content
for file_name in text_files:
    file_path = os.path.join(data_dir, file_name)

    with open(file_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # Print the file name and its content
    print(f"File: {file_name}\n")
    print(content)
    print("-" * 50)  # Separator between files

# Read the Excel file, handling potential FileNotFoundError
try:
    df = pd.read_excel('/content/drive/MyDrive/Blackcoffer/Input.xlsx')
except FileNotFoundError:
    print("Error: Input.xlsx file not found. Please check the file path.")
    # You might want to add more error handling here, like exiting the script or prompting the user for the correct path.

# Specify the output directory in your Google Drive
output_dir = '/content/drive/MyDrive/Blackcoffer'

# Create a list to store the extracted data
extracted_data = []

# Iterate through each row
for index, row in df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']

    # Fetch the webpage content
    try:
        response = requests.get(url)
        response.raise_for_status()  # Raise an exception for bad status codes (4xx and 5xx)
    except requests.exceptions.RequestException as e:
        print(f"Error fetching {url}: {e}")
        continue  # Skip to the next URL if there's an error

    # Parse the HTML content
    soup = BeautifulSoup(response.content, 'html.parser')

    # Extract the article title
    title_tag = soup.find('h1', class_='entry-title')
    if title_tag:
        title = title_tag.text.strip()
    else:
        title = "Title not found"

    # Find the main content area
    main_content = soup.find('div', class_='td-post-content')

    # Remove unwanted elements within the main content
    for unwanted_tag in main_content.find_all(['header', 'footer', 'nav', 'aside', 'div', 'figure']):
        unwanted_tag.decompose()

    # Specifically target paragraphs within the main content
    paragraphs = main_content.find_all('p')

    # Filter out paragraphs that might contain extraneous information
    filtered_paragraphs = [p for p in paragraphs if not ('blackcoffer' in p.text.lower() or 'summarized' in p.text.lower())]

    # Extract the article text from the filtered paragraphs, joining with spaces
    article_content = " ".join([p.text.strip() for p in filtered_paragraphs])

    # Append the extracted data to the list
    extracted_data.append({'URL_ID': url_id, 'Title': title, 'Article_text': article_content})

# Create a DataFrame from the extracted data
output_df = pd.DataFrame(extracted_data)

# Construct the full file path for saving with the new filename
file_path = os.path.join(output_dir, "extracted_data.csv")

# Save the DataFrame to a CSV file
output_df.to_csv(file_path, index=False)

print(f"Extracted data saved to {file_path}")

df=pd.read_csv('/content/drive/MyDrive/Blackcoffer/extracted_data.csv')

df.head()

df.info()

df.shape

# Load the dataset
df = pd.read_csv('/content/drive/MyDrive/Blackcoffer/extracted_data.csv')

# Define paths to StopWords and MasterDictionary folders
stopwords_folder_path = '/content/drive/MyDrive/Blackcoffer/StopWords'
master_dictionary_folder_path = '/content/drive/MyDrive/Blackcoffer/Master dictionary'

# Create sets to store stop words, positive words, and negative words
all_stop_words = set()
positive_words = set()
negative_words = set()

# Read stop words from all files in the StopWords folder
for filename in os.listdir(stopwords_folder_path):
    if filename.endswith('.txt'):
        with open(os.path.join(stopwords_folder_path, filename), 'r', encoding='latin-1') as f:
            all_stop_words.update(f.read().splitlines())

# Read positive and negative words from files in the MasterDictionary folder
for filename in os.listdir(master_dictionary_folder_path):
    if filename.endswith('.txt'):
        with open(os.path.join(master_dictionary_folder_path, filename), 'r', encoding='latin-1') as f:
            words = f.read().splitlines()
            if 'positive' in filename.lower():
                positive_words.update(words)
            elif 'negative' in filename.lower():
                negative_words.update(words)

# Remove stop words from positive and negative word sets
positive_words = positive_words - all_stop_words
negative_words = negative_words - all_stop_words

# Create the final sentiment dictionary
sentiment_dict = {
    'positive': list(positive_words),
    'negative': list(negative_words)
}

# Assuming the text column to be cleaned is named 'text'
def clean_text(text):
    words = text.lower().split()
    cleaned_words = [word for word in words if word not in all_stop_words]
    return ' '.join(cleaned_words)

# Apply the cleaning function to the text column
df['Article_text_cleaned'] = df['Article_text'].astype(str).apply(clean_text)

# Function to calculate the scores
def calculate_scores(text):
    tokens = word_tokenize(text)
    positive_score = sum([1 for word in tokens if word in sentiment_dict['positive']])
    negative_score = sum([1 for word in tokens if word in sentiment_dict['negative']])

    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)
    subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)

    return positive_score, negative_score, polarity_score, subjectivity_score

# Apply the calculation function to the cleaned text column
df[['POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE']] = df['Article_text_cleaned'].apply(calculate_scores).apply(pd.Series)

# Print the first few rows to get a glimpse of the results
print(df[['URL_ID', 'Article_text_cleaned', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE']].head())

# Function to calculate average sentence length using regex
def calculate_avg_sentence_length(text):
  sentences = re.split(r'[.?!]+ ', text)
  num_words = len(text.split())

  # Handle cases where there are no real sentences or only one sentence
  if len(sentences) <= 1:
    return num_words  # If there's only one sentence, the avg sentence length is the total number of words

  avg_sentence_length = num_words / (len(sentences) - 1)
  return avg_sentence_length

# Apply the function to the 'Article_text' column
df['AVG SENTENCE LENGTH'] = df['Article_text'].astype(str).apply(calculate_avg_sentence_length)

# Print the first 5 rows with the required columns
print(df[['URL_ID', 'Article_text', 'AVG SENTENCE LENGTH']].head().to_markdown(index=False, numalign="left", stralign="left"))

# Function to count complex words (words with more than two syllables)
def count_complex_words(text):
    # Define a helper function to count syllables in a word (approximate)
    def count_syllables(word):
        vowel_groups = re.findall(r'[aeiouy]+', word.lower())
        return len(vowel_groups)

    # Count words with more than two syllables
    words = text.split()
    complex_words = [word for word in words if count_syllables(word) > 2]
    return len(complex_words)

# Apply the function to calculate the number of complex words for each row
df['Complex Word Count'] = df['Article_text'].astype(str).apply(count_complex_words)

# Calculate the total number of words in each row
df['Total Word Count'] = df['Article_text'].astype(str).apply(lambda x: len(x.split()))

# Calculate the percentage of complex words
df['PERCENTAGE OF COMPLEX WORDS'] = (df['Complex Word Count'] / df['Total Word Count']) * 100

# Print the first 5 rows with relevant columns
print(df[['URL_ID', 'Article_text', 'PERCENTAGE OF COMPLEX WORDS']].head().to_markdown(index=False, numalign="left", stralign="left"))

# Calculate Fog Index
df['FOG INDEX'] = 0.4 * (df['AVG SENTENCE LENGTH'] + df['PERCENTAGE OF COMPLEX WORDS'])

# Print the first 5 rows with relevant columns (including Fog Index)
print(df[['URL_ID', 'Article_text', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX']].head().to_markdown(index=False, numalign="left", stralign="left"))

# Function to count sentences using regex (you already have this)

def count_sentences(text):
    # Improved regex to handle more cases
    sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', text)

    # Filter out empty sentences
    sentences = [sentence for sentence in sentences if sentence.strip()]

    return len(sentences)
# Apply the function to create the 'total_sentences' column
df['total_sentences'] = df['Article_text'].astype(str).apply(count_sentences)

# Calculate the total number of words in each article
df['the total number of words'] = df['Article_text'].astype(str).apply(lambda x: len(x.split()))

# Calculate the average number of words per sentence
df['AVG NUMBER OF WORDS PER SENTENCE'] = df['the total number of words'] / df['total_sentences']

# Print the first 5 rows with the required columns
print(df[['URL_ID', 'Article_text', 'total_sentences', 'the total number of words', 'AVG NUMBER OF WORDS PER SENTENCE']].head().to_markdown(index=False, numalign="left", stralign="left"))

#Checking once
text = df['Article_text'].iloc[0]  # Get the first row's text
sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?)\s', text)

print("Original Text:")
print(text)
print("\nSplit Sentences:")
for i, sentence in enumerate(sentences):
    print(f"{i+1}. {sentence}")

# Function to count complex words (words with more than two syllables)
def count_complex_words(text):
    # Define a helper function to count syllables in a word (approximate)
    def count_syllables(word):
        vowel_groups = re.findall(r'[aeiouy]+', word.lower())
        return len(vowel_groups)

    # Count words with more than two syllables
    words = text.split()
    complex_words = [word for word in words if count_syllables(word) > 2]
    return len(complex_words)

# Apply the function to calculate the number of complex words for each row
df['COMPLEX WORD COUNT'] = df['Article_text'].astype(str).apply(count_complex_words)
print(df[['URL_ID', 'Article_text', 'COMPLEX WORD COUNT']].head().to_markdown(index=False, numalign="left", stralign="left"))

# Initialize NLTK stopwords
nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

# Function to clean and count words
def clean_and_count_words(text):
    # Remove punctuation
    translator = str.maketrans('', '', string.punctuation)
    cleaned_text = text.translate(translator)

    # Tokenize into words
    words = cleaned_text.split()

    # Remove stopwords
    filtered_words = [word for word in words if word.lower() not in stop_words]

    return len(filtered_words)

# Apply the function to create the 'Word Count' column
df['WORD COUNT'] = df['Article_text'].apply(clean_and_count_words)

# Print the resulting DataFrame
print(df)

# Function to count syllables in a word
def count_syllables(word):
    # Remove any trailing 'es' or 'ed'
    word = re.sub(r'(es|ed)$', '', word.lower())

    # Count the vowels (assuming a simple rule: each vowel is a syllable)
    return sum(1 for char in word if char in 'aeiouy')

# Apply the function to create the 'SYLLABLE PER WORD' column
df['SYLLABLE PER WORD'] = df['Article_text'].apply(lambda Article_text: sum(count_syllables(word) for word in Article_text.split()))

# Print the resulting DataFrame
print(df)

# Define the personal pronouns
personal_pronouns = ["I", "we", "my", "ours", "us"]

# Function to count personal pronouns
def count_personal_pronouns(text):
    # Convert text to lowercase for case-insensitive matching
    text_lower = text.lower()

    # Initialize count
    count = 0

    # Search for each personal pronoun using regex
    for pronoun in personal_pronouns:
        # Exclude "us" if it's part of the word "US"
        if pronoun != "us":
            count += len(re.findall(r'\b' + pronoun + r'\b', text_lower))

    return count

# Apply the function to create the 'Personal Pronouns' column
df['PERSONAL PRONOUNS'] = df['Article_text'].apply(count_personal_pronouns)

# Print the resulting DataFrame
print(df)

# Function to calculate average word length
def calculate_avg_word_length(text):
    # Tokenize into words
    words = text.split()

    # Calculate the total number of characters in all words
    total_char_count = sum(len(word) for word in words)

    # Calculate the total number of words
    total_word_count = len(words)

    # Calculate the average word length
    avg_word_length = total_char_count / total_word_count

    return avg_word_length

# Apply the function to create the 'AVG WORD LENGTH' column
df['AVG WORD LENGTH'] = df['Article_text'].apply(calculate_avg_word_length)

# Print the resulting DataFrame
print(df)

df.head()

df.info()

# Load the Excel file
excel_df = pd.read_excel('/content/drive/MyDrive/Blackcoffer/Output Data Structure.xlsx')  # Replace with your actual file path

# Rename the existing 'URL' column in df (if it exists)
if 'URL' in df.columns:
    df = df.rename(columns={'URL': 'Existing_URL'})  # Or any other suitable name

# Merge the DataFrames based on the 'URL_ID' column
df = pd.merge(df, excel_df[['URL_ID', 'URL']], on='URL_ID', how='left')

# Print the updated DataFrame (first few rows)
print(df.head().to_markdown(index=False, numalign="left", stralign="left"))

df.drop(columns=['Title','Article_text','total_sentences','the total number of words'],inplace=True)

df.drop(columns=['Article_text_cleaned','Total Word Count'],inplace=True)

# Define the desired column order
new_column_order = ['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'COMPLEX WORD COUNT', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']

# Reorder the columns
df = df[new_column_order]

# Print the updated DataFrame (first few rows)
print(df.head().to_markdown(index=False, numalign="left", stralign="left"))

df.head()

df.info()

# Specify the output file path
output_file_path = '/content/drive/MyDrive/Blackcoffer/Output Data Structure.csv'

# Save the DataFrame to a CSV file
df.to_csv(output_file_path, index=False)

print(f"DataFrame saved to {output_file_path}")