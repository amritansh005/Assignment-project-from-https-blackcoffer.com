{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RlwCFW9A6N27",
        "outputId": "5545bfe6-57c1-486d-9e95-d2a054be6795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import csv\n",
        "import os\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import re\n",
        "import string\n",
        "nltk.download('punkt')  # Download the necessary tokenizer resource"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEHvTaLP6aAu",
        "outputId": "c362aa1b-80a8-402a-c8e8-15da4a11bbd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Excel file\n",
        "df = pd.read_excel('/content/drive/MyDrive/Blackcoffer/Input.xlsx')\n",
        "\n",
        "# Specify the output directory in your Google Drive\n",
        "output_dir = '/content/drive/MyDrive/Blackcoffer'\n",
        "\n",
        "# Iterate through each row\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    # Fetch the webpage content\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the article title\n",
        "    title_tag = soup.find('h1', class_='entry-title')\n",
        "    if title_tag:\n",
        "        title = title_tag.text.strip()\n",
        "    else:\n",
        "        title = \"Title not found\"\n",
        "\n",
        "    # Find the main content area\n",
        "    main_content = soup.find('div', class_='td-post-content')\n",
        "\n",
        "    # Remove unwanted elements within the main content\n",
        "    for unwanted_tag in main_content.find_all(['header', 'footer', 'nav', 'aside', 'div', 'figure']):\n",
        "        unwanted_tag.decompose()\n",
        "\n",
        "    # Specifically target paragraphs within the main content\n",
        "    paragraphs = main_content.find_all('p')\n",
        "\n",
        "    # Filter out paragraphs that might contain extraneous information\n",
        "    filtered_paragraphs = [p for p in paragraphs if not ('blackcoffer' in p.text.lower() or 'summarized' in p.text.lower())]\n",
        "\n",
        "    # Extract the article text from the filtered paragraphs\n",
        "    article_content = \"\"\n",
        "    for p_tag in filtered_paragraphs:\n",
        "        article_content += p_tag.text.strip() + \"\\n\"\n",
        "\n",
        "    # Construct the full file path for saving\n",
        "    file_path = os.path.join(output_dir, f\"{url_id}.txt\")\n",
        "\n",
        "    # Save the extracted text to the specified file path\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(title + \"\\n\\n\")\n",
        "        f.write(article_content)\n",
        "\n",
        "    print(f\"Extracted and saved article for {url_id} to {file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmE71Rl4LqUV",
        "outputId": "abb86f17-0d62-40bc-b2de-6ab72918eae3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted and saved article for bctech2011 to /content/drive/MyDrive/Blackcoffer/bctech2011.txt\n",
            "Extracted and saved article for bctech2012 to /content/drive/MyDrive/Blackcoffer/bctech2012.txt\n",
            "Extracted and saved article for bctech2013 to /content/drive/MyDrive/Blackcoffer/bctech2013.txt\n",
            "Extracted and saved article for bctech2014 to /content/drive/MyDrive/Blackcoffer/bctech2014.txt\n",
            "Extracted and saved article for bctech2015 to /content/drive/MyDrive/Blackcoffer/bctech2015.txt\n",
            "Extracted and saved article for bctech2016 to /content/drive/MyDrive/Blackcoffer/bctech2016.txt\n",
            "Extracted and saved article for bctech2017 to /content/drive/MyDrive/Blackcoffer/bctech2017.txt\n",
            "Extracted and saved article for bctech2018 to /content/drive/MyDrive/Blackcoffer/bctech2018.txt\n",
            "Extracted and saved article for bctech2019 to /content/drive/MyDrive/Blackcoffer/bctech2019.txt\n",
            "Extracted and saved article for bctech2020 to /content/drive/MyDrive/Blackcoffer/bctech2020.txt\n",
            "Extracted and saved article for bctech2021 to /content/drive/MyDrive/Blackcoffer/bctech2021.txt\n",
            "Extracted and saved article for bctech2022 to /content/drive/MyDrive/Blackcoffer/bctech2022.txt\n",
            "Extracted and saved article for bctech2023 to /content/drive/MyDrive/Blackcoffer/bctech2023.txt\n",
            "Extracted and saved article for bctech2024 to /content/drive/MyDrive/Blackcoffer/bctech2024.txt\n",
            "Extracted and saved article for bctech2025 to /content/drive/MyDrive/Blackcoffer/bctech2025.txt\n",
            "Extracted and saved article for bctech2026 to /content/drive/MyDrive/Blackcoffer/bctech2026.txt\n",
            "Extracted and saved article for bctech2027 to /content/drive/MyDrive/Blackcoffer/bctech2027.txt\n",
            "Extracted and saved article for bctech2028 to /content/drive/MyDrive/Blackcoffer/bctech2028.txt\n",
            "Extracted and saved article for bctech2029 to /content/drive/MyDrive/Blackcoffer/bctech2029.txt\n",
            "Extracted and saved article for bctech2030 to /content/drive/MyDrive/Blackcoffer/bctech2030.txt\n",
            "Extracted and saved article for bctech2031 to /content/drive/MyDrive/Blackcoffer/bctech2031.txt\n",
            "Extracted and saved article for bctech2032 to /content/drive/MyDrive/Blackcoffer/bctech2032.txt\n",
            "Extracted and saved article for bctech2033 to /content/drive/MyDrive/Blackcoffer/bctech2033.txt\n",
            "Extracted and saved article for bctech2034 to /content/drive/MyDrive/Blackcoffer/bctech2034.txt\n",
            "Extracted and saved article for bctech2035 to /content/drive/MyDrive/Blackcoffer/bctech2035.txt\n",
            "Extracted and saved article for bctech2036 to /content/drive/MyDrive/Blackcoffer/bctech2036.txt\n",
            "Extracted and saved article for bctech2037 to /content/drive/MyDrive/Blackcoffer/bctech2037.txt\n",
            "Extracted and saved article for bctech2038 to /content/drive/MyDrive/Blackcoffer/bctech2038.txt\n",
            "Extracted and saved article for bctech2039 to /content/drive/MyDrive/Blackcoffer/bctech2039.txt\n",
            "Extracted and saved article for bctech2040 to /content/drive/MyDrive/Blackcoffer/bctech2040.txt\n",
            "Extracted and saved article for bctech2041 to /content/drive/MyDrive/Blackcoffer/bctech2041.txt\n",
            "Extracted and saved article for bctech2042 to /content/drive/MyDrive/Blackcoffer/bctech2042.txt\n",
            "Extracted and saved article for bctech2043 to /content/drive/MyDrive/Blackcoffer/bctech2043.txt\n",
            "Extracted and saved article for bctech2044 to /content/drive/MyDrive/Blackcoffer/bctech2044.txt\n",
            "Extracted and saved article for bctech2045 to /content/drive/MyDrive/Blackcoffer/bctech2045.txt\n",
            "Extracted and saved article for bctech2046 to /content/drive/MyDrive/Blackcoffer/bctech2046.txt\n",
            "Extracted and saved article for bctech2047 to /content/drive/MyDrive/Blackcoffer/bctech2047.txt\n",
            "Extracted and saved article for bctech2048 to /content/drive/MyDrive/Blackcoffer/bctech2048.txt\n",
            "Extracted and saved article for bctech2049 to /content/drive/MyDrive/Blackcoffer/bctech2049.txt\n",
            "Extracted and saved article for bctech2050 to /content/drive/MyDrive/Blackcoffer/bctech2050.txt\n",
            "Extracted and saved article for bctech2051 to /content/drive/MyDrive/Blackcoffer/bctech2051.txt\n",
            "Extracted and saved article for bctech2052 to /content/drive/MyDrive/Blackcoffer/bctech2052.txt\n",
            "Extracted and saved article for bctech2053 to /content/drive/MyDrive/Blackcoffer/bctech2053.txt\n",
            "Extracted and saved article for bctech2054 to /content/drive/MyDrive/Blackcoffer/bctech2054.txt\n",
            "Extracted and saved article for bctech2055 to /content/drive/MyDrive/Blackcoffer/bctech2055.txt\n",
            "Extracted and saved article for bctech2056 to /content/drive/MyDrive/Blackcoffer/bctech2056.txt\n",
            "Extracted and saved article for bctech2057 to /content/drive/MyDrive/Blackcoffer/bctech2057.txt\n",
            "Extracted and saved article for bctech2058 to /content/drive/MyDrive/Blackcoffer/bctech2058.txt\n",
            "Extracted and saved article for bctech2059 to /content/drive/MyDrive/Blackcoffer/bctech2059.txt\n",
            "Extracted and saved article for bctech2060 to /content/drive/MyDrive/Blackcoffer/bctech2060.txt\n",
            "Extracted and saved article for bctech2061 to /content/drive/MyDrive/Blackcoffer/bctech2061.txt\n",
            "Extracted and saved article for bctech2062 to /content/drive/MyDrive/Blackcoffer/bctech2062.txt\n",
            "Extracted and saved article for bctech2063 to /content/drive/MyDrive/Blackcoffer/bctech2063.txt\n",
            "Extracted and saved article for bctech2064 to /content/drive/MyDrive/Blackcoffer/bctech2064.txt\n",
            "Extracted and saved article for bctech2065 to /content/drive/MyDrive/Blackcoffer/bctech2065.txt\n",
            "Extracted and saved article for bctech2066 to /content/drive/MyDrive/Blackcoffer/bctech2066.txt\n",
            "Extracted and saved article for bctech2067 to /content/drive/MyDrive/Blackcoffer/bctech2067.txt\n",
            "Extracted and saved article for bctech2068 to /content/drive/MyDrive/Blackcoffer/bctech2068.txt\n",
            "Extracted and saved article for bctech2069 to /content/drive/MyDrive/Blackcoffer/bctech2069.txt\n",
            "Extracted and saved article for bctech2070 to /content/drive/MyDrive/Blackcoffer/bctech2070.txt\n",
            "Extracted and saved article for bctech2071 to /content/drive/MyDrive/Blackcoffer/bctech2071.txt\n",
            "Extracted and saved article for bctech2072 to /content/drive/MyDrive/Blackcoffer/bctech2072.txt\n",
            "Extracted and saved article for bctech2073 to /content/drive/MyDrive/Blackcoffer/bctech2073.txt\n",
            "Extracted and saved article for bctech2074 to /content/drive/MyDrive/Blackcoffer/bctech2074.txt\n",
            "Extracted and saved article for bctech2075 to /content/drive/MyDrive/Blackcoffer/bctech2075.txt\n",
            "Extracted and saved article for bctech2076 to /content/drive/MyDrive/Blackcoffer/bctech2076.txt\n",
            "Extracted and saved article for bctech2077 to /content/drive/MyDrive/Blackcoffer/bctech2077.txt\n",
            "Extracted and saved article for bctech2078 to /content/drive/MyDrive/Blackcoffer/bctech2078.txt\n",
            "Extracted and saved article for bctech2079 to /content/drive/MyDrive/Blackcoffer/bctech2079.txt\n",
            "Extracted and saved article for bctech2080 to /content/drive/MyDrive/Blackcoffer/bctech2080.txt\n",
            "Extracted and saved article for bctech2081 to /content/drive/MyDrive/Blackcoffer/bctech2081.txt\n",
            "Extracted and saved article for bctech2082 to /content/drive/MyDrive/Blackcoffer/bctech2082.txt\n",
            "Extracted and saved article for bctech2083 to /content/drive/MyDrive/Blackcoffer/bctech2083.txt\n",
            "Extracted and saved article for bctech2084 to /content/drive/MyDrive/Blackcoffer/bctech2084.txt\n",
            "Extracted and saved article for bctech2085 to /content/drive/MyDrive/Blackcoffer/bctech2085.txt\n",
            "Extracted and saved article for bctech2086 to /content/drive/MyDrive/Blackcoffer/bctech2086.txt\n",
            "Extracted and saved article for bctech2087 to /content/drive/MyDrive/Blackcoffer/bctech2087.txt\n",
            "Extracted and saved article for bctech2088 to /content/drive/MyDrive/Blackcoffer/bctech2088.txt\n",
            "Extracted and saved article for bctech2089 to /content/drive/MyDrive/Blackcoffer/bctech2089.txt\n",
            "Extracted and saved article for bctech2090 to /content/drive/MyDrive/Blackcoffer/bctech2090.txt\n",
            "Extracted and saved article for bctech2091 to /content/drive/MyDrive/Blackcoffer/bctech2091.txt\n",
            "Extracted and saved article for bctech2092 to /content/drive/MyDrive/Blackcoffer/bctech2092.txt\n",
            "Extracted and saved article for bctech2093 to /content/drive/MyDrive/Blackcoffer/bctech2093.txt\n",
            "Extracted and saved article for bctech2094 to /content/drive/MyDrive/Blackcoffer/bctech2094.txt\n",
            "Extracted and saved article for bctech2095 to /content/drive/MyDrive/Blackcoffer/bctech2095.txt\n",
            "Extracted and saved article for bctech2096 to /content/drive/MyDrive/Blackcoffer/bctech2096.txt\n",
            "Extracted and saved article for bctech2097 to /content/drive/MyDrive/Blackcoffer/bctech2097.txt\n",
            "Extracted and saved article for bctech2098 to /content/drive/MyDrive/Blackcoffer/bctech2098.txt\n",
            "Extracted and saved article for bctech2099 to /content/drive/MyDrive/Blackcoffer/bctech2099.txt\n",
            "Extracted and saved article for bctech2100 to /content/drive/MyDrive/Blackcoffer/bctech2100.txt\n",
            "Extracted and saved article for bctech2101 to /content/drive/MyDrive/Blackcoffer/bctech2101.txt\n",
            "Extracted and saved article for bctech2102 to /content/drive/MyDrive/Blackcoffer/bctech2102.txt\n",
            "Extracted and saved article for bctech2103 to /content/drive/MyDrive/Blackcoffer/bctech2103.txt\n",
            "Extracted and saved article for bctech2104 to /content/drive/MyDrive/Blackcoffer/bctech2104.txt\n",
            "Extracted and saved article for bctech2105 to /content/drive/MyDrive/Blackcoffer/bctech2105.txt\n",
            "Extracted and saved article for bctech2106 to /content/drive/MyDrive/Blackcoffer/bctech2106.txt\n",
            "Extracted and saved article for bctech2107 to /content/drive/MyDrive/Blackcoffer/bctech2107.txt\n",
            "Extracted and saved article for bctech2108 to /content/drive/MyDrive/Blackcoffer/bctech2108.txt\n",
            "Extracted and saved article for bctech2109 to /content/drive/MyDrive/Blackcoffer/bctech2109.txt\n",
            "Extracted and saved article for bctech2110 to /content/drive/MyDrive/Blackcoffer/bctech2110.txt\n",
            "Extracted and saved article for bctech2111 to /content/drive/MyDrive/Blackcoffer/bctech2111.txt\n",
            "Extracted and saved article for bctech2112 to /content/drive/MyDrive/Blackcoffer/bctech2112.txt\n",
            "Extracted and saved article for bctech2113 to /content/drive/MyDrive/Blackcoffer/bctech2113.txt\n",
            "Extracted and saved article for bctech2114 to /content/drive/MyDrive/Blackcoffer/bctech2114.txt\n",
            "Extracted and saved article for bctech2115 to /content/drive/MyDrive/Blackcoffer/bctech2115.txt\n",
            "Extracted and saved article for bctech2116 to /content/drive/MyDrive/Blackcoffer/bctech2116.txt\n",
            "Extracted and saved article for bctech2117 to /content/drive/MyDrive/Blackcoffer/bctech2117.txt\n",
            "Extracted and saved article for bctech2118 to /content/drive/MyDrive/Blackcoffer/bctech2118.txt\n",
            "Extracted and saved article for bctech2119 to /content/drive/MyDrive/Blackcoffer/bctech2119.txt\n",
            "Extracted and saved article for bctech2120 to /content/drive/MyDrive/Blackcoffer/bctech2120.txt\n",
            "Extracted and saved article for bctech2121 to /content/drive/MyDrive/Blackcoffer/bctech2121.txt\n",
            "Extracted and saved article for bctech2122 to /content/drive/MyDrive/Blackcoffer/bctech2122.txt\n",
            "Extracted and saved article for bctech2123 to /content/drive/MyDrive/Blackcoffer/bctech2123.txt\n",
            "Extracted and saved article for bctech2124 to /content/drive/MyDrive/Blackcoffer/bctech2124.txt\n",
            "Extracted and saved article for bctech2125 to /content/drive/MyDrive/Blackcoffer/bctech2125.txt\n",
            "Extracted and saved article for bctech2126 to /content/drive/MyDrive/Blackcoffer/bctech2126.txt\n",
            "Extracted and saved article for bctech2127 to /content/drive/MyDrive/Blackcoffer/bctech2127.txt\n",
            "Extracted and saved article for bctech2128 to /content/drive/MyDrive/Blackcoffer/bctech2128.txt\n",
            "Extracted and saved article for bctech2129 to /content/drive/MyDrive/Blackcoffer/bctech2129.txt\n",
            "Extracted and saved article for bctech2130 to /content/drive/MyDrive/Blackcoffer/bctech2130.txt\n",
            "Extracted and saved article for bctech2131 to /content/drive/MyDrive/Blackcoffer/bctech2131.txt\n",
            "Extracted and saved article for bctech2132 to /content/drive/MyDrive/Blackcoffer/bctech2132.txt\n",
            "Extracted and saved article for bctech2133 to /content/drive/MyDrive/Blackcoffer/bctech2133.txt\n",
            "Extracted and saved article for bctech2134 to /content/drive/MyDrive/Blackcoffer/bctech2134.txt\n",
            "Extracted and saved article for bctech2135 to /content/drive/MyDrive/Blackcoffer/bctech2135.txt\n",
            "Extracted and saved article for bctech2136 to /content/drive/MyDrive/Blackcoffer/bctech2136.txt\n",
            "Extracted and saved article for bctech2137 to /content/drive/MyDrive/Blackcoffer/bctech2137.txt\n",
            "Extracted and saved article for bctech2138 to /content/drive/MyDrive/Blackcoffer/bctech2138.txt\n",
            "Extracted and saved article for bctech2139 to /content/drive/MyDrive/Blackcoffer/bctech2139.txt\n",
            "Extracted and saved article for bctech2140 to /content/drive/MyDrive/Blackcoffer/bctech2140.txt\n",
            "Extracted and saved article for bctech2141 to /content/drive/MyDrive/Blackcoffer/bctech2141.txt\n",
            "Extracted and saved article for bctech2142 to /content/drive/MyDrive/Blackcoffer/bctech2142.txt\n",
            "Extracted and saved article for bctech2143 to /content/drive/MyDrive/Blackcoffer/bctech2143.txt\n",
            "Extracted and saved article for bctech2144 to /content/drive/MyDrive/Blackcoffer/bctech2144.txt\n",
            "Extracted and saved article for bctech2145 to /content/drive/MyDrive/Blackcoffer/bctech2145.txt\n",
            "Extracted and saved article for bctech2146 to /content/drive/MyDrive/Blackcoffer/bctech2146.txt\n",
            "Extracted and saved article for bctech2147 to /content/drive/MyDrive/Blackcoffer/bctech2147.txt\n",
            "Extracted and saved article for bctech2148 to /content/drive/MyDrive/Blackcoffer/bctech2148.txt\n",
            "Extracted and saved article for bctech2149 to /content/drive/MyDrive/Blackcoffer/bctech2149.txt\n",
            "Extracted and saved article for bctech2150 to /content/drive/MyDrive/Blackcoffer/bctech2150.txt\n",
            "Extracted and saved article for bctech2151 to /content/drive/MyDrive/Blackcoffer/bctech2151.txt\n",
            "Extracted and saved article for bctech2152 to /content/drive/MyDrive/Blackcoffer/bctech2152.txt\n",
            "Extracted and saved article for bctech2153 to /content/drive/MyDrive/Blackcoffer/bctech2153.txt\n",
            "Extracted and saved article for bctech2154 to /content/drive/MyDrive/Blackcoffer/bctech2154.txt\n",
            "Extracted and saved article for bctech2155 to /content/drive/MyDrive/Blackcoffer/bctech2155.txt\n",
            "Extracted and saved article for bctech2156 to /content/drive/MyDrive/Blackcoffer/bctech2156.txt\n",
            "Extracted and saved article for bctech2157 to /content/drive/MyDrive/Blackcoffer/bctech2157.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the directory where the text files are saved\n",
        "data_dir = '/content/drive/MyDrive/Blackcoffer'\n",
        "\n",
        "# Get a list of all the text files in the directory\n",
        "text_files = [f for f in os.listdir(data_dir) if f.endswith('.txt')]\n",
        "\n",
        "# Iterate through each text file and read its content\n",
        "for file_name in text_files:\n",
        "    file_path = os.path.join(data_dir, file_name)\n",
        "\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "\n",
        "    # Print the file name and its content\n",
        "    print(f\"File: {file_name}\\n\")\n",
        "    print(content)\n",
        "    print(\"-\" * 50)  # Separator between files"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCSkkkIQ-Qrt",
        "outputId": "6498e1a3-37d6-4c1d-dad1-d648cc11865f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File: bctech2011.txt\n",
            "\n",
            "ML and AI-based insurance premium model to predict premium to be charged by the insurance company\n",
            "\n",
            "Client: A leading insurance firm worldwide\n",
            "Industry Type: BFSI\n",
            "Products & Services: Insurance\n",
            "Organization Size: 10000+\n",
            "The insurance industry, particularly in the context of providing coverage to Public Company Directors against Insider Trading public lawsuits, faces a significant challenge in accurately determining insurance premiums. Traditional methods of premium calculation may lack precision, and there is a growing need for more sophisticated and data-driven approaches. The integration of Artificial Intelligence (AI) and Machine Learning (ML) models in predicting insurance premiums for this specialized coverage is essential to enhance accuracy, fairness, and responsiveness in adapting to evolving risk factors.\n",
            "The problem at hand involves developing robust AI and ML models that can effectively analyze a multitude of dynamic variables influencing the risk profile of Public Company Directors. These variables include market conditions, regulatory changes, historical legal precedents, financial performance of the insured company, and individual directorial behaviors. The goal is to create a predictive model that not only accurately assesses the risk associated with potential insider trading public lawsuits but also adapts in real-time to new information, ensuring that the insurance premiums charged by the global insurance firm are reflective of the current risk landscape.\n",
            "Key Challenges:\n",
            "Addressing these challenges will not only improve the accuracy of insurance premium predictions but also contribute to the overall efficiency and effectiveness of the insurance services provided to Public Company Directors by the leading global insurance firm.\n",
            "To develop an ML and AI-based insurance premium prediction model for Public Company Directors in the USA, safeguarding them against insider trading public lawsuits, we propose a comprehensive solution leveraging advanced machine learning techniques. The goal is to create a model that accurately assesses the risk associated with individual directors and adapts to dynamic market conditions.\n",
            "By adopting this ML and AI-based approach, the insurance company can enhance its ability to predict insurance premiums accurately, adapt to changing risk landscapes, and provide tailored coverage for Public Company Directors against insider trading public lawsuits in the dynamic environment of the USA.\n",
            "\n",
            "\n",
            "Building an ML and AI-based insurance premium prediction model involves the use of various tools and technologies across different stages of development. Here’s a list of tools and technologies that can be employed for creating such a model for a leading insurance firm in the USA, specifically targeting Public Company Directors against insider trading public lawsuits:\n",
            "It’s important to note that the choice of specific tools may vary based on the preferences of the data science team, the complexity of the model, and the existing technology stack of the insurance company. Additionally, compliance with regulatory requirements and industry standards should be considered in the selection of tools and technologies.\n",
            "The deliverables for an ML and AI-based insurance premium model for Public Company Directors in the USA, aiming to predict premiums for protection against insider trading public lawsuits, would encompass various stages of the development and deployment process. Here is a comprehensive list of deliverables:\n",
            "1.1 Project Proposal:\n",
            "1.2 Requirements Document:\n",
            "2.1 Data Collection Report:\n",
            "2.2 Cleaned and Preprocessed Dataset:\n",
            "3.1 Feature Selection and Engineering Report:\n",
            "4.1 Trained ML Models:\n",
            "4.2 Model Evaluation Report:\n",
            "5.1 Real-Time Integration Component:\n",
            "5.2 Scenario Analysis Module:\n",
            "6.1 Fairness Assessment Report:\n",
            "6.2 Explainability Module:\n",
            "7.1 Deployed API:\n",
            "7.2 User Interface (UI):\n",
            "7.3 Documentation for Integration:\n",
            "8.1 Monitoring Dashboard:\n",
            "8.2 Automated Model Update Pipeline:\n",
            "9.1 Model Architecture Document:\n",
            "9.2 Technical User Manual:\n",
            "10.1 Training Sessions:\n",
            "10.2 Knowledge Transfer Documentation:\n",
            "11.1 Regulatory Compliance Report:\n",
            "11.2 Data Privacy and Security Documentation:\n",
            "12.1 Support and Maintenance Plan:\n",
            "By delivering these items, the insurance firm can ensure a thorough and transparent development process, facilitating successful integration and utilization of the ML and AI-based insurance premium prediction model.\n",
            "The implementation of an ML and AI-based insurance premium model for Public Company Directors in the USA, specifically tailored to protect them from insider trading public lawsuits, can have significant business impacts for the leading insurance firm. Here are several potential business impacts:\n",
            "By recognizing and leveraging these business impacts, the leading insurance firm can derive significant value from the implementation of an ML and AI-based insurance premium model tailored for Public Company Directors in the USA.\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2012.txt\n",
            "\n",
            "Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application\n",
            "\n",
            "Client: A leading fintech firm in the USA\n",
            "Industry Type: Finance\n",
            "Products & Services: Trading, Banking, Financing\n",
            "Organization Size: 100+\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2013.txt\n",
            "\n",
            "Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type: IT\n",
            "Products & Services: IT Consulting\n",
            "Organization Size: 100+\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2014.txt\n",
            "\n",
            "Effective Management of Social Media Data Extraction: Strategies for Authentication, Security, and Reliability\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type: IT\n",
            "Products & Services: Consulting, Product & Services\n",
            "Organization Size: 100+\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2015.txt\n",
            "\n",
            "Streamlined Trading Operations Interface for MetaTrader 4: Empowering Efficient Management and Monitoring\n",
            "\n",
            "Client: A leading fintech firm in the USA\n",
            "Industry Type: Finance\n",
            "Products & Services: Trading, Investment, Financing\n",
            "Organization Size: 100+\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2016.txt\n",
            "\n",
            "Efficient AWS Infrastructure Setup and Management: Addressing Security, Scalability, and Compliance\n",
            "\n",
            "Client: A leading Consulting firm in the USA\n",
            "Industry Type: IT\n",
            "Products & Services: IT Consulting\n",
            "Organization Size: 1000+\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2017.txt\n",
            "\n",
            "Streamlined Equity Waterfall Calculation and Deal Management System\n",
            "\n",
            "Client: A leading real estate firm in the USA\n",
            "Industry Type: Real Estate\n",
            "Products & Services: Real Estate, Construction, Financing\n",
            "Organization Size: 100+\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2018.txt\n",
            "\n",
            "Automated Orthopedic Case Report Generation: Harnessing Web Scraping and AI Integration\n",
            "\n",
            "Client: A leading health-tech firm in the USA\n",
            "Industry Type: Healthcare\n",
            "Products & Services: Medical solutions, healthcare\n",
            "Organization Size: 100+\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2019.txt\n",
            "\n",
            "Streamlining Time Calculation in Warehouse Management: Leveraging ShipHero API and Google BigQuery Integration\n",
            "\n",
            "Client: A leading retail firm in the USA\n",
            "Industry Type: Retail\n",
            "Products & Services: Retail Solutions, Supply Chain, Warehouse Management\n",
            "Organization Size: 100+\n",
            "[GitHub Repositories URL:\n",
            "http://app.shiphero.com/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2020.txt\n",
            "\n",
            "Efficient Database Design and Management: Streamlining Access and Integration for Partner Entity Management\n",
            "\n",
            "Client: A leading IT firm in the Europe\n",
            "Industry Type: IT\n",
            "Products & Services: IT Services, Consulting and Automation\n",
            "Organization Size: 100+\n",
            "http://34.18.45.30:8000/api/admin/login/?next=/api/admin/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2021.txt\n",
            "\n",
            "Automated Campaign Management System: A Comprehensive Solution with LinkedIn and Email Integration\n",
            "\n",
            "Client: A leading marketing tech firm worldwide\n",
            "Industry Type: Marketing\n",
            "Products & Services: Ad Tech, Marketing Automation, Lead Management\n",
            "Organization Size: 100+\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2022.txt\n",
            "\n",
            "AI-driven data analysis AI tool using Langchain for a leading real estate and financing firm worldwide\n",
            "\n",
            "Client: A leading real estate and financing firm worldwide\n",
            "Industry Type: Real Estate\n",
            "Products & Services: Infrastructure Development, Financing, Real Estate\n",
            "Organization Size: 10000+\n",
            "Creating a user-friendly data analysis tool capable of interpreting natural language queries and providing insightful analyses from CSV data. The tool should facilitate seamless interaction, enabling users to gain valuable insights without the need for technical expertise. Key functionalities should include data exploration, trend identification, pattern recognition, and anomaly detection, all presented in a comprehensible format. The tool must also ensure efficient handling of CSV datasets while maintaining accuracy and reliability in its analyses.\n",
            "CSV data is acquired from a source (local file system, cloud storage, etc.).\n",
            "The data is then converted into a pandas DataFrame using the read_csv() function or similar methods provided by the pandas library.\n",
            "Data Cleaning operations are performed on the dataframe so that it serves as an ideal input for Pandas Agent. These may include:\n",
            "Column Data type conversion.\n",
            "Handling Duplicates\n",
            "Handling unnecessary columns, etc.\n",
            "Langchain’s Pandas Agent is initialized with the necessary parameters. These parameters include:\n",
            "System prompt: A custom prompt provided by the user or defined in the application.\n",
            "Temperature: A parameter controlling the randomness of the model’s outputs.\n",
            "Model: The specific model or model configuration to be used by the agent.\n",
            "Other relevant parameters based on the requirements and capabilities of the agent.\n",
            "The DataFrame created in the previous step serves as input for the Pandas Agent. It contains the structured data which will serve as input for the Pandas Agent.\n",
            "The user interacts with the system by posing queries in natural language.\n",
            "Langchain’s Pandas Agent interprets these queries using GPT-4 backend and converts them into executable commands or operations on the DataFrame.\n",
            "The Pandas Agent executes the operations needed on the DataFrame. These operations may include:\n",
            "Filtering: Selecting rows or columns based on specified criteria.\n",
            "Aggregation: Computing summary statistics or aggregating data based on groups.\n",
            "Transformation: Modifying data in the DataFrame (e.g., adding or removing columns, changing data types).\n",
            "Joining/Merging: Combining multiple DataFrames based on common keys or indices.\n",
            "Sorting: Arranging rows or columns in a specified order.\n",
            "Other pandas DataFrame operations as required by the user queries.\n",
            "The processed output is delivered to the end user through the streamlit user interface.\n",
            "The user can review the insights provided by the system and further refine their queries if needed.\n",
            "Data Analysis Tool with Streamlit frontend.\n",
            "To make the tool follow the Indian standards in terms of Financial Year Quarters, currency and human readable values instead of exponential values.\n",
            "The challenge was solved by decreasing the temperature of Pandas agent to 0 and make a custom system prompt to introduce maximum bias approximating the desirable answers.\n",
            "The user was able get data analysis insights without expertise in python, pandas and other tools used in the process of Data Analysis in a fraction of time compared to what it would have been if the process was done manually.\n",
            "URL: https://app-test-pandas-agent-vjbjfjkmxfrvhkhc455p4k.streamlit.app/ (Non-Functional due to the expiry of OpenAI API Key)\n",
            "Link: https://www.loom.com/share/c2099f20e9214e18a2125f5b2fde794c?sid=faa8cc4b-001c-4c51-926c-6a551dfb7c63\n",
            "\n",
            "Video Demo: https://www.loom.com/share/c2099f20e9214e18a2125f5b2fde794c?sid=faa8cc4b-001c-4c51-926c-6a551dfb7c63\n",
            "URL to test App: https://app-test-pandas-agent-vjbjfjkmxfrvhkhc455p4k.streamlit.app/\n",
            "Project Success Story: https://docs.google.com/document/d/17VZukkZW6LsXVmb6IDIZWpp61sRQY_cE/edit?usp=sharing&ouid=111848530990018600604&rtpof=true&sd=true\n",
            "Solution Diagram: https://drive.google.com/file/d/16T56xrxBHioAIRnoA0EmHlSdMcmzEWP3/view?usp=sharing\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2023.txt\n",
            "\n",
            "Grafana Dashboard to visualize and analyze sensors’ data\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type: IT\n",
            "Products & Services: IT & Consulting, Software Development, DevOps\n",
            "Organization Size: 100+\n",
            "The client requires a Grafana dashboard that can fetch data from a web API providing historical data of building automation systems. The dashboard needs to allow manual entry of a target URL for individual buildings, selection of a history name from a dropdown or search bar, selectable time range for displaying history data, and the ability to choose from various chart types for visualization. Additionally, the client wants to set up alarms for certain metrics like CPU, RAM, and hard disk usage. Each user should only be able to view their own STier API data, which is controlled by their IP.\n",
            "To meet these requirements, we will set up a Grafana dashboard using the Grafana API. We will configure the dashboard to connect to the web API and fetch data based on the user’s input for the target URL, history name, and time range. For visualization, we will implement various chart types including Bar, Line, and Scatter plot charts. To set up alarms for specific metrics, we will utilize Grafana’s built-in alerting feature.\n",
            "\n",
            "The proposed Grafana dashboard will significantly enhance the business’s ability to monitor and manage building automation systems. By providing real-time data visualization and the ability to set alarms for specific metrics, the business can quickly identify and address potential issues, ensuring optimal system performance and efficiency. Furthermore, the user-specific permissions will ensure that sensitive data remains secure and accessible only to authorized individuals. This will not only streamline operations but also boost confidence among staff members who can now make informed decisions based on accurate and timely data. The dashboard’s flexibility in terms of selectable history names and time ranges will allow for comprehensive analysis of historical data, leading to improved decision-making processes. Overall, this solution will contribute to increased operational efficiency, reduced downtime, and improved customer satisfaction by ensuring smooth operation of building automation systems.\n",
            "https://mailhvac.postman.co/workspace/Team-Workspace~902b44a6-966b-4e59-8400-3ae02c12ce6b/collection/17767455-eb2c775e-421d-4f7c-9ec5-b4f6a73f1a5a?action=share&creator=17767455\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2024.txt\n",
            "\n",
            "MVP for a software that analyses content from audio (Pharma-based)\n",
            "\n",
            "Client: A leading pharma-tech firm in the USA\n",
            "Industry Type: Healthcare\n",
            "Products & Services: Pharma Apps\n",
            "Organization Size: 100+\n",
            "The problem lies in creating a backend model for an application that records audio responses from students and uses AI to analyze the content. The backend needs to convert audio to text, transform the text into analytics KPIs, handle login/logout operations, and manage analytics API calls. The application should also calculate the cosine similarity of the student’s response with the expected response.\n",
            "To solve this problem, we will use Python as the primary programming language for backend development. The solution involves several steps:\n",
            "\n",
            "The implementation of this backend model will have significant business impacts:\n",
            "These impacts align with the objectives of the business, making the project a high priority. The business impact analysis will ensure that the project is aligned with the organization’s strategic goals and that potential disruptions are identified and managed effectively\n",
            "\n",
            "Domain and SSL setup is completed :https://www.pharmacyinterns.com.au/\n",
            "Web App is running successfully on  URL – http://34.30.224.139/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2025.txt\n",
            "\n",
            "Data Engineering and Management tool (Airbyte) with custom data connectors to manage CRM database\n",
            "\n",
            "Client: A leading tech firm in Europe\n",
            "Industry Type: IT\n",
            "Products & Services: IT & Consulting, Software Development\n",
            "Organization Size: 1000+\n",
            "Our company requires a robust, scalable, and secure data integration solution that can handle thousands of connections. We need to develop Airbyte connectors for various software applications listed in 2-nx-integration, including Join Portal, ClickUp, Coach Accountable, Hubspot, Quickbooks, Quickbooks Time, and Sales Flow. These connectors should be developed in Python and then wrapped into Docker images. The code should be housed in GitHub and automatically applied to Airbyte for execution using a CI/CD pipeline from GitHub to Airbyte. We also need a full production-ready version of Airbyte hosted on Google Cloud Platform (GCP) Kubernetes, secured via Google Sign In.\n",
            "Moreover, we want to add custom features to Airbyte to control BigQuery projects/datasets. Both Airbyte and BigQuery should be monitored via Sentry, which will also be housed/hosted in the same project for all error reporting/monitoring. We also need to develop transformations to clean and transform the data from the software source to the client’s GCP Project for BigQuery. The code for these transformations should be stored in GitHub.\n",
            "We propose to develop an instance of Airbyte that is production-ready on GCP over Kubernetes. This will be secured using Google Sign On linked to our organization. We will deploy Airbyte using the official documentation 8. To secure the Kubernetes setup, we plan to use Traefik’s ForwardAuth feature.\n",
            "Next, we will code Airbyte Python integrations for our needed software list. We have already gathered the API documentation for each software application and have started coding the integrations. Once the initial integration is complete, we will document the process in ClickUp to guide future integrations.\n",
            "We will use GitHub to host both the source code and Docker images of Airbyte integrations. We will also use Google Cloud’s Sentry for error reporting and monitoring.\n",
            "\n",
            "By developing a robust and scalable data integration solution using Airbyte, we aim to significantly enhance our business operations. This solution will enable us to efficiently manage and analyze data from various software applications, leading to improved decision-making processes.\n",
            "Firstly, the ability to extract and load data from different software applications will allow us to centralize our data management, reducing the complexity of handling multiple data sources. This will streamline our data analysis processes and provide a unified view of our business data.\n",
            "Secondly, the scalability of our solution means that it can handle a growing volume of data as our business grows. This is crucial in today’s digital age where businesses generate vast amounts of data daily.\n",
            "Lastly, by securing our data integration solution with Google Sign In, we can ensure that only authorized individuals can access our sensitive business data. This adds an extra layer of security to our data management practices and helps protect against potential data breaches.\n",
            "Moreover, by using Google Cloud Platform (GCP) for hosting our solution, we can take advantage of its advanced features and robust infrastructure. This will further enhance the reliability and performance of our data integration solution.\n",
            "Overall, implementing this solution will enable us to harness the power of data to drive our business growth and success\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2026.txt\n",
            "\n",
            "Text Summarizing Tool to scrape and summarize pubmed medical papers\n",
            "\n",
            "Client: A leading medical R&D firm in the USA\n",
            "Industry Type: Medical\n",
            "Products & Services: R&D\n",
            "Organization Size: 10000+\n",
            "An advanced AI tool designed specifically for doctors to assist them in retrieving answers to their\n",
            "queries. Powered by state-of-the-art AI technologies, including web scraping and ChatGPT, The AI\n",
            "Assistant aims to streamline information retrieval and provide valuable insights to professionals.\n",
            "This AI Assistant leverages the capabilities of AI to facilitate seamless and efficient access to\n",
            "knowledge and information. It combines web scraping techniques to gather relevant data from\n",
            "trusted sources with ChatGPT and PubMed, providing accurate responses to doctors’ queries.\n",
            "Query Retrieval: AI Assistant utilizes web scraping techniques to fetch information from credible\n",
            "websites, academic journals, medical databases, and other trusted sources. It provides doctors with\n",
            "immediate access to a vast array of knowledge and resources.\n",
            "Benefits:\n",
            "Time Efficiency: By quickly retrieving information and answering queries, AI Assistant saves\n",
            "valuable time for doctors, allowing them to focus more on patient care and critical tasks.\n",
            "Access to Knowledge: AI Assistant grants doctors easy access to a vast repository of knowledge,\n",
            "ensuring they stay updated with the latest research, treatment guidelines, and best practices.\n",
            "Decision Support: The tool provides valuable insights and recommendations, assisting doctors in\n",
            "making informed decisions about diagnosis, treatment plans, and patient management.\n",
            "To address this problem, we will build a web scraping tool that uses Python libraries such as BeautifulSoup, Selenium, and OpenAI’s GPT-3. The program will work as follows:\n",
            "\n",
            "The implementation of our web scraping and summarization tool has had significant positive impacts on our business operations.\n",
            "Firstly, it has streamlined our research process by automating the extraction of crucial information from various online sources. This has saved us considerable time and effort, allowing us to focus on more complex tasks.\n",
            "Secondly, the summarization feature has improved our understanding of the information we collect. By reducing large volumes of text down to a few key points, we’ve been able to quickly grasp the main ideas and insights presented in the articles, videos, and user comments.\n",
            "Thirdly, the tool has enabled us to stay up-to-date with the latest advancements in the field of orthopedics. By pulling data from recent articles on PubMed.gov, we’ve been able to stay informed about the latest research and treatments.\n",
            "Finally, the tool has facilitated the creation of comprehensive case reports. These reports have been instrumental in our ability to present detailed and accurate information to our clients, thereby enhancing our reputation and credibility in the industry.\n",
            "Overall, the implementation of this tool has greatly improved our efficiency and effectiveness, contributing significantly to our business success\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Link: https://www.loom.com/share/535828aad7184c1b82db707dcca8e52c?sid=c79d19b1-b963-45a1-bec5-6228cc753cc2\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2027.txt\n",
            "\n",
            "7up7down, 10upDown, Snakes and Ladder Games built using OOPs\n",
            "\n",
            "Client: A leading game development firm in the USA\n",
            "Industry Type: Gaming Software\n",
            "Products & Services: Gaming Software Development\n",
            "Organization Size: 200+\n",
            "Our client sends records of millions of sports bets in real time from all over the world via an API. These bets are recorded in MySQL servers. We are tasked with processing and calculating the expected Profit and Loss (PNL) as per the bets records for each sport. Our goal is to analyze these records in real time via API and calculate PNL as per the game records history provided via API. This requires building a serverless application in Python (or similar) that reads all bets records and updates PNL in real time (within milliseconds, records need to be updated). The application should be capable of handling 10,000+ records of bets per second for numbers of different games, with PNL needing to be updated for each game separately.\n",
            "To address this problem, we propose developing a Python-based serverless application that leverages machine learning models for real-time PNL calculation. The application will use the MySQL database to store and retrieve betting records. It will employ parallel computing techniques to ensure efficient processing of high volumes of data. The application will also utilize APIs to fetch real-time data and update PNL accordingly.\n",
            "The application will follow these steps:\n",
            "\n",
            "The implementation of the proposed Python-based serverless application for real-time PNL calculation had significant positive impacts on our business operations.\n",
            "Firstly, the application enabled us to process and analyze millions of sports bets in real time, enhancing our decision-making capabilities and allowing for quicker responses to changes in the betting market. This improved our ability to predict outcomes and adjust our betting strategies accordingly.\n",
            "Secondly, the application significantly reduced the time taken to calculate PNL, from hours to mere minutes. This resulted in faster decision-making processes and timely financial reporting, which were crucial for our clients and investors.\n",
            "Lastly, the application’s ability to handle high volumes of data and provide real-time updates facilitated a more globalized betting market. With real-time data and digital platforms, geographical boundaries became less relevant, allowing bettors from around the world to place bets on any event globally, with real-time odds reflecting local nuances and dynamics. This led to increased liquidity and more competitive odds.\n",
            "Overall, the successful implementation of the application led to a more efficient, accurate, and timely PNL calculation process, resulting in improved business performance and customer satisfaction.\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2028.txt\n",
            "\n",
            "Data Studio Dashboard with a data pipeline tool synced with Podio using custom Webhooks and Google Cloud Function\n",
            "\n",
            "Client: A leading retail firm in the USA\n",
            "Industry Type: Retail\n",
            "Products & Services: Retail Business, e-commerce\n",
            "Organization Size: 300+\n",
            "The client needs a consolidated KPI dashboard that aggregates data from various applications and SaaS products. Currently, the data is scattered across different platforms, making it difficult to track key performance indicators (KPIs) effectively. The client wants a dashboard that automatically updates with new data, eliminating the need for manual updates. The dashboard should contain separate tabs for current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances. Additionally, the client wants to use Google Cloud Functions to sync data regularly between the Podio data app and Google Sheets.\n",
            "The proposed solution involves the creation of a KPI dashboard in Google Sheets, which will serve as a central hub for all the client’s data. This dashboard will be populated with data from various sources, including Google Sheets and the Podio data app. The data will be organized into separate tabs, each representing a different aspect of the business. The dashboard will be designed to automatically update with new data, removing the need for manual updates.\n",
            "The process begins with obtaining access to the data in Google Sheets. Once the data is accessed, a list of KPIs to be visualized will be prepared. The data from Google Sheets will then be connected to the Google Data Studio dashboard for visualization. The dashboard will be designed to align with the client’s goals, prioritizing the most important KPIs and positioning them at the top of the dashboard. The dashboard will also be protected to prevent further or accidental changes, ensuring that data can only be added or changed through designated data sheets. Collaborators will be invited via email, with specific roles assigned to ensure effective collaboration. The dashboard will be customized with brand-aligned colors and fonts to enhance its appearance and authority.\n",
            "In addition to the dashboard, webhooks will be created for the Podio data app deployed as a Google Cloud Function. This will enable regular data synchronization between the Podio data app and Google Sheets, ensuring that the dashboard is always up-to-date with the latest data.\n",
            "\n",
            "The implementation of the proposed solution has significantly improved the client’s ability to track and manage key performance indicators (KPIs). Prior to the solution, the client was struggling with data fragmentation across different SaaS products and applications, which made it difficult to compile comprehensive insights. The KPI dashboard, now consolidated in Google Sheets, has streamlined this process, providing a unified view of the business metrics.\n",
            "This solution has also automated the data update process, saving valuable time and resources that were previously spent on manual updates. The automatic update feature has allowed the client to focus on analyzing the data rather than spending hours updating it.\n",
            "Additionally, the integration of the Podio data app with Google Sheets via Google Cloud Functions has improved data synchronization efficiency. Regular data synchronization ensures that the KPI dashboard is always up-to-date, providing real-time insights into the business performance.\n",
            "These improvements have led to enhanced decision-making processes within the client’s organization. With accurate and timely data, managers can now set and achieve goals more effectively. The consolidation of data has also facilitated cross-departmental collaboration, as teams can now access and share data easily.\n",
            "Overall, the solution has resulted in significant business impact, leading to improved operational efficiency, informed decision-making, and strategic planning\n",
            "\n",
            "\n",
            "https://lookerstudio.google.com/u/3/reporting/da134941-6efc-43e4-9b2a-37b7a6aab1b0/page/p_kfrjaxka8c/edit\n",
            "https://console.cloud.google.com/welcome?authuser=1&project=t4a-dashboard\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2029.txt\n",
            "\n",
            "End-to-end tool to optimize routing and planning of field engineers using Google’s CVRP-TW algorithm\n",
            "\n",
            "Client: A leading hardware firm in the USA\n",
            "Industry Type: IT\n",
            "Products & Services: IT Consulting, Support, Hardware Installations\n",
            "Organization Size: 300+\n",
            "The client specializes in installing blinds and related products in customers’ homes. They are currently struggling with scheduling appointments efficiently due to a variety of factors such as location, installation duration, team member availability, and customer preferences. We need a tool that can suggest optimal schedules based on these criteria and adapt to changes as customers approve or reject proposed appointment times. The goal is to create a proof of concept for a route and job planning model that can potentially streamline our scheduling process and make a significant impact on our business operations.\n",
            "The To address this challenge, we propose developing a proof of concept for a route and job planning model. This model will be based on the concept of Constrained Vehicle Routing Problem with Time Windows (CVRP-TW), a well-established approach in operations research and logistics. The model will take a dataset, which could be extracted from a Google sheet or converted from a CSV file, and generate optimal schedules.\n",
            "The development process will involve several stages:\n",
            "In terms of technology, we’ll use Python, a popular language for data analysis and machine learning. We’ll also use the Anaconda distribution, which provides a powerful environment for scientific computing and data analysis.\n",
            "\n",
            "Implementing an efficient route and job planning model had a significant positive impact on our business operations. By automating the scheduling process, we were able to reduce manual errors and streamline our workflow, resulting in quicker response times and deliveries. This not only improved our operational efficiency but also enhanced our ability to provide better service to our customers.\n",
            "Moreover, the model allowed us to maximize each driver’s productivity by optimizing routes, which led to cost savings in fuel and vehicle maintenance. The automated nature of the system also enabled us to make real-time adjustments to the route in response to last-minute orders or unexpected situations, such as a driver being unavailable.\n",
            "The model also provided us with valuable insights into our operations, allowing us to identify bottlenecks and areas for improvement. This helped us to proactively address potential issues and continuously enhance our processes, thereby increasing our overall business performance.\n",
            "As a result of these improvements, we were able to attract more skilled workers by focusing on cutting down unskilled labor. This shift towards more automation allowed us to invest more in our workforce, leading to higher employee satisfaction and retention rates.\n",
            "Lastly, the successful implementation of the route and job planning model has opened up new opportunities for our business. With the ability to efficiently cover our market and manage our resources effectively, we have been able to consider expanding our territory by entering new markets. This strategic route planning has helped us determine whether we need to acquire more vehicles or hire more operators before moving, providing a clear pathway for future growth.\n",
            "\n",
            "https://docs.google.com/spreadsheets/d/1kS7Em9NitvMD_49MoLCpt_KoPJGGIAGjCES_KI8rEQk/edit?userstoinvite=raymondchow%40stanbondsa.com.au#gid=766964619\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2030.txt\n",
            "\n",
            "End-to-end tool to predict Biofuel prices using IESO data\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type: IT\n",
            "Products & Services: IT Consulting, Software Development\n",
            "Organization Size: 100+\n",
            "The task involves creating an end-to-end data pipeline to extract data from various reports, store it in a Google Cloud Platform (GCP) database, build a dashboard, and develop a machine learning model for price forecasting. The data is pulled from different links, each having a slightly different report layout, with some being in CSV and others in XML format. The goal is to extract data daily and hourly for the past three years. The extracted data is intended to be used for building a dashboard and training/testing a model based on user-defined inputs on the dashboard. The challenge lies in handling the varied formats of the data, ensuring accurate extraction, and maintaining the integrity of the data throughout the pipeline.\n",
            "To solve this problem, we will use Python, along with libraries such as pandas and BeautifulSoup, to scrape data from various report links. The scraped data is stored in dataframes and then loaded into Google Cloud Storage buckets. This data is then transferred to BigQuery tables for efficient processing. The data extraction process is automated with a Cronjob/Google Cloud Scheduler.\n",
            "For the machine learning part, we will build and run various machine learning models in GCP’s BigQuery to predict future fuel/energy prices. We will test LSTM univariate/multivariate, GRU for time series problems, and ANN Regressor, Random Forests regression for regression problems. The ANN regression model will provide the best results for our use case.\n",
            "After modeling, we will generate a data visualization report on Google Data Studio for further insights. The report includes a pie chart about the distribution of fuel generated by each fuel type, a stacked column chart about the distribution of fuel generated each month, and a time series visualization of fuel generation during each quarter of the year.\n",
            "\n",
            "The successful implementation of the end-to-end data pipeline project had several significant business impacts.\n",
            "Firstly, it led to improved data quality and accessibility. The project streamlined the process of data extraction from various sources, ensuring that the data was clean, consistent, and readily available for analysis. This resulted in more reliable and accurate predictions, leading to better decision-making and strategic planning.\n",
            "Secondly, the project enhanced operational efficiency. By automating the data extraction process with a Cronjob/Google Cloud Scheduler, the team saved considerable time and effort. This allowed the team to focus on more strategic tasks, thereby increasing productivity.\n",
            "Thirdly, the project facilitated informed decision-making. The dashboard built on Google Data Studio provided users with real-time insights into fuel consumption patterns and energy prices. This helped stakeholders make informed decisions regarding energy usage and pricing strategies.\n",
            "Lastly, the project demonstrated the company’s commitment to leveraging advanced technologies for business growth. The use of Google Cloud Platform, BigQuery, and Google Data Studio showcased the company’s ability to innovate and stay competitive in the rapidly evolving digital landscape.\n",
            "Overall, the project had a positive impact on the company’s operations, decision-making processes, and reputation among stakeholders. It underscored the importance of data-driven decision making and highlighted the potential benefits of investing in advanced technologies.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "https://console.cloud.google.com/compute/instances?authuser=1&project=ieso&pli=1\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2031.txt\n",
            "\n",
            "ETL Discovery Tool using LLMA, Langchain, OpenAI\n",
            "\n",
            "Client: A leading retail firm in the USA\n",
            "Industry Type: Retail\n",
            "Products & Services: Retail Business, e-commerce\n",
            "Organization Size: 100+\n",
            "To develop an ETL discovery tool that can answer the queries related to ETL pipelines in conversational format. The areas of the concerned queries should include Environment Analysis, Workflow Analysis, Data Source and Target Mapping, Transformation Logic, Data Volume and Velocity, Error Handling and Logging and Security and Access Control.\n",
            "In developing our solution, we began by aggregating Open-Source Generic ETL Tool Code from various repositories on GitHub and other relevant sources. Subsequently, we meticulously fine-tuned the collected ETL tool code, organizing and saving it into distinct folders, each containing different ETL pipelines.\n",
            "Following this, we implemented an OpenAI Assistant, integrating it with all the refined ETL pipelines. To facilitate communication with these pipelines, we employed the OpenAI Assistant ID within our Flask API.\n",
            "For the user interface, we opted for a Streamlit front-end, providing a seamless and user-friendly interaction with our OpenAI Assistant and the integrated ETL pipelines.\n",
            "ETL Discovery Tool serves as the core engine for Extract, Transform, and Load (ETL) operations. It is designed to handle data extraction, transformation, and loading tasks efficiently. It will be used for training the OpenAI model on the ETL Discovery tools.\n",
            "Step 1. Open-Source Generic ETL Tool Code:\n",
            "The Open-Source Generic ETL Tool serves as the core engine for Extract, Transform, and Load (ETL) operations. It is designed to handle data extraction, transformation, and loading tasks efficiently. It will be used for training the OpenAI model on the ETL Discovery tools.\n",
            "Step 2. Data Cleaning:\n",
            "Data Cleaning is a critical stage that involves cleansing and pre-processing raw data to enhance its quality and integrity. In this step the ETL understands the expected data format that is organized and cleaned for uniformity of data.\n",
            "Step 3. Files/DB\n",
            "Represents the storage or databases utilized for storing processed data. In this step, solutions for processed data the code files will be arranged and catalogued so that they are ready to be used by the OpenAI Assistants API.\n",
            "Step 4. OpenAI Assistant Creation via API:\n",
            "This step involves creating an OpenAI Assistant using the OpenAI API.\n",
            "Step 5. OpenAI Assistant:\n",
            "In this step, the Assistant that is created from previous step will be queried by the API with instructions for the context accommodation.\n",
            "Step 6. Django/Flask/FastAPI API:\n",
            "This step involves setting up an API using popular frameworks like Django, Flask, or FastAPI.\n",
            "Step 7. Chat Frontend (Streamlit):\n",
            "Represents the user interface for interacting with the system, built using Streamlit.\n",
            "Finding the ETL pipelines and fine tuning the ETL pipelines\n",
            "Our approach to overcoming technical challenges involved an extensive internet search focused on ETL pipelines. We scoured various online resources, eventually identifying the most effective ETL pipelines available on GitHub.\n",
            "To address each challenge systematically, we created individual files for each ETL pipeline. In the process, we meticulously fine-tuned and optimized each pipeline, documenting the specific tasks and functions within the respective files. This approach allowed us to provide detailed descriptions of the work performed for every ETL pipeline, ensuring a comprehensive understanding of the solutions implemented to tackle the technical hurdles encountered.\n",
            "The business impact was substantial as the client efficiently analysed numerous ETL tool pipelines. Instant answers in a chat format replaced the time-consuming manual work that could take Data Engineers days or weeks. This streamlined process significantly enhanced productivity and responsiveness, reflecting a tangible improvement in operational efficiency for the client.\n",
            "Assistant_creator.py\n",
            "Main.py\n",
            "Project Demo Video link:- https://www.loom.com/share/5ee7d0835412474ea4aa3383af5a0814?sid=999739fc-e91a-4cda-a30e-9cd02957205f\n",
            "Part 1 (Backend):- https://www.loom.com/share/338c4e09c90e453e83b86050d469d98b?sid=03299e7a-0699-464e-be2c-689a409ec01e\n",
            "Part 2 (Frontend):- https://www.loom.com/share/8e7942f3a03e49889c6c70fba77f76b0?sid=eca0586f-b767-45fa-854d-853bca1890dc\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2032.txt\n",
            "\n",
            "GPT/OCR API\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type: IT & Consulting\n",
            "Products & Services: IT Solutions, Software Development\n",
            "Organization Size: 100+\n",
            "Design and develop an API as a service backend, the API should be integrated with GPT and OCR technologies to extract documents it should be hosted on Azure\n",
            "All the APIs on the Azure server\n",
            "fastapi, gpt api, pytessaract, pypdf2\n",
            "fastapi, gpt api, pytessaract, pypdf2, python\n",
            "python, Rest API development\n",
            "MS Sql\n",
            "Azure\n",
            "Main challenge in this project extracting text from images and pdfs and generate json output according to template\n",
            "In the apis we can upload .pdf, .docx, .png, .jpg, .jpeg, .txt files. It has basically 2 parts. We can just upload the document or we can also provide template id to process the uploaded document according to the template id.\n",
            "It fetches the template and document from the database and uses the ocr method to extract the text from the document. This extracted text and template are then processed by gpt api which generates the final output..\n",
            "This will help users to directly upload any pdf or image and extract useful information in json format.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2033.txt\n",
            "\n",
            "Dockerize the AWS Lambda for serverless architecture\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type: IT & Consulting\n",
            "Products & Services: IT Solutions, Software Development\n",
            "Organization Size: 100+\n",
            "AWS Lambda, a powerful serverless compute service, faces limitations in terms of runtime customization, dependency management, and execution environment isolation.\n",
            "To overcome the challenges mentioned above, we propose a comprehensive solution that involves Dockerizing AWS Lambda functions for improved flexibility, control, and efficiency within a serverless architecture.\n",
            "Below is a high-level architecture diagram:\n",
            "Key Components:\n",
            "Some of the key deliverables:\n",
            "Dockerfile:\n",
            "A Dockerfile in the root of your Lambda function project, specifying the instructions to build the Docker image. This file includes the base image, installation of dependencies, copying of Lambda function code, and setting the handler function.\n",
            "Docker Image:\n",
            "The Docker image built from the Dockerfile. This image encapsulates your Lambda function code and its dependencies.\n",
            "Pushed Image to ECR:\n",
            "The Docker image pushed to your Amazon Elastic Container Registry (ECR) repository. This involves tagging the image with the ECR repository URI and pushing it to the repository.\n",
            "Updated Lambda Function Configuration:\n",
            "The Lambda function configuration was updated to use the Docker image from ECR. This may involve specifying the ECR URI in the Lambda configuration.\n",
            "Documentation:\n",
            "Documentation outlining the steps to Dockerize the Lambda function and push it to ECR. This documentation should include prerequisites, step-by-step instructions, and any additional considerations.\n",
            "Challenge: AWS Lambda imposes constraints on runtime dependencies, making it challenging to manage and control library versions.\n",
            "Challenge: AWS Lambda’s managed environment may lack certain runtime configurations and isolation.\n",
            "Challenge: Efficiently capturing and analyzing performance metrics and logs from Dockerized Lambda functions.\n",
            "Solution: Use a containerization approach to package dependencies along with the Lambda function, providing better control and isolation. Implement a robust dependency management system within the Docker container.\n",
            "Solution: Docker containers offer enhanced isolation. Utilize containers to encapsulate the Lambda function and its dependencies, ensuring consistent execution environments.\n",
            "Solution: Integrate AWS CloudWatch for basic monitoring.\n",
            "Dockerizing a Lambda Function:\n",
            "https://www.loom.com/share/e90438538dbb43fd884a51dab6c175e9?t=586&sid=b2e4112e-16b9-4d78-a955-77a289453e59\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2034.txt\n",
            "\n",
            "Design and develop a product recommendation engine based on the features of products\n",
            "\n",
            "Client: A leading retail firm in the USA\n",
            "Industry Type: Retail\n",
            "Products & Services: Retail Business, e-commerce\n",
            "Organization Size: 100+\n",
            "Design and develop a product recommendation engine based on the features of products\n",
            "Content-based product recommendation system has been created using Machine Learning Algorithm and Python.\n",
            "Recommendation engine have six cases which are mentioned below:\n",
            "Case 1:\n",
            "Case 2:\n",
            "Case 3:\n",
            "Case 4:\n",
            "Case 5:\n",
            "Case 6:\n",
            "The APIs for all the above cases have been created\n",
            "The code of the recommendation engine and its API is been delivered.\n",
            "Python, Postman\n",
            "Python, Machine Learning, Flask API, Pandas\n",
            "Affinity Propagation is a clustering algorithm that does not require a predefined number of clusters. It is used to group products based on their similarities.\n",
            "Python, Logical Reasoning, Machine Learning, Data Engineering.\n",
            "This recommendation engine can significantly enhance customers’ shopping experience by increasing the likelihood of them discovering products that perfectly align with their preferences. This personalized approach not only saves them valuable time and effort in searching for relevant items but also ensures that their unique needs and desires are met. As a result, customers are more likely to make purchases, leading to increased sales and revenue for the business.\n",
            "Moreover, this recommendation engine plays a crucial role in improving customer satisfaction and fostering long-term loyalty. By suggesting products based on individual preferences and specific features, customers feel understood and valued. This tailored experience enhances their overall satisfaction, making them more inclined to return to the business for future purchases. Additionally, satisfied customers are more likely to spread positive word-of-mouth, attracting new customers and expanding their customer base.\n",
            "Project Snapshots\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2035.txt\n",
            "\n",
            "Database Discovery Tool using OpenAI\n",
            "\n",
            "Client: A leading retail firm in the USA\n",
            "Industry Type: Retail\n",
            "Products & Services: Retail Business, e-commerce\n",
            "Organization Size: 100+\n",
            "Organizations often face challenges in managing and understanding their vast and complex databases. As data infrastructure evolves, new databases are introduced, and existing ones are modified, leading to a lack of comprehensive visibility into the entire data landscape. This lack of awareness poses several issues, including increased difficulty in ensuring data quality, security vulnerabilities, and inefficiencies in database administration.\n",
            "To address these challenges, there is a need for a Database Discovery Tool using OpenAI, aimed at providing an automated and intelligent solution for discovering, cataloging, and understanding the various databases within an organization’s ecosystem.\n",
            "Key Problems to Solve:\n",
            "Develop an OpenAI-powered Database Discovery Tool that leverages natural language processing (NLP) and machine learning capabilities to automatically discover, catalog, and provide insights into the organization’s databases. The tool should be able to:\n",
            "By addressing these challenges, the Database Discovery Tool using OpenAI aims to empower organizations with a holistic view of their data landscape, facilitating better management, security, and optimization of databases.\n",
            "Step 1. Database Support\n",
            "In this step we communicate with different types of databases, like SQL and Oracle. This means it can connect and retrieve information from a variety of database systems using Python, providing users with more flexibility and compatibility across various database environments.\n",
            "Step 2. Data Extraction\n",
            "In this step we are using python for our Extract, Transform, Load (ETL) processes this involves efficiently reading and extracting data from the connected databases. Python handled the data-related tasks, ensuring a robust and effective extraction process and save the result in csv files which in turn are converted to .db files for sqlite.\n",
            "Step 3. Fine-Tuning\n",
            "In this step fine-tuning mechanisms to optimize the performance and accuracy of data extraction processes. This Ensures the ETL tool finds data accurately and quickly.\n",
            "Step 4. Integration with OpenAI\n",
            "In this step we have utilized SQL Agent for communication with OpenAI, By communicating with OpenAI, the SQL agent get the ability to understand and respond in a more intelligent and context-aware manner.\n",
            "Step 5. API Integration\n",
            "In this step we made Django API endpoints for requesting and receiving data. This means that external systems or applications can interact with the SQL Agent through OpenAI by sending requests and receiving responses through these APIs.\n",
            "Step 6. Streamlit Frontend\n",
            "In this step we made a streamlit frontend to chat with the SQL Agent. The user can ask question about the database and receive responses in form of insights.\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2036.txt\n",
            "\n",
            "Automate the Data Management Process\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: SaaS, Products\n",
            "Organization Size: 100+\n",
            "Businesses now have more access to data than ever before in today’s digital economy. This information is utilised to make key business choices. Businesses should invest in data management systems that increase visibility, dependability, security, and scalability to ensure that workers have the required data for decision-making. The client wanted to get the data management process automated using a tool from Python. Multiple operations like merging,sorting, filtering had to be performed on data from various resources. The data resources were mainly csv files and data from SQL queries in PostgreSQL.\n",
            "The project solution contained two tools that would aid in automatic efficient data storage. The first tool will concatenate all of the CSV files before merging them with the data from the SQL file. The acquired Excel file will be used as input for the second tool. The second tool will sort, filter, and lookup the Excel file received in the first tool. This tool will add columns that will be useful for the client’s analysis. The major goal is to assist the client with data management while requiring as little manual labour as possible. The files obtain the needed data in an Excel file by giving the proper input files.\n",
            "The project deliverables can be divided into two parts:\n",
            "Two types of databases were used: Google excel sheets and PostgreSQL.\n",
            "Some minor challenges were faced such as data discrepancies generated during the automation process.\n",
            "The challenges were solved by reworking on the automation tool and consulting with the clients for their requirements.\n",
            "It is critical to use appropriate data management procedures to ensure the smooth running of a firm. Furthermore, data management must be very precise, cost-effective, and completed as soon as possible. The inability to handle data can result in costly consequences and a permanent stain on the company’s image. Every company is responsible for developing a robust data management plan. The following are some of the reasons why data management is critical to the success of the firm. Instant Availability of Information: Data management makes information easily available for quick access based on company needs. Data management is also essential for accounting procedures like auditing and other strategy-based operations like company planning. The more time you spend hunting for misplaced files and missing documents, the less productive you will be. And you are aware that time is money. Keeping all of your documents structured might therefore assist to make procedures run more smoothly and quickly. Compliance: The government passed legislation requiring businesses to maintain these data. There are also periodical checks to verify that there is no manipulation. Furthermore, if a corporation is involved in a dispute, they must maintain these records for years until a solid verdict on the matter is reached. Faster Transitions to New Technology: Because technology trends change so quickly, organizations must embrace whatever comes their way. Losing information due to obsolete or outdated systems is the last thing you want for your company. Every piece of data preserved in the firm records is essential for everyday operations, managing multiple divisions, completing computations, audits, and so on. Make Right Business Decisions: Businesses use a variety of information sources for company planning, trend research, and performance management. To execute the same activity, different departments’ teams employ different sources of information. Because the legitimacy and precision of information are highly dependent on the source, analyzing several sources may have a detrimental influence on the organization. Robust data management prevents this from happening.\n",
            "\n",
            "Fig.1: Python code of Exceltool1\n",
            "Fig.2: Python code of Exceltool1\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2037.txt\n",
            "\n",
            "Realtime Kibana Dashboard for a financial tech firm\n",
            "\n",
            "Client: A leading fintech firm in the USA\n",
            "Industry Type: Finance\n",
            "Services: Financial services\n",
            "Organization Size: 100+\n",
            "Create a real-time Kibana dashboard to monitor the real-time movement and activities related to company/stock on the AWS to analyse data and get insights through dashboards to prevent due diligence. Dashboard should include visualizations of sentiments, FOIA requests, stock prices, volume, borrow rate, etc.\n",
            "Create real-time dashboards to get insights about the data and to analyse the relative change in different activities. Someone filing FOIA SEC request or FOIA FDA request and/or registering for conference calls might also have posted some negative tweets on tweeter to influence the market. Dashboard should display data of requests, sentiments, stock prices, etc on the same timeline, so that we will be able to observe the changes and relative changes with respect to time. Make separate dashboard for 2 stock symbols to analyse the activities and changes specific to that and a dashboard for all the data, eg. stocks, requests, etc. Change in sentiments effecting the price of the stock, borrow rate, trading volume, etc. should be noticeable. There is a list of names, make alert on the dashboard when the requests are filed by them on the same timeline used for other data. Also include the candlestick chart to view the stock details like open, close, high, low, volume with respect to time.\n",
            "For FOIA SEC and FDA requests, made a metric chart representing the total number of requests and requesters, created a date histogram to view the frequency of requests and requesters with respect to time, bar chart to view the top requester name, organization, category, pie chart to view the proportion of final disposition of requests and tag cloud for the description of the requests for the entries present in the selected time range and a search table that contains the selected columns (only relevant ones) for both SEC filings and FDA filings.\n",
            "Similarly, for citation data, created a date histogram to view the frequency of citations and names of firms who posted with respect to time and bar chart to view number of citations by firm in the selected time range and a search table that contains the selected columns (only relevant ones). Index containing fail to deliver data is used to plot the date histogram in which volume failed is represented by the bar along the line representing the price at that time, bar chart where bars represents the total volume failed to deliver with respect to stock symbol and average price of the stock symbol in the selected time range by a dot size add on and tag cloud of the stock symbol as per fail to delivers.\n",
            "For twitter data (short seller’s data), made a pie chart to show the proportion of polarity, metric table to show the highest 10 average retweets with respect to user name, made a date histogram to show the frequency of tweets as per time and another date histogram representing the amount of positive and negative sentiments with the help of bars as per time to leverage us to observe if change in amount of sentiments is affecting price of stock, volume in trade and fail to deliver, etc., bar chart to show the total posts and number of posts in the selected time range and another bar chart to show the count of followers and friends in the index in selected time range. A search table is made with columns like polarity, follower counts, retweets and post with timestamp to get precise info of what we have in visualizations.\n",
            "For the list of names to be tracked on requests made and to make alert for them, added a annotation on the TSVB graph and added all of these along with the above visualizations on the dashboard on Kibana to make it a real-time dashboard and we can use this dashboard to do relative analysis.\n",
            "For the dedicated dashboards to the stock, created and added following visualizations:\n",
            "3 dashboards- 1 dashboard for complete data and 2 dashboards dedicatedly for one stock each.\n",
            "Kibana and Elasticsearch\n",
            "Visualizations and analytical skills were used\n",
            "Following databases are used to:\n",
            "AWS Management Console\n",
            "As I was using Kibana and studying the stock data for the first time, I faced challenges in making complex visualizations and understanding the terms related to stock data. Using filters while making Vega Charts to make candlestick chart with inconsistent data was displeasing.\n",
            "Challenges related to the creation of complex visualization was solved exploring options on the Kibana and getting reference from the online sources. In order to understand the stock information and how things work, I got immense amount of knowledge from the client and from my project manager. For filtering of data in Vega charts I took help from the online sources.\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2038.txt\n",
            "\n",
            "Data Management, ETL, and Data Automation\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: SaaS, Products\n",
            "Organization Size: 100+\n",
            "To extract the data for the given keywords from the listed websites https://www.ferguson.com/, https://www.bakerdist.com/, https://www.hajoca.com/, https://www.carrier.com/residential/en/us/, https://www.gemaire.com/, https://www.fwwebb.com/ and store the count of each keywords for each website it in an Excel File.\n",
            "A list of websites is provided from which we were supposed to find out the mentioned keywords and store their respective counts for each website in an Excel sheet with different tabs for different set of keywords.\n",
            "We used Selenium as well as Bs4(Beautiful Soup) to extract data from the given websites. To accomplish the given task, 2 tools were developed for each website.\n",
            "Extracted content from all the websites was stored in their respective text files. After that number of keywords in the text were counted using substring and count method and stored the keyword and its corresponding count in an Ordered Dictionary and then the count was transferred to a list and Excel file was created for the same. Counts received from search tool and content tool were combined and final output file was created.\n",
            "Python Interpreter\n",
            "Language Used: Python\n",
            "Libraries used: BeautifulSoup, collection.OrderedDict, pandas, requests, xlsxwriter, selenium.webdriver\n",
            "Some of the websites cannot be accessed using Indian IP address as it was having captchas. Also, we cannot go to each and every page by clicking the results and get the count.\n",
            "To bypass the captcha and reach the website, we need to use VPN of Singapore. And to get access to each and every page of the website, we found out sitemap for each website which includes link to every page present in it.\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2039.txt\n",
            "\n",
            "Data Management – EGEAS\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: SaaS, Products\n",
            "Organization Size: 100+\n",
            "To extract various Reports from the given input files. Reports to be extracted are: PRODUCTION COST – ANNUAL BY UNITS REPORT, SYSTEM EMISSIONS ANNUAL REPORT, RPS CONSTRAINT – ANNUAL REPORT, RELIABILITY – ANNUAL REPORT, RESERVE – ANNUAL REPORT and CAPACITY TOTALS ANNUAL REPORT. We had to extract above mentioned reports from the given .out files and store it in the respective .csv files.\n",
            "We were given a bunch of .out files in which various Reports were available in table format. We need to extract some of the required reports from the given files and store them in their respective .csv files. A tool had to be developed in python in order to accomplish this task.\n",
            "From each .out file its content extracted and stored in a list. Using regular expression, we searched the required report in the content. Another regular expression is used to mark as end of the table content. Content between the two given regular expressions is stored in a dataframe which is then stored into respective .csv file.\n",
            "Python Interpreter\n",
            "Language Used: Python\n",
            "Libraries Used: re, pandas, os\n",
            "Programming\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2040.txt\n",
            "\n",
            "Design and develop PowerShell script\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: SaaS, Products\n",
            "Organization Size: 100+\n",
            "Create a PowerShell script for the following:\n",
            "check and enable auditing\n",
            "for checking and enabling auditing of the file we used  PowerShell NTFSSecurity module\n",
            "configuring winrm for remote windows server\n",
            "For this we created 2 script:\n",
            "check audit of windows/system32 folder and windows/inf folder of remote windows server\n",
            "for this, we created a script that connects to the remote windows server using the Enter-PSSession command and then checks the audit for windows/system32 and windows/inf folder also we can add audit rule to windows/system32 and windows/inf folder from remote servers\n",
            "Powershell script\n",
            "powershell\n",
            "Check audit\n",
            "Add audit\n",
            "Check audit\n",
            "Before running create script\n",
            "Create script for winrm listner\n",
            "List of listeners after running create script\n",
            "Connect with remote machine\n",
            "When rights are not applied\n",
            "When rights are applied\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2041.txt\n",
            "\n",
            "Design and develop Jenkins shared library\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: SaaS, Products\n",
            "Organization Size: 100+\n",
            "Create Jenkins shared library for the following:\n",
            "Jenkins Libraries\n",
            "AWS\n",
            "\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2042.txt\n",
            "\n",
            "Design and develop retool app for wholecell.io and Asana data using their api’s\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: SaaS, Products\n",
            "Organization Size: 100+\n",
            "Create retool app for wholecell.io and Asana data using their api’s\n",
            "We have created two table one table contain data from wholecell.io platform and another table contain data from Assna.\n",
            "In that wholecell.io table we are providing:\n",
            "In Assna Table we are providing following details:\n",
            "As client data from wholecell and Assna was linked client can search the order by PO-id in Assna table\n",
            "App in retool\n",
            "Api was not providing all required details according to the client requirement and there were less options for data pre-processing as retool only javascript\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2043.txt\n",
            "\n",
            "Design and develop a retool app that will show stock and crypto related information using IEX API\n",
            "\n",
            "Client: A leading fintech firm in the USA\n",
            "Industry Type:  Finance\n",
            "Services: Crypto, financial services, banking, trading, stock markets\n",
            "Organization Size: 100+\n",
            "Create a retool app that will show stock and crypto related information using IEX API\n",
            "Created a flask web application with following features and pages:\n",
            "Page 1 (Home page)– Show a Stock & Crypto Search Bar that will show the most relevant option in the IEX API via ticker search. Upon submit, user will be taken to the “Ticker Page”– List the 10 top trending stocks for each category (link click to ticker page)(logo, Stock ticker, company name, stock price, % change.Mega CapLarge CapMid CapSmall CapMicro Cap\n",
            "Page 2 (Ticker Page)\n",
            "-Show Company Data – (Ticker, Company Name, Logo, Market Cap, and all the other corporate data (employees, CEO, HQ, Founded, Website)-Stock Price Chart – 1 year chart, daily.-Stock Price Volume – Weekly average 20 weeks\n",
            "-Recent News – list of 25 most recent articles\n",
            "Deployed flask web application on AWS\n",
            "AWS\n",
            "www.stocks.bullish.studio\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2044.txt\n",
            "\n",
            "CRM (Monday.com, Make.com) to Data Warehouse to Klipfolio Dashboard\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: Marketing, promotions, campaigns, consulting, business growth\n",
            "Organization Size: 100+\n",
            "The client requires a dashboard for a ”week in review” and “human resources”. The dashboard should be dynamic whenever the client opens the dashboard, it should show the current week and should also have a dropdown choice option based on different time periods. So the client requires a meaningful KPI on the dashboard.\n",
            "Taking the problem statement into consideration the following objectives are established.\n",
            "Objective 1: Getting access to the Monday.com site, Make.com, Google sheet, and Klipfolio. Objective 2: Connect Monday.com data to the Google sheet.\n",
            "Objective 3: Data Integration using make.com.\n",
            "Objective 4: Building KPIs using various calculations and formulas to get meaningful insights.\n",
            "Objective 5: Creating a dashboard from insight driven by KPIs.\n",
            "1. Data Integration\n",
            "Fig.3.4: Data Integration\n",
            "2. Overall Architecture\n",
            "Fig.3.4.2 Overall Architecture\n",
            "Klipfoliomake.com\n",
            "Klip Formula\n",
            "Data IntegrationData ProcessingData Visualization\n",
            "Google Sheet\n",
            "During the project execution we faced following challenges:1. Mapping the values in make.com from Monday.com2. Whenever the update is generated on Monday.com, a new row is added to the Google sheet.\n",
            "3. Extracting insights from the data\n",
            "To solve the technical challenges, we provided following solutions as follow:1. For mapping the values from Monday.com to make.com, we got access as admin to reach out the columns id on Monday.com.\n",
            "2. On make.com, we created multiple models linking each other based on the row id in the google sheet.\n",
            "3. After completing the data integration, we use calculations to extract meaningful insights from the data.\n",
            "Using this dashboard, a client can keep track of the employee’s work process. So he can analyze employee workflow nature.\n",
            "Google Sheet:https://docs.google.com/spreadsheets/d/15ADtNWh63O7DVbg-FRH0SmWb-TemqldOVK7dq16N7Xs/edit?usp=sharing\n",
            "Data Integration using make.com:https://us1.make.com/146703/scenarios?folder=all&tab=all\n",
            "Monday.com:https://primus-business-management.monday.com/\n",
            "List Of Employees listed on Klipfolio:https://app.klipfolio.com/clients/index\n",
            "Klipfolio Dashboard:\n",
            "https://app.klipfolio.com/dashboard?tab=012f404bf82f8b4e331c4a0c48d32978#:~:text=https%3A//app.klipfolio.com/dashboard/add_tab/8ca9ae6808284b158f640834f3e2afd8%3Fparam%3AstartDate%3D1671926400%26param%3ADatepickerB%3D1671753600%26param%3ADatePickerA%3D1671408000%26param%3Adropdown%3DWorking%20on%20it%26param%3AendDate%3D1672444800%26param%3AKTdate%3DFY%20to%20Last%20month%26param%3ADatePeriodq%3DThis%20Week\n",
            "Todo Board Part 1: https://www.youtube.com/watch?v=qnTV64RhGWk\n",
            "Todo Board Part 2: https://www.youtube.com/watch?v=vDyaVkNv6bU\n",
            "Todo Board part 3: https://www.youtube.com/watch?v=FciSkP-uRkM\n",
            "Census Board Part 1: https://www.youtube.com/watch?v=jpgzakxdvZw\n",
            "Census Board Part 2: https://www.youtube.com/watch?v=3y6DmUGNmTE\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2045.txt\n",
            "\n",
            "NER Task using BERT with data in XML-format\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: SaaS, Products\n",
            "Organization Size: 100+\n",
            "The goal of this task is to create and implement a workflow that annotates People/Places/Organizations and assigns them a specific number (from a normdatabase). The NER-Task should be done by using Bert (NER-German https://huggingface.co/flair/ner-german or something similar).\n",
            "The input to this first task is a text in XML-Format. It is important that the structuring text is not altered by the NER. This could be possible by tokenizing the XML-elements in a different/seperate way, to then run the NER with BERT and afterwards add the elements afterwards at the exact position where the initially were. The tags that were added by the NER than can be easily replaced with the required tags in the XML-format.\n",
            "Input Data 🡪 XML Text Tokenization 🡪 NER Model 🡪 Replace NER Tags with XML Tags 🡪 Final Output\n",
            "Python tool\n",
            "Documentation\n",
            "Installation\n",
            "VSCode For Python script\n",
            "Python Programming Language\n",
            "Named Entity Recognition (NER)\n",
            "FuzzyWuzzytqdmFlairPandas\n",
            "Data LoadingData ProcessingData Restoring\n",
            "During the project execution, we faced the following challenges:\n",
            "To solve the technical challenges, we provided following solutions as follow:\n",
            "The client can know easily predict the Name, Place, and Organisation from XML containing file by using our python script model.\n",
            "Fig. Input XML file\n",
            "Fig. Output XML file with predicted values.\n",
            "Github: https://github.com/AjayBidyarthy/Sven-Meier-XML-tool/tree/master\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2046.txt\n",
            "\n",
            "Qualtrics API integration using Python\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: SaaS, Products\n",
            "Organization Size: 100+\n",
            "API Integration to read/write data in SQL tables from an online application.\n",
            "To write the api between qualtrics and sql server using python programming language.\n",
            "Fig. System Architecture\n",
            "Python Software\n",
            "Documentation\n",
            "PythonQualtrics\n",
            "PandasRequestsnumpyZipfileiopyodbc\n",
            "Extract Transfer Load\n",
            "SQL Server\n",
            "During the project execution, we faced the following challenges:\n",
            "To solve the technical challenges, we provided the following solutions as follow:\n",
            "Using this script the client can now fetch the Qualtrics data into the SQL server automatically after every 1 hour.\n",
            "Fig. Data in CSV Format\n",
            "Fig. Data in Table form\n",
            "Fig. SQL data\n",
            "Github:  https://github.com/AjayBidyarthy/Richi-S-api\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2047.txt\n",
            "\n",
            "Design and develop MLops framework for Data-centric AI\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: SaaS, Products\n",
            "Organization Size: 100+\n",
            "The task involves finding models and tools for several different tasks across various domains. The tasks include video and image capturing, working with documents such as PDF and Excel files, converting text to audio, audio capturing and transcription, translation to major languages, utilizing language models with a focus on Jina finetuner and its limitations, creative AI for generating pictures and designs, synthesizing language texts, creating Kibana dashboards and data storytelling, code creation for specific platforms like Editorjs and Nextjs, integrating Jina API inference into function blocks in Editorjs/Nextjs, UX/UI creation for the front end of Editorjs and Nextjs, transfer learning and reinforcement learning, utilizing Wikipedia for general knowledge, and utilizing an epistemic model called EPINET. To fulfill this task, you will need to search for relevant models, tools, and resources specific to each task mentioned above.\n",
            "Jina Hub/AI, Python, Hugging Face, Argilla, Redis stack, Kibana\n",
            "Python\n",
            "Epistemic Neural Nets, weight watcher, OpenAI Whisper transformer, Epinet\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2048.txt\n",
            "\n",
            "NLP-based Approach for Data Transformation\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: SaaS, Products\n",
            "Organization Size: 100+\n",
            "Performing Readability and Quality testing on the text corpus from text files\n",
            "The intention was to create a tool/system that can consume text files through a given csv file having a path for all the text files through this csv file our tool should be able to read all files one by one and could perform some tests and analysis on that text data and output the results in a csv format presenting all the metrics.\n",
            "In order to achieve this goal we created a Python-based ready-to-use code that will read all text files presented in the given csv files and perform 14 different evaluations on that text data and save the results in a excel and csv based format.\n",
            "The final deliverable was the tool/system/code for processing and evaluation text.\n",
            "Python Programming\n",
            "The architecture of the solution for this project problem statement was simple, no challenges were faced during the execution of the project.\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2049.txt\n",
            "\n",
            "An ETL tool to pull data from Shiphero to Google Bigquery Data Warehouse\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: SaaS, Products\n",
            "Organization Size: 100+\n",
            "Shiphero company is an organization providing shipping solutions to vendors. The data created by shiphero for different product picking and packing time period doesn’t provide much insight into the efficiency of ship hero employees and other aspects that are needed and useful for vendors/brands to make better decisions for their business in order words the ‘key’ data is missing.\n",
            "The solution is an effort to create the missing data by the existing data as we came to know that the ‘key’ data can be created by involving some deep methodologies and vast logical aspects linked to it. The incoming data from shiphero company is timestamp data therefore using this sequential data we can create the missing data we need to get the required KPI’s.\n",
            "The overall architecture included getting data from shiphero through api doing some preprocessing and creating our ‘key’ through this data and populating it on Google big query. This google big query is linked to Google data studio for insights visualisation.\n",
            "The data coming from Shiphero is extracted every day using a cron job scheduler. Google app engine service is used to preprocess and apply a transformation to the data.\n",
            "Ready-to-use Google data studio Dashboard. Google app engine service-based scheduler code.\n",
            "Initially the approach client introduced that could be able to solve the problem directly failed to give proper results and because of that we need to come up with a solution that could be able to estimate our ‘key’ column to some extent.With the way around solution using statistics and data modelling there were a series of challenges coming that were creating a question mark for us but with keen solution building and delivering the desired results we came to solution for every challenge that arose.\n",
            "Statistics was the only way around for the challenges we faced because it was the data which was missing and as the incoming data was in sequential format so we were able to figure out the patterns from that and the main problem of missing data for our KPI’s\n",
            "Better insights into the business.\n",
            "Dashboards aren’t finalised but yes giving desired solutions.\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2050.txt\n",
            "\n",
            "Plaid Financial Analytics – A Data-Driven Dashboard to generate insights\n",
            "\n",
            "Client: A leading financial firm in the USA\n",
            "Industry Type:  Finance\n",
            "Services: Financial Services\n",
            "Organization Size: 100+\n",
            "Applying automation to Financial data coming from the Plaid platform that needs to be visualized in order to get better insights and metrics from data.\n",
            "The intention was to create an automation tool that could consume the financial csv format data and perform preprocessing on that data and could directly present the insights on visually appealing dashboard.\n",
            "Initially the step was to create a tool/website that could consume the data and preprocess it and send it either directly to dashboard or into a database so the data could be safe and through the database the dashboard could be linked and updates accordingly.\n",
            "The data source for the tool was to be a manual entry therefore we created a website and hosted it on a cloud platform(Heroku) to make it available all the time for all the desired users. The processed data from this tool will be send to the Google big query database and our GBQ will be linked to the Google Data Studio for the insights presentation. Therefore as the data will keep on updating in the google big query accordingly the dashboard in our google data studio will gets updated.\n",
            "The final deliverable was the ready-to-use dashboard and website where the preprocessing of the data happens.\n",
            "The project was easy to implement and the architecture was simple therefore no major challenges were encountered.\n",
            "\n",
            "https://plaid-conversion.herokuapp.com/\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2051.txt\n",
            "\n",
            "Recommendation Engine for Insurance Sector to Expand Business in the Rural Area\n",
            "\n",
            "Client: A leading insurance firm in the globe\n",
            "Industry Type:  Insurance\n",
            "Services: SaaS, Products, Insurance\n",
            "Organization Size: 10000+\n",
            "BangDB is the platform that manages the static data stored on the cluster and also works with live streaming data as Hadoop does. Wherever the bangdb is able to manage machine learning model deployment with their inbuilt parameter and hyper tuning parameters for each model.\n",
            "Streaming data from the client which relates to the customer details and the numbers of products offered by the client on their platform, such as Insurance, loans (Business Loans and Personal Loans), Mobile recharge, UPI transactions done by their platform, etc.\n",
            "They wanted the recommendation of other services provided by them to each of their customers who are using their platform.\n",
            "This Project Module develops according to the Clients Requirements which involves item-based collaborative filtering based on customer behaviour, Firstly classify the customers into various segments on the basis of age, location, gender, and product usage. On the basis of RFM (marketing tactics to classify the customer on the basis of their purchase history, amount spend, and frequency of usage of product) classify them and recommend them the other services based on item-based collaborative filtering.\n",
            "We generated the synthetic data (90 Million events) for the testing of the recommendation model and its accuracy for recommending the other products to customers.\n",
            "–   KPI of the Customers\n",
            "–   Recommendation model\n",
            "–   Graph databased model\n",
            "–   Data Generation code based on python (using copula-based on PyTorch)\n",
            "-K means model for clustering\n",
            "-Recommendation Engine model\n",
            "-Collaborative based filtering model\n",
            "– Machine learning\n",
            "– NoSQL Database\n",
            "– Graph database\n",
            "– Data Generation using python\n",
            "– Linux\n",
            "– Data Visualization\n",
            "– BangDB\n",
            "– Graph Database\n",
            "– Microsoft MYSQL server\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2052.txt\n",
            "\n",
            "Data from CRM via Zapier to Google Sheets (Dynamic) to PowerBI\n",
            "\n",
            "Client: A leading solar panel firm in the USA\n",
            "Industry Type: Energy\n",
            "Services: Solar Panel\n",
            "Organization Size: 500+\n",
            "Solar Panel organization from America wants to keep track of sales data. They want to see the leadership dashboard of their organization in terms of sales. They also want to keep track of their campaigns and leads generated from sources of those campaigns. They want to keep track of sales data from different sources.\n",
            "First, we fetch the data from CRM to PowerBI. Clean the data of CRM using DAX and then perform calculations on the data. Using cleaned data, we build KPI on PowerBI.\n",
            "To complete the project, we follow the following data flow pipeline:\n",
            "Data from CRM 🡪 Zapier 🡪 Google Sheet (Dynamic) 🡪PowerBI\n",
            "PowerBI, DAX Language\n",
            "CRM, Zapier , PowerBI, Google Sheet\n",
            "Challenges Faced during the Project Execution :\n",
            "Solution:\n",
            "Using this Dashboard client can make important decisions like from which campaign they are getting a greater number of leads and out of those leads how many are actually a Sale. They can keep track of their sales leadership of employee of the month in term of sales.\n",
            "CRM\n",
            "Zapier\n",
            "Dashboard\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2053.txt\n",
            "\n",
            "Data Warehouse to Google Data Studio (Looker) Dashboard\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: SaaS, Products, healthcare, government, energy\n",
            "Organization Size: 100+\n",
            "Our client needed a Google Data Studio dashboard for different sectors such as Oil and Gas, Government, Healthcare, and Sales analysis. They want to see an analysis of data from which they can provide insights in different domains. They want us to create visual KPIs of meaningful insights.\n",
            "They provided us with data for different sectors. Using those data first we analyze the data and perform EDA on data for cleaning the data. After cleaning the data, we performed calculations to extract insights for KPIs. Using those KPIs we build a dashboard on Oil and Gas, Government, Healthcare, and Sales analysis.\n",
            "To build the dashboard we follow the pipeline as follows:\n",
            "Data 🡪 EDA(Cleaning data )🡪 Connection(GDS) 🡪 Building KPIs(Visuals)\n",
            "Google Data Studio\n",
            "EDA, Google data studio\n",
            "During the project execution, we faced the following challenges:\n",
            "To solve the technical challenges, we provided following solutions as follow:\n",
            "Using these dashboards client can visualize the sales insights and understand the workflow. They can take crucial decisions based on these insights which will help them to make an impact on their sales.\n",
            "Sales Dashboard:\n",
            "Government Dashboard:\n",
            "Oil and Gas Dashboard:\n",
            "Hospital Analysis:\n",
            "Dashboards on Google Data Studio:\n",
            "1.Government:- https://datastudio.google.com/reporting/dda94ce8-5b77-46aa-a1e0-1a57ccaef5f9\n",
            "2.Oil:- https://datastudio.google.com/reporting/47c6529e-1355-4072-babf-1a96f9f842cf\n",
            "3.Healthcare:- https://datastudio.google.com/reporting/b1e95a11-4380-465c-ad45-2d1995c799fb\n",
            "4.Sales:- https://datastudio.google.com/reporting/36ec0e42-6b77-4fbb-9dea-760cccaa741f\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2054.txt\n",
            "\n",
            "CRM, Monday.com via Zapier to Power BI Dashboard\n",
            "\n",
            "Client: A leading solar panel firm in the USA\n",
            "Industry Type:  Energy\n",
            "Services: Solar Panel\n",
            "Organization Size: 200+\n",
            "Mohsin has Solar Panel Company. He has setup CRMs for that. He wanted to use CRMs data and want to visualize the leads in PowerBI\n",
            "First, we check CRMs thoroughly and understand the work culture of his company. It was not easy to fetch data into PowerBI using API key. To fetch new leads from CRMs we used Zapier. The limitation of Zapier is it cannot fetch historical data into spreadsheet. So we download data from CRMs and fetch it into spreadsheet. For new leads we created zaps for every instance. After that we connect the spreadsheet with PowerBI and clean the data accordingly. Using that data, we build KPIs according to client need.\n",
            "API , Zapier , Spreadsheet , PowerBI\n",
            "M language , DAX\n",
            "API , M language , DAX , PowerBI\n",
            "First challenge was to fetch data from CRMs using API key. Data we were getting was uncleaned and were not able to fetch all data. If there were multiple pages in the CRMs we will not be able to fetch all data from the pages.\n",
            "Technical challenge in this project was to extract data from CRMs. So for that we used Zapier connector from CRMs to spreadsheet. But there was some limitation with Zapier that it will not fetch the historical data of our CRMs. So to solve that we download all historical data from CRMs and append it to the spreadsheet we were using. We fetch new leads to our spreadsheet using Zapier. By doing this now we have all the data historical and new lead which will be pushed by Zapier.Then we fetch the data to our PowerBI and do some cleaning in data. By using cleaned data, we build the KPIs for our client according to there requirements.\n",
            "Client will be able keep track on his company data on PowerBI and it helps them to make decisions accordingly.\n",
            "CRMs\n",
            "Zapier\n",
            "PowerBI Dashboard\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2055.txt\n",
            "\n",
            "Monday.com to KPI Dashboard to manage, view, and generate insights from the CRM data\n",
            "\n",
            "Client: A leading energy firm in the USA\n",
            "Industry Type:  Energy\n",
            "Services: Solar panel\n",
            "Organization Size: 200+\n",
            "Mohsin has CRM for his business where he has all data regarding leads of his clients. He wanted to see all his client appointments at one place. Client took subscription of Monday.com. It is an CRM where you can manage your work more easily in neat and clean user-friendly environment. We can easily track our task on Monday.com. Pipeline for Monday.com is very easy to use and also customized according to our needs.\n",
            "The challenging part of this project was to get CRM data on Monday.com dashboard. Client also has subscription of Zapier. Zapier is a connector which connect two apps to transfer data from each other. Zapier also has limitation to fetch limited type of data from CRM. Like for Mohsin CRM we can only fetch hot lead comes on CRM. But in his CRM there are also other functions like if a customer lead comes on CRM. They manually book appointment for that client. So there is no way to get that data from CRM. Issue for client was he has attached integrated four google calendar account with CRM so whenever he confirms appointment on CRM that data fetched on google calendar. But he has check manually one by one on each calendar which was bit hard task for him. So, we advised Monday.com where he can track all his task at one place.\n",
            "The challenging part of this project was to get CRM data on Monday.com dashboard.There is no direct integration of CRM and Monday.com to fetch data.\n",
            "To solve challenges, we used Zapier to get CRM data to dashboard of Monday.com. We used google calendar of client which were integrated with CRM. All the appointment confirmed leads were present on google calendar.Pipeline of Data:\n",
            "CRM 🡪 Google Calendar 🡪 Zapier 🡪 Monday.com\n",
            "Using the Monday.com dashboard client can easily track all appointments of customers. He can track data of his team members and connect with them at one place. He will not miss any of his meeting with customer. Monday.com also has timeline and calendar view using that client can see all activity of his work.\n",
            "CRM Calendar view\n",
            "Monday.com\n",
            "Google Calendar\n",
            "Zapier\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2056.txt\n",
            "\n",
            "Data Management for a Political SaaS Application\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: SaaS, Products\n",
            "Organization Size: 100+\n",
            "As per the guidelines and discussion. Political Research Automated Data Acquisition (PRADA) in the following phases which included.\n",
            "1. Get pics for existing EOs (Elected Officials)\n",
            "2. Get new EOs and Pictures.\n",
            "3. Run QA checks regularly on EOs\n",
            "4. Get data from government Facebook pages.\n",
            "5. Geospatial project: Create a new version of provided KML without using google earth.  Creating a nested directories which contained description and Map-URL at the designated location.\n",
            "6. Get data of US States and Counties(Including Boroughs and Parishes)\n",
            "By building an automated generated structured data that allows a non – programmer to create a config for each page allowing a bot to scrap and update the data.\n",
            "We created an automated python scripts for designated phases with respective requirements. Solutions to various type of problems varied such as most of data scrapping automation was done through python developed scripts including the geospatial KML task. In addition to this different ranges of data was scrapped generated directed output for the respective tasks in the form of CSV format. So the user’s main aim requirement was achieved i.e. a non programmer could create a con-fig and initiate a bot to scrap the required data.\n",
            "The majority task of project consisted of web data scraping automation so a high- level overview, and specific implementation details of project shall will be as follows:\n",
            "None. All the structured data was in the form of either python Data Frames, CSV or Excel Sheets.\n",
            "Chrome driver initiated\n",
            "Chrome driver visiting the directed links and accessing the image URLs\n",
            "Directed to next linkKML task\n",
            "Facebook Data extraction\n",
            "\n",
            "Data of State Governments of US\n",
            "\n",
            "Accessing links through wiki directing to counties\n",
            "Nesting within the list of counties of a particular stateFinding and Extracting link of the website of County\n",
            "The GitHub repository link:- https://github.com/AjayBidyarthy/Paul-Andr-Savoie/tree/main\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2057.txt\n",
            "\n",
            "Google LSA Ads (Google Local Service Ads) – ETL tools and Dashboards\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Ads, Marketing, Campaign, Consulting\n",
            "Organization Size: 200+\n",
            "The client has a Google LSA Ads Manager Account with about 100+ accounts and wishes to collect data available through the Google LSA API daily. The client wishes to set up a private Databases that is automatically created for newly added accounts and stores all of the collected data (Lead and Phone Call data). Finally, all collected data must be presented through the Google Looker Studio Dashboards, with the design layouts as suggested by the client.\n",
            "The solution involves a number of Python-based ETL tools that are responsible for fetching the data from Google’s LSA API daily and updating the same in the Google BigQuery Databases.\n",
            "Two different tools run are:\n",
            "The fetched data is stored in BigQuery Databases on the client-provided (Google)manager account.\n",
            "Carefully curated Google Looker Studio dashboards implemented with client-suggested theme layout which are updated upon client request, represent a number of KPIs and graphs indicating major data trends.\n",
            "The designed dashboards have a number of data-controlling filters that filter the data account-wise and date-wise.\n",
            "Heroku: Cloud Application Platform\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2058.txt\n",
            "\n",
            "Ad Networks Marketing Campaign Data Dashboard in Looker (Google Data Studio)\n",
            "\n",
            "Client: A leading financial firm in Dubai\n",
            "Industry Type:  Financial Services\n",
            "Services: Banking, Financial Services, Card Payments, Mobile Payments, Digital Bank, and FinTech\n",
            "Organization Size: 200+\n",
            "Build dashboards unifying all the platforms in use: Google Ads, FB ads, Appsflyer, Mixpanel, etc,in order to be able to track everything in the funnel from traffic source to total installs (paid, organic and by channel\n",
            "https://datastudio.google.com/reporting/8af163c1-b328-4ed3-91fc-cf8a026d0d9f\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2059.txt\n",
            "\n",
            "Analytical solution for a tech firm\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: Consulting\n",
            "Organization Size: 100+\n",
            "The client’s organization had a project that matches URLs up using TF-IDF algorithm.\n",
            "The script threw some errors and resolving these errors was the immediate ask.\n",
            "The client also required us to adjust the script for better accuracy and faster computation.\n",
            "Python\n",
            "Google spreadsheets\n",
            "https://colab.research.google.com/github/AjayBidyarthy/Daniel-Emery/blob/main/vanilla.ipynb#scrollTo=vPp14xj020RL\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2060.txt\n",
            "\n",
            "AI solution for a Technology, Information and Internet firm\n",
            "\n",
            "Client: A leading Technology, Information and Internet firm in India\n",
            "Industry Type:  IT\n",
            "Services: Emerging Technologies, 2030, and 2050\n",
            "Organization Size: 10+\n",
            "The objective was to analyze, research, and propose data science solutions in the product based on the product design, use cases, and services.\n",
            "b. List needed data\n",
            "c. List process\n",
            "d. List models\n",
            "e. List solution\n",
            "Statement of Work (SoW) with a solution documentation\n",
            "Python- Flask\n",
            "Amazon S3\n",
            "AWS EC2\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2061.txt\n",
            "\n",
            "AI and NLP-based Solutions to Automate Data Discovery for Venture Capital and Private Equity Principals\n",
            "\n",
            "Client: A leading Venture Capital and Private Equity Principals in the Globe\n",
            "Industry Type:  Venture Capital and Private Equity Principals\n",
            "Services: Private Equity, Venture Capital, Data Analysis, Fund Performance, Alternative Assets, Competitive Intelligence, Limited Partners, Customized Benchmarks, Service Providers, Fund of Funds, M&A, and Financial Services\n",
            "Organization Size: 100+\n",
            "Flask, Spacy, NLTK, pandas, numpy, transformers, elasticsearch etc.\n",
            "Question answering in NLP, web scraping, web application Flask, Python\n",
            "Distil-bert model, en-core-web-sm (pre trained model of spacy)\n",
            "NLP, Data Analysis, Flask web app, Pandas, Numpy, transformers, fastapi, elasticsearch etc.\n",
            "Elasticsearch database\n",
            "AWS\n",
            "This funding-related data would be used in two ways. From this project, companies can find suitable investors for their startups. Companies can search for investors based on industry, verticals, etc., and find investors to help their startups.\n",
            "Investors can use it to find a startup in which they want to invest based on their preferences like industry, verticals, etc.\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2062.txt\n",
            "\n",
            "An ETL solution for an Internet Publishing firm\n",
            "\n",
            "Client: A leading internet publishing firm in Singapore and Australia\n",
            "Industry Type:  Internet Publishing\n",
            "Services: peer-to-peer car sharing platform where you can rent a large variety of cars, always nearby at great value\n",
            "Organization Size: 100+\n",
            "We need to fetch last month’s call details (from user, to user, call_time, call_status ) using zendesk api.\n",
            "Then we need to analyse all call logs and need to identify the number of calls made by a particular user to the company and the most recent call timing from the company server.\n",
            "To fetch all call logs using zendesk api we used python language in programming. When we checked call details in the zendesk api, the details were in json format which is very tough to understand the calls details. So first we have fetched only needed details (call made from person, to person and call timing) converted into tabular format. In tabular format it was easy to identify call details.\n",
            "After that we need to identify the number of calls made by the user to the company in the last month.  We used the python pandas module here which is very fast and effective to handle tabular data. First we separated the user who made a call to the company last month and then counted each unique user’s call records. For recent dates we used python’s datetime module which can easily identify recent date time.\n",
            "2 python scripts\n",
            "VS Code, Google Drive, and MS Excel.\n",
            "Python programming language, Data Analytics with numpy and pandas, python datetime.\n",
            "Data Analytics,, Python, Mathematics\n",
            "local data from MS Excel Sheet\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2063.txt\n",
            "\n",
            "AI-Based Algorithmic Trading Bot for Forex\n",
            "\n",
            "Client: A leading trading firm in the USA\n",
            "Industry Type:  Finance\n",
            "Services: Trading, Banking, Investment\n",
            "Organization Size: 100+\n",
            "\n",
            "Pandas, numpy, scikit-learn, tensorflow, flask etc.\n",
            "Data Analysis, Data Visualization, Machine learning, Deep learning, flask web app etc.\n",
            "Logistic Regression, LSTM model\n",
            "Data Analysis, Data Visualization, Machine learning, Deep learning, flask, python etc.\n",
            "MongoDB\n",
            "AWS Ec2\n",
            "It will help traders to predict the stock market earlier and get better returns from this project.\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2064.txt\n",
            "\n",
            "Equity Waterfalls Model-Based SaaS Application for Real Estate Sector\n",
            "\n",
            "Client: A leading real estate firm in the USA\n",
            "Industry Type:  real estate\n",
            "Services: Property business, investment, real estate\n",
            "Organization Size: 100+\n",
            "The objective is to create software that will calculate the equity waterfalls for different cases. And there should be 3 users admin, sponsor and investor. We need to create the equity waterfall calculation according to the csv file that is shared by the client. All users have their own UI portal.\n",
            "The project is created using python language, working on django rest framework and for frontend we use reactjs and the code deployed on google cloud app engine service. We need to create a software that will calculate the equity waterfalls. And there should be 3 users admin, sponsor and investor. We need to create the calculation according to the csv file that is shared by the client.\n",
            "All users should have their own UI portal.\n",
            "Sponsors can create deals and send deal invitations to all investors or specific investors.\n",
            "Investors can see all the deals that are offered by the sponsor’s. After that Investors can subscribe that deal after subscription it is depending on sponsor that he will accept the investor subscription or not.\n",
            "We have created api’s that will calculate the equity waterfall calculation according to the selection of the waterfall tiers.\n",
            "Google cloud platform\n",
            "The technical issues faced during the project is how to calculate the equity waterfall calculation for different tiers and different cases. And also invite the sponsors by admin or sponsors invite their investors.\n",
            "We have used conditional statements in code and write different codes for different calculations. so that it will check which case we need to run and it will run accordingly.\n",
            "Added the functionality in which admin can invite the sponsors to the website and sponsors can invite their investor through sending the invitation link to their email.\n",
            "https://stackshares.io/dashboard/add-new-deal\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2065.txt\n",
            "\n",
            "AI Solutions for Foreign Exchange – An Automated Algo Trading Tool\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  Financial Services\n",
            "Services: Trading, consulting, financial serivices\n",
            "Organization Size: 100+\n",
            "Our main objective in this project was to help with setting up with given Broker API using MT4 and extracting historical data from it, and solving different tasks which are related to extracting important values from the data. And tasks assigned by the client were related to working around the data, i.e. formatting, connecting with the IG trade broker, automating the Python script and scheduling the script accordingly.\n",
            "During the initial phase, we were assigned to set up an MT4 with given Broker API access to extract historical prices, which was delivered to the client. In the second phase, the client requested to implement Profit/Loss, Spread Direction and Time in Trade. There were minute tasks related to the R script, which was duly completed. In the third phase, the client was assigned a task related to distinguishing the tickers according to cluster types which he provided and implemented code to distinguish the sell and buy spread for the given STD. In the fourth phase, I implemented the logic (Profit/Loss – (1% of 1st Currency + 1% of 2nd Currency)) into the existing code and worked on retrieving Historical prices from another Broker API and retrieving Watchlist given attributes by the client. Automated the Python script to retrieve yesterday’s market price of the given list\n",
            "Successfully delivered set-up in MT4 for retrieving historical prices, Created logic for automating the profit and loss, Implemented code to distinguish the tickers according to the cluster type, Implemented code for distinguish the sell and buy spread for the given STD, Implemented the logic (Profit/Loss – (1% of 1st Currency + 1% of 2nd Currency)) into the existing code. Automated the Python script to retrieve yesterday’s market price.\n",
            "MT4, Jupyter Notebook, Excel, IG trade, Remote Desktop setup\n",
            "MQL, Python, R\n",
            "Critical thinking, Logical Thinking\n",
            "While setting up MT4 platform and its configurations\n",
            "The above-mentioned challenges were resolved after many hours of effort and understanding.\n",
            "\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2066.txt\n",
            "\n",
            "AI agent development and Deployment in Jina AI\n",
            "\n",
            "Client: A leading tech firm in Europe\n",
            "Industry Type:  IT\n",
            "Services: IT and Consulting\n",
            "Organization Size: 100+\n",
            "The client’s object was to create AI agents for his website, which the end-users will utilize for many tasks. The client had some recommendations on the models are utilized.\n",
            "Created a feasible models list that complements the client’s requirement and when ahead and executed the Executor code for every model for compatibility with JinaAI deployment. After implementing Executor codes, I created a Flow to connect every executor and deployed it successfully.\n",
            "Successfully delivered executable deployed models in Jina Ai\n",
            "Jina AI, VSCode, HuggingFace\n",
            "Python\n",
            "Whisper, Stable Diffusion, GPT3, Codex, YOLO, CoquiAI, PDF Segmentor\n",
            "Python, Model APIs\n",
            "JinaAI Cloud\n",
            "There were minute challenges, such as deployment issues and Execution issues\n",
            "I resolved the issues effectively after long hours of understanding the concept because JinaAI is a new growing technology that does not have many forums to solve errors and issues.\n",
            "\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2067.txt\n",
            "\n",
            "Golden Record – A knowledge graph database approach to unfold discovery using Neo4j\n",
            "\n",
            "Client: A leading retail firm in the USA\n",
            "Industry Type:  Retail\n",
            "Services: Retail business, consumer services\n",
            "Organization Size: 100+\n",
            "To use data ingested into Neo4j and use the nodes and relationships with its properties to determine which nodes are actually the same person. For eg: we have Person nodes in the data, now people might enter their names in different ways. Our main aim is to identify Person nodes that may have similar data and are actually the same person. This will be represented as a perfect match between the nodes. This single-person view is referred to as the Golden Record\n",
            "Till date, we have loaded data into Neo4j and created relationships with score property which defines match strength. We have created some criterias by which we can determine what constitutes two nodes being the same and then based on them created ‘perfect match’ and ‘probable match’.We have considered four properties for our criteria – full name, address, driver’s license, and passport number. We have relationships between nodes for these properties with scores, we use these in our perfect match and probable match creation.\n",
            "We have also configured Graphlytics (a viz software) in the virtual machine which connects to the neo4j database and helps vizualize the nodes and relationships.\n",
            "We have also worked on some algorithms using the GDS library in neo4j to produce more information on the graph, the common neighbors algorithm was used to produce scores based on node similarity and the higher the score the higher the similarity. Other algorithms were tried as well but since all the properties are of String format it did not work on it.\n",
            "We have Resolved issues neo4j is facing when deleting a Large set of data and Provided steps to recover neo4j if it fails by going OutofMemory.\n",
            "We have figured out the issues with the probable and perfect match cypher queries not working as intended and proposed a solution.\n",
            "Neo4j\n",
            "Cypher Query Language\n",
            "The common neighbors algorithm\n",
            "CQL\n",
            "Neo4j\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2068.txt\n",
            "\n",
            "Advanced AI for Trading Automation\n",
            "\n",
            "Client: A leading tech firm in Europe\n",
            "Industry Type:  Banking & Finance\n",
            "Services: Trading, and financial services\n",
            "Organization Size: 100+\n",
            "The Problem\n",
            "Create an automated trading application with fully automated trading capabilities from selecting pair of assets to buying/selling assets. This application uses AI to decide what action to take while trading.\n",
            "Our Solution\n",
            "We have integrated coin_api with the application from which data is extracted. We have created the homepage for this application. We have changed the code structure of the front end to make it more fast and efficient.\n",
            "Solution Architecture\n",
            "An application, where the first automated top asset pair selection happens. If the coins are co-integrated, then only one indicator must be executed else trading starts based on 2 indicators.\n",
            "The AI agent will take specific action to trade based on the algorithm.\n",
            "Deliverables\n",
            "We have removed the old API and integrated the new api with the application.\n",
            "We have altered the code structure of the front end to make the code faster and more efficient.\n",
            "Tools used\n",
            "Visual studio code\n",
            "Language/techniques used\n",
            "Python\n",
            "Skills used\n",
            "Django\n",
            "Databases used\n",
            "SQlite\n",
            "Web Cloud Servers used\n",
            "Digital Ocean\n",
            "What are the technical Challenges Faced during Project Execution\n",
            "We faced an issue while integrating coin api with the application while retrieving the data. To retrieve the data using the coin api, we need to input a symbol id. This symbol id is a combination of exchange_name, symbol_type, currency_we_want_to_trade, and quote_currency. There are N coins that can be retrieved using coin api. There are more than multiple exchanges, multiple symbol types, and multiple quote currencies for ONE SINGLE COIN. This makes there is a huge no. Of combinations for one single coin. This made the execution of the api integration very slow.\n",
            "How the Technical Challenges were Solved\n",
            "We created one drop-down for exchange selection, one drop-down for symbol type selection, one drop for coin, and one drop-down for quote currency selection. The user selects these, and in the backend, a combination is created and is sent as input to the coin api code and the data is retrieved without slowing down the process.\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2069.txt\n",
            "\n",
            "Create a Knowledge Graph to Provide Real-time Analytics, Recommendations, and a Single Source of Truth\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  Retail\n",
            "Services: Retail Business\n",
            "Organization Size: 100+\n",
            "The Client was using NoSql Database which was slow and did not provide real-time response for complex queries. The data had many Connections and it was difficult to represent them in NoSQL or Relational Databases.\n",
            "Create a Knowledge Graph and Provide Real-time Analytics and Recommendations using Machine Learning.\n",
            "Neo4j was Installed on a Cloud VM based on Linodes.\n",
            "Knowledge graphs and Data Pipelines are used to Populate the Graph.\n",
            "API’s to Perform CRUD operations in real-time.\n",
            "Node-Relationship model\n",
            "Neo4j\n",
            "Linode\n",
            "Integration of Firestore with Neo4j without any native integration method or driver.\n",
            "The challenge was solved by using api to retrieve data from Firestore.\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2070.txt\n",
            "\n",
            "Advanced AI for Thermal Person Detection\n",
            "\n",
            "Client: A leading tech firm in the Middle East\n",
            "Industry Type:  Security\n",
            "Services: Security services\n",
            "Organization Size: 100+\n",
            "Detect a Person from thermal image and videos. Why this model was created was not told to us by the client.\n",
            "Use Deeplearning Computer Vision to train the model on custom dataset and get the results.\n",
            "Trained model\n",
            "Python\n",
            "Yolov7\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2071.txt\n",
            "\n",
            "Advanced AI for Road Cam Threat Detection\n",
            "\n",
            "Client: A leading tech firm in the Middle East\n",
            "Industry Type:  Security\n",
            "Services: Security services\n",
            "Organization Size: 100+\n",
            "Detect the threat level of accidents between a Pedestrian and a Car.\n",
            "Use Deeplearning Computer vision and logic to detect the threat level as defined by the Client.\n",
            "Linux 22.04\n",
            "Python\n",
            "Yolov7\n",
            "The technical challenge was sorted by testing, experimenting and later on finding and modifying an already existing repository to use as a baseline for our code for integration.\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2072.txt\n",
            "\n",
            "Advanced AI for Pedestrian Crossing Safety\n",
            "\n",
            "Client: A leading tech firm in the Middle East\n",
            "Industry Type:  Security\n",
            "Services: Security services\n",
            "Organization Size: 100+\n",
            "Traffic Signals are inefficient because even if there are no cars or no pedestrians on the road it still works on a timer and stops the traffic or pedestrian unnecessarily.\n",
            "We provide a Computer vision-logic to Manipulate the traffic signal to work such that it turns red only when x number of pedestrians are waiting to cross the signal.\n",
            "There was no existing solution and we had to create the logic from scratch.\n",
            "Researching Computer Vision. Learning new Techniques and Experimentation.\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2073.txt\n",
            "\n",
            "Advanced AI for Handgun Detection\n",
            "\n",
            "Client: A leading tech firm in the Middle East\n",
            "Industry Type:  Security\n",
            "Services: Security services\n",
            "Organization Size: 100+\n",
            "Detecting Handguns in images and videos.\n",
            "We use Yolov7 instance segmentation model to detect and provide coordinates for handguns.\n",
            "Trained model of yolov7 instance segmentation\n",
            "Python\n",
            "Yolov7_mask\n",
            "Retrieving handgun images in bulk from opensource.\n",
            "Found Openimages dataset with good amount of required images\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2074.txt\n",
            "\n",
            "Using Graph Technology to Create Single Customer View.\n",
            "\n",
            "Client: A leading retail firm in Newzealand\n",
            "Industry Type:  Retail\n",
            "Services: Retail business\n",
            "Organization Size: 100+\n",
            "Companies face issue of having a Single customer under various rows with slightly different information in the same database. This causes unwanted duplication and inaccurate statistics. It also results in inaccurate ad targeting and financial loss.\n",
            "We leverage graph technology to create a single customer view by using Complex cypher queries  and Graph Algorithms.\n",
            "We have an Azure VM on which we have installed the Neo4j Database. Deployment architecture is a single Instance because of using the Community version of the software.\n",
            "Node-Relationship model\n",
            "Data Analytics\n",
            "Data Engineering\n",
            "Data Science\n",
            "Neo4j\n",
            "AZURE\n",
            "Only 1 Difficulty was faced in this Project and that was to migrate data from Elasticsearch to Neo4j.\n",
            "Research and Experimentation.\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2075.txt\n",
            "\n",
            "Car Detection in Satellite Images\n",
            "\n",
            "Client: A leading retail firm in the USA\n",
            "Industry Type:  Retail\n",
            "Services: Retail business\n",
            "Organization Size: 100+\n",
            "The objective of this project was to detect cars in satellite images and highlight them using a bounding box.\n",
            "The client, Steffen Schneider, approached us with a requirement to develop a Python project that dealt in the field of computer vision. The main aim of the project was to detect cars present in a satellite image and highlight them using a bounding box. To achieve this, we decided to use the Darknet model and train it on Yolov4 dataset of cars in satellite images.\n",
            "We used Google Colab for coding and training the Darknet model. Kaggle was used to download the Yolov4 dataset of cars in satellite images. We preprocessed the dataset and trained the model on it. Once the model was trained, we tested it on sample satellite images and it worked perfectly fine. Finally, we created a script that detected the cars in an image and highlighted them using a bounding box.\n",
            "The final deliverable was a ipython Notebook presented on Google Colab.\n",
            "Google Colab, Kaggle, Slack(For Communication)\n",
            "Python\n",
            "Darknet(CV Model)\n",
            "Python programming, AI/ML.\n",
            "The main challenge we faced was related to the pre-processing of the Yolov4 dataset of cars in satellite images. The dataset was large and had to be cleaned and formatted before it could be used for training the model.\n",
            "The project was a success and the client was very happy with the final product. The car detection model worked perfectly fine on sample satellite images and could be used for further development of an application that could detect cars in real-time.\n",
            "https://colab.research.google.com/drive/1AoeHdZdpi0lWLf3X2G800J0VT_7wJtnE\n",
            "\n",
            "Here are my contact details:\n",
            "Skype: asbidyarthy\n",
            "WhatsApp: +91 9717367468\n",
            "Telegram: @asbidyarthy\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2076.txt\n",
            "\n",
            "Building a Physics-Informed Neural Network for Circuit Evaluation\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  Retail\n",
            "Services: Consulting\n",
            "Organization Size: 100+\n",
            "The objective of this project was to build a Physics Informed Neural Network (PINN) using TensorFlow, which could evaluate circuits based on the parameters provided through a MATLAB simulation.\n",
            "Mohamed provided us with a dataset generated from a MATLAB simulation of a circuit, consisting of various input parameters and the corresponding circuit performance outputs. We were tasked with developing a machine learning model that could accurately predict circuit performance based on the input parameters, while also incorporating the underlying physics principles that govern circuit behavior.\n",
            "Our team utilized Jupyter Notebook, Google Colab, Octave, and MATLAB to build the PINN. We used TensorFlow models to build the neural network and Microsoft Excel to clean and preprocess the data. Our team employed Python programming, TensorFlow, Pandas, and MATLAB skills to build the PINN. We did not use any databases for this project, nor did we use any web/cloud servers.\n",
            "The final deliverable was a functional PINN capable of evaluating circuits based on the provided parameters.\n",
            "Our team used Jupyter Notebook, Google Colab, Octave, MATLAB, and Microsoft Excel.\n",
            "The primary languages and techniques we used were Python programming, TensorFlow, and MATLAB.\n",
            "We used TensorFlow models to build the neural network for the PINN.\n",
            "Our team utilized Python programming, TensorFlow, Pandas, and MATLAB skills to build the PINN.\n",
            "We did not use any databases for this project.\n",
            "We did not use any web/cloud servers for this project.\n",
            "The project was very challenging since our team did not have a background in electrical engineering. It was difficult to understand the physics behind the circuit evaluation, and we faced issues when using MATLAB to provide data for the project.\n",
            "The PINN we built for Mohamed Zamil allowed for efficient circuit evaluation and improved the overall accuracy of the evaluation process.\n",
            "https://colab.research.google.com/drive/1HX37MP4Jcb39SWJgkE_5z5n1gQwqWmV9\n",
            "\n",
            "Here are my contact details:\n",
            "For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2077.txt\n",
            "\n",
            "Connecting MongoDB Database to Power BI Dashboard: Dashboard Automation\n",
            "\n",
            "Client: A leading tech firm in Newzealand\n",
            "Industry Type:  Retail\n",
            "Services: Retail business\n",
            "Organization Size: 100+\n",
            "Brodie Johnco had a MongoDB Database that he wanted to connect to a Power BI Dashboard. However, ODBC connectors were not working for his level of subscription, so he needed a cheaper workaround.\n",
            "Brodie Johnco had a MongoDB Database containing a large amount of data that he wanted to visualize in a Power BI Dashboard. He initially tried to use ODBC connectors to connect his database to Power BI, but ran into issues due to his level of subscription. We were brought in to help find a cheaper workaround.\n",
            "Our solution involved using Python to extract the relevant data from Brodie’s MongoDB Database. We used the Pandas library to create Dataframes, which we then uploaded to Azure Blob Storage as tables. We set up an Azure pipeline that ran a Python script every 30 minutes to update the tables with new data from the database.\n",
            "We used Brodie’s MongoDB Database keys to extract relevant Data Clusters as Pandas Dataframes. We then added them as tables to Azure Blob Storage and set up a Python script to an Azure pipeline that refreshed every 30 minutes. This allowed us to keep the data in sync and provide Brodie with up-to-date information for his Power BI Dashboard.\n",
            "The final deliverable was a readable CSV file that contained the converted data from the original JSON format.\n",
            "Jupyter Notebook, Google Colab, Power BI, MongoDB Compass, Microsoft Excel, Azure Blob Storage\n",
            "Python, Pandas, Azure Cloud Storage\n",
            "Python programming, Azure Cloud Storage, data extraction and manipulation\n",
            "MongoDB Database\n",
            "Azure Blob Storage\n",
            "The main challenge we faced was finding a way to connect Brodie’s MongoDB Database to his Power BI Dashboard without using ODBC connectors. We overcame this challenge by using Python and Azure Blob Storage to extract and store the relevant data.\n",
            "Our solution allowed Brodie to visualize his data in a Power BI Dashboard without having to pay for expensive ODBC connectors. The Azure Blob Storage solution we implemented was much more cost-effective and provided him with up-to-date information every 30 minutes.\n",
            "https://github.com/AjayBidyarthy/Brodie-Johnco\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2078.txt\n",
            "\n",
            "Data Transformation\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  Retail\n",
            "Services: Retail business\n",
            "Organization Size: 100+\n",
            "The objective of this project was to convert dirty JSON data present in a CSV file to a readable CSV file. The CSV file contained data in JSON format, which was split into columns in an Excel file, making it hard to read. The client wanted the data to be extracted and converted into a readable format to perform further analysis on it.\n",
            "Our client had provided us with a CSV file that contained data in JSON format, which was split into columns in an Excel file. The data was hard to read and understand, making it difficult to perform any analysis on it. Our objective was to extract the data, convert it to a readable format, and validate the JSON file to ensure that it was in a correct format. Finally, we had to convert the JSON data into a CSV file that could be easily read and analyzed.\n",
            "To extract the data, we used Python programming language and Pandas library. We extracted every piece of text present in the Excel sheet using Pandas and converted it into a readable text format. We then validated the JSON file with a JSON validator website to ensure that it was in the correct format. Finally, we used Pandas again to convert the JSON data into a CSV file that could be easily read and analyzed.\n",
            "To perform the conversion, we used Jupyter Notebook, Json Validator, and Microsoft Excel.\n",
            "The final deliverable was a readable CSV file that contained the converted data from the original JSON format.\n",
            "Jupyter Notebook, Json Validator, and Microsoft Excel.\n",
            "Python programming language and Pandas library.\n",
            "Python programming and Pandas data manipulation.\n",
            "The main technical challenge we faced during the project was dealing with dirty JSON data present in a CSV file that was split into columns in an Excel file. This made it hard to read and understand, and required extra effort to extract the data and convert it into a readable format.\n",
            "We solved the technical challenges by using Python programming language and Pandas library to extract and manipulate the data. We validated the JSON data using a JSON validator website to ensure that it was in the correct format. Finally, we used Pandas to convert the JSON data into a readable CSV file that could be easily analyzed.\n",
            "The business impact of this project was that the client was able to perform further analysis on the extracted data in a readable format, which was previously hard to read and understand.\n",
            "https://colab.research.google.com/drive/1yWDj8_HXu6hOYatrzWQ3ezqBxsUON3JY\n",
            "\n",
            "For project discussions and daily updates, would you like to use Slack, or Skype or Whatsapp? Please recommend, what would work best for you.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2079.txt\n",
            "\n",
            "E-commerce Store Analysis – Purchase Behavior, Ad Spend, Conversion, Traffic, etc…\n",
            "\n",
            "Client: A leading retail firm in the USA\n",
            "Industry Type:  Retail\n",
            "Services: Retail business\n",
            "Organization Size: 100+\n",
            "To create a well-designed and informative dashboard for Symbiome e-commerce website using data sourced from Bigquery Database, Google Ads, Google Analytics, and Facebook Ads.\n",
            "Our client, Arik Oganesian, approached us with a requirement to create a dashboard for his friend’s e-commerce website, Symbiome. The dashboard needed to be visually appealing and provide comprehensive insights into the website’s performance. We sourced data from various sources such as Bigquery Database, Google Ads, Google Analytics, and Facebook Ads. To create the dashboard, we used Google Data Studio and Google Sheets to link the data sources. We also used SQL language to extract data from Bigquery Database. The client specifically asked for cohort retention and cohort revenue charts to be included in the dashboard. With our expertise in data analytics, we were able to fulfill the client’s requirements and provide a dashboard that helped the client make data-driven decisions.\n",
            "We used Google Data Studio to create the dashboard and Google Sheets to link the data sources. To extract data from Bigquery Database, we used SQL language. We created a set of charts including cohort retention and cohort revenue charts to fulfill the client’s requirements.\n",
            "Symbiome E-commerce Dashboard\n",
            "Google Data Studio and Google Sheets\n",
            "SQL for Bigquery\n",
            "Data analytics\n",
            "Bigquery Database\n",
            "One of the major challenges we faced was extracting data from Bigquery Database using SQL language. However, we were able to overcome this challenge by using our expertise in data analytics.\n",
            "To solve this issue, we used Google Data Studio and Google Sheets to link the data sources. We also used SQL language to extract data from Bigquery Database. By using these tools, we were able to integrate the data from different sources and create a single comprehensive dashboard that met the client’s requirements.\n",
            "The dashboard we created provided a clear view of the website’s performance and helped the client to make data-driven decisions. This resulted in an increase in website traffic and revenue.\n",
            "https://lookerstudio.google.com/u/1/reporting/c25c55ae-8052-4166-b363-347a2f8059da/page/SI6uC\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2080.txt\n",
            "\n",
            "KPI Dashboard for Accountants\n",
            "\n",
            "Client: A leading accounting firm in the USA\n",
            "Industry Type:  Finance and Accouting\n",
            "Services: Accounting and financial services\n",
            "Organization Size: 100+\n",
            "The objective of the project was to create a simple and easy-to-use dashboard for the accounting firm Tech 4 Accountants to track their highest performers, target number of clients, current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.\n",
            "Our client, Andrew Lassise, wanted a KPI dashboard for Tech 4 Accountants that would help them track their business performance easily. The dashboard needed to have various charts and tables that would display important KPIs in a visually appealing manner.\n",
            "To achieve our client’s objectives, we used Google Data Studio and Google Sheets to create a visually appealing and easy-to-use KPI dashboard. We created various charts and tables that displayed the KPIs that our client wanted to track. We used Google Sheets to store the data and created visualizations using Data Studio.\n",
            "We delivered a KPI dashboard for Tech 4 Accountants that included charts and tables for tracking the highest performers, target number of clients, current week sales, tickets, customer satisfaction, leads, conversion, company records, and finances.\n",
            "Google Data Studio and Google Sheets\n",
            "Data Analytics\n",
            "There were no major technical challenges faced during the project execution as the data was stored in Google Sheets, and Data Studio allowed us to easily create visualizations using the data.\n",
            "No major technical challenges were encountered, and the project was completed smoothly.\n",
            "The KPI dashboard that we created for Tech 4 Accountants allowed them to track their business performance easily and make informed decisions. The dashboard helped them identify areas where they needed to improve and make changes to their business strategy accordingly.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2081.txt\n",
            "\n",
            "Return on Advertising Spend Dashboard: Marketing Automation and Analytics using ETL and Dashboard\n",
            "\n",
            "Client: A leading ad firm in India\n",
            "Industry Type:  Ads\n",
            "Services: Ads, Marketing, and Promotions\n",
            "Organization Size: 100+\n",
            "The main problem that was addressed in this project was the manual calculation of Return on Advertising Spend (ROAS) due to the lack of a centralized platform for running ads. The client’s ads were spread across multiple revenue generating platforms, including Google Adsense, Adx, and Ezoic, while the spending was managed through the Google Ads Platform. At that time, the client lacked a centralized dashboard or website that could effectively calculate ROAS by integrating revenue and cost streams. This fragmentation made it challenging for the client to track and evaluate the effectiveness of their advertising campaigns. Therefore, a comprehensive solution was developed and implemented, providing a centralized platform for calculating ROAS, aligning revenue and cost data from various sources, and enabling informed decision-making regarding advertising investments.\n",
            "We developed a comprehensive solution to address the challenges faced by the client in calculating Return on Advertising Spend (ROAS) and centralizing their advertising data. The solution involved collecting data from four different APIs: Google Ads API for spending data, Google Adsense API, Ad Manager API, and Ezoic data for revenue data. To ensure compatibility, we utilized an Extract, Transform, Load (ETL) tool to convert the data received from each API, which was in different formats, into a standardized format storing them Pandas Dataframe for both revenue and spending data.\n",
            "The transformed data was then stored in a Postgres database for easy access and management. To automate the data extraction process, we implemented an ETL script that runs twice daily via cronjob on a Digital Ocean VM, ensuring the latest data is always available.\n",
            "Moreover, we designed a backend API using the Flask framework. This API fetched the required data from the Postgres DB, allowing users to retrieve relevant information efficiently.\n",
            "Finally, we implemented a ROAS Dashboard frontend to display the calculated ROAS using the fetched values. The dashboard provided a visually appealing and intuitive interface for users to track and monitor their advertising performance. With our solution in place, the client could now easily monitor ROAS over time, access consolidated data, and make informed decisions regarding their advertising investments.\n",
            "The solution architecture involved a multi-step process to address the challenges faced by the client in calculating ROAS and centralizing their advertising data. Data was collected from various APIs, including Google Ads API, Google Adsense API, Ad Manager API, and Ezoic data, and transformed into a standardized format using an ETL tool.\n",
            "The transformed data was stored in a Postgres database, and a backend API was developed using the Flask framework to fetch the required data. The calculated ROAS was then displayed on a Next Js Dashboard, providing users with an intuitive interface to track and analyze their advertising performance.\n",
            "Python 3.9\n",
            "Flask API\n",
            "DigitalOcean Droplet\n",
            "Functional Programming in Python\n",
            "ETL Tool\n",
            "Python\n",
            "Git\n",
            "Deployment\n",
            "Data Engineering\n",
            "Web Development using Next js\n",
            "We used PostgreSQL database for the project.\n",
            "Digital Ocean Droplet\n",
            "Some of the technical challenges encountered were:\n",
            "1. Ensuring data integrity: Implemented checks, cleansing, and validation to maintain the accuracy and reliability of the data.\n",
            "2. Docker image deployment on VM: Configured VM to support Docker Image for ETL and deployed the image for seamless execution.\n",
            "3. Setting up automated ETL pipeline: Automated data extraction, transformation, and loading processes for efficient data management via cronjob.\n",
            "4. Adding SSL certificate to backend API: Secured backend API with SSL certificate, enabling encrypted communication for enhanced data protection.\n",
            "The implemented solution had a significant positive impact on the client’s business. By providing a centralized platform for calculating ROAS and integrating data from multiple revenue-generating platforms, the client gained valuable insights into the effectiveness of their advertising campaigns. The availability of real-time, consolidated data enabled informed decision-making regarding advertising investments. The user-friendly interface of the RAOS Dashboard allowed the client to easily track and monitor their advertising performance, leading to improved campaign optimization and potentially higher returns on advertising spend. Overall, the solution streamlined the client’s advertising operations, resulting in increased efficiency and improved business outcomes.\n",
            "Here are the project snapshots:\n",
            "https://roasing.com/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2082.txt\n",
            "\n",
            "Ranking customer behaviours for business strategy\n",
            "\n",
            "Client: A Leading Retail Firm in the USA\n",
            "Industry Type: Retail\n",
            "Services: Retail Business\n",
            "Organization Size: 100+\n",
            "Create an API service that will parse text, include comments, analyse the remarks, assign a score based on sentiment or other criteria, etc. Feed it comments, and it should analyse the syntax and sentiment of the comments as well as extract key terms to add to the extended meta data of that model. In order for us to know a user’s behaviour, personal information, and more meta data about their interests\n",
            "Created a flask API, that will take comments as input and will textual analysis as follows:\n",
            "CommentScoringAPI that will take comments/reviews as input, and do the textual analysis on the given comment and will return the Comment Score based on counts of spell and grammar errors, sentiments, hot keywords.\n",
            "Numpy, pandas, flask, NLTK, Spacy (Keyword Extraction), language tool python (spell and grammar check), flair (Sentimental Analysis)\n",
            "Python\n",
            "Client have a user schema that contain all the information of users that have visited there platform, and he/she want to build a Script that will take all the reviews of a certain User as input and than will do textual analysis on all the comments of the user , by textual analysis we mean Spell and Grammar Check, Sentimental Analysis, and Keywords extraction. Based on these factors our Script scored each user and helped Client to understand his/her users well.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2083.txt\n",
            "\n",
            "Algorithmic trading for multiple commodities markets, like Forex, Metals, Energy, etc.\n",
            "\n",
            "Client: A Leading Trading Firm in the USA\n",
            "Industry Type: Finance\n",
            "Services: Trading, Consulting, Software\n",
            "Organization Size: 100+\n",
            "A Trading site will have all the required features, allowing users to trade in multiple commodities markets, like Forex, Agriculture, Metals, Energy etc.\n",
            "Designed the website with technical indicators, and the ability to trade in live market, plus allows the user to create his/her own strategy to backtest. Functionalities like all types of technical indicators:\n",
            "Strategies are specific scripts, which are able to send, modify, execute, and cancel buy or sell orders and simulate real trading right on your chart. Backtesting is the process of recreating the work of your strategies on historical data, essentially all of your past strategic work. Forward testing allows for the recreation of your strategy work in real time, all while your charts refresh their data.\n",
            "A Fully functional trading platform that lets you customize technical indicators, create charts, and analyse financial assets. These indicators are patterns, lines, and shapes that millions of traders use every day. Platform designed is entirely browser-based, with no need to download a client. Allowing the user to use all types of indicators:\n",
            "Numpy\n",
            "pandas\n",
            "Python\n",
            "Clients want a social media network, analysis platform, and mobile app for traders and investors. So we designed a website with all the client’s requirements, where traders, investors, educators, and market enthusiasts can connect to share ideas and talk about the market. By actively participating in community engagement and conversation, you can accelerate your growth as a trader, and your ability to trade in the live market, plus allows the user to create his/her own strategy to backtest. A Fully functional trading platform that lets you customize technical indicators, create charts and analyze financial assets. These indicators are patterns, lines, and shapes that millions of traders use every day. Platform designed is entirely browser-based, with no need to download a client. Allowing the user to use all types of indicators\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2084.txt\n",
            "\n",
            "Trading Bot for FOREX\n",
            "\n",
            "Client: A Leading Trading Firm in the USA\n",
            "Industry Type: Finance\n",
            "Services: Trading, Consulting\n",
            "Organization Size: 100+\n",
            "PyTrader\n",
            "numpy\n",
            "pandas\n",
            "Python (Automation)\n",
            "Mql4 (To save tick data)\n",
            "Client requirements were  to automate his forex trading strategy  on Meta Trader4 terminal, so that he doesn’t have to bother trading anymore, the Python script we designed to not only do it, plus it offers a safe exit point for Ongoing Trades, that saved the client’s money and time.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2085.txt\n",
            "\n",
            "Python model for the analysis of sector-specific stock ETFs for investment purposes\n",
            "\n",
            "Client: A Leading Investment Firm in the USA\n",
            "Industry Type: Finance\n",
            "Services: Investment, Consulting\n",
            "Organization Size: 100+\n",
            "Have an existing Python model that has been built for the analysis of sector-specific stock ETFs for investment purposes. Need to update the existing selection criteria to adjust the selection filter and add a screening criterion that drops off one or more of the proposed holdings, and to have the ability to adjust the parameters of the selection criteria to test different variables.\n",
            "The 2 in 4 Fundamental model screens a fundamental ranking of stock market sectors, picks the top ranked holding and continues to hold that sector as long as it remains in the top four rankings.  The model holds two positions at a time.  The sector ranking data is in the wcm5.xlxs file.  We input data from the PRICES.CSV file to pull up monthly returns.  When I go to run the program, I use the 2_in_4_New.py and that give me the current rankings for both the fundamental and technical rankings.\n",
            "Sometimes a sector is ranked as being fundamentally attractive because it has become cheaper because of problems going on within an industry.  What I would like to do is to test out a way of screening out a sector based upon poor performance over a lookback period.  Here is what the new model would do.\n",
            "An Updated, Optimised Python script that will filter and return Technical and Financial holdings, with a Price filter that will do price analysis on a certain lookback period.\n",
            "Numpy\n",
            "pandas\n",
            "itertools,\n",
            "combinations\n",
            "permutations\n",
            "Python\n",
            "The client now can get more than 2 Financial and technical holdings , up to maximum 5 holdings for both Technical and Financial, plus the holdings were more accurate because of the new added Price Filter that will Exclude the holding that has the weakest performance over a specify lookback period, default 52 weeks. It boosted the Client’s profit because of the more accurate and optimised functional filters.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2086.txt\n",
            "\n",
            "Medical Classification\n",
            "\n",
            "Client: A Leading Tech Firm in the USA\n",
            "Industry Type: IT Consulting\n",
            "Services: Software, Consulting\n",
            "Organization Size: 100+\n",
            "We have given an excel sheet of medical research paper text and provided some phrases to identify research papers that can be used for future medical research. If the phrase is not present in a research paper then it will not be used for research. After annotation, we need to find the best ML/DL model to train research data and evaluate the model on test data.\n",
            "We have created a python script that can compare all medical research paper text to research phrases and annot 0 if research phrases are not present in a medical research paper and 1 if research phrases present in medical research paper.\n",
            "After annotation we have trained different machine learning and deep learning models like Bert base uncased using Tensorflow, bert large, XGBoost Classifier, Random Forest Classifier and Logistic Regression. Among these models we have chosen the best accuracy  parameters model. In our case the bert-base model performed good and gave 95% test accuracy.\n",
            "ML/DL model which is trained on medical research classification data to classify other medical research papers.\n",
            "Google Colab notebooks, Tensorflow, PyTorch, Transformers, MS Excel\n",
            "Python, Machine learning, Deep learning, Data Science, Natural Language Processing (NLP).\n",
            "Tensorflow-Bert model, PyTorch LSTM model, Random Forest Classifier, XGBoost Classifier, Logistic Regression.\n",
            "Machine Learning, Deep learning, NLP, Python programming.\n",
            "used ms excel data\n",
            "There are various technical challenges faced during project execution:\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2087.txt\n",
            "\n",
            "Design & Develop BERT Question Answering model explanations with visualization\n",
            "\n",
            "Client: A Leading Tech Firm in the USA\n",
            "Industry Type: IT Consulting\n",
            "Services: Software, Consulting\n",
            "Organization Size: 100+\n",
            "We need to use a pre-trained bert question answering model and create a notebook that has explanations of model’s working with some visuals of bertviz, allennlp and gradient values.\n",
            "A notebook which has an explanation of the bert question answering model using some visualization.\n",
            "Google colab notebooks, Tensorflow, Bertviz, Allennlp, Transformers\n",
            "Python programming language, Deep learning, NLP, Data Visualization\n",
            "Pretrained bert-base-uncased model and distilbert model (both trained on squad2 dataset)\n",
            "Data visualization, Deep learning, NLP, python\n",
            "Among these models we kept the best one.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2088.txt\n",
            "\n",
            "Design and develop solution to anomaly detection classification problems\n",
            "\n",
            "Client: A Leading Tech Firm in the USA\n",
            "Industry Type: IT Consulting\n",
            "Services: Software, Consulting\n",
            "Organization Size: 100+\n",
            "We need to create a notebook with solutions to binary classification-related anomaly detection problems. We need to use machine learning and deep learning models which have greater than 90% accuracy.\n",
            "We created a notebook for anomaly detection. We used 10 to 15 machine learning and deep learning models but only  3 different types of auto encoder models that were giving greater than 90% accuracy. We trained all 3 models on one classification data which have anomalies and evaluated trained models on test data.\n",
            "A notebook that has solutions for anomaly detection related classification problems and accuracy should be above 90%.\n",
            "Google colab notebooks, Tensorflow, Google drive\n",
            "Python programming language, Machine learning, Deep learning, Data analysis and Data visualization.\n",
            "Auto Encoder and Variational Auto Encoder\n",
            "Python, Data Analysis, Data visualization, Machine learning, Deep learning.\n",
            "MS Excel\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2089.txt\n",
            "\n",
            "An ETL Solution for Currency Data to Google Big Query\n",
            "\n",
            "Client: A Leading Tech Firm in the USA\n",
            "Industry Type: IT Consulting\n",
            "Services: Software, Consulting\n",
            "Organization Size: 100+\n",
            "We have given a pure-clear API and a google cloud account. We need to fetch currency data from that pure-clear API using python and need to store fetched data in Google Cloud Bigquery.\n",
            "We also need to automate the above process like the process runs on a daily basis and update the currency data on Bigquery.\n",
            "We have created a python program that can fetch pure-clear API data. The API data was in JSON format but we needed table format so we used python package pandas. We converted json data to tabular format using pandas. After that, we connected python code to google cloud using google’s authentication module and then stored data frame (table) directly to BigQuery using the “.to_gbq” method.\n",
            "We also need to run the above process daily to update new data in BigQuery. For this Google cloud provides a “Cloud function” tool. In this, we can create a function and set up their running process. So we created a function and attached the above code to that function and set up a cloud function to run daily.\n",
            "A Google cloud function that runs daily and updates data on Google BigQuery\n",
            "Cloud function, BigQuery of Google Cloud, Google Colab notebook, Python programming, Pandas\n",
            "Python language and pandas module\n",
            "Python programming, Data handling, Google Cloud\n",
            "Google Cloud BigQuery\n",
            "Google Cloud Server\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2090.txt\n",
            "\n",
            "ETL and MLOps Infrastructure for Blockchain Analytics\n",
            "\n",
            "Client: A Leading Blockchain Tech Firm in the USA\n",
            "Industry Type: AR/VR\n",
            "Services: Metaverse, NFT, Digital Currency\n",
            "Organization Size: 100+\n",
            "ETL and MLOps Infrastructure for Blockchain Analytics this entire project completes in 4 outlines and stages. In the first segment data scraping for the price of the cryptocurrency. The second stage is, Loading the data into the Microsoft MYSQL server and Transforming data into the required shape for the automated process data Load into the Amazon RDS tool management service which knows as the Amazon relational database service, and creating DB instances (DB instance class – db.t3.small).\n",
            "In the fourth stage, built the FastAPI for the get data to the fingertips and easily accessible for the client because it reduces the time to fetch the price of a particular cryptocurrency with a single click, and increases the efficiency of understanding.\n",
            "This Project Module develops according to the Client’s Requirements which involves Data extraction of Cryptocurrency data from a given URL by the Client, it also changes the data format, and attributes nomenclature according to the requirements. After extracting the data its loads into Microsoft MYSQL Server for the transformation of data and for full automation process, used Amazon RDS and built the FastAPI.\n",
            "–  Data Scraping code using Python\n",
            "–  ETL code for extracting, Transform and Loading into Microsoft MYSQL server\n",
            "–  AWS RDS (db.t3.samll) instances for storing data and for deployment\n",
            "–  Built FastAPI for getting the price of cryptocurrency\n",
            "– VC code and Google Collab\n",
            "– Microsoft MYSQL server\n",
            "– AWS RDS services\n",
            "– Data scraping using python\n",
            "– ETL setup\n",
            "– Aws web services\n",
            "– FastAPI using Python\n",
            "– Microsoft MYSQL server\n",
            "– Aws RDS (Amazon Relational Database services)\n",
            "-AWS RDS services\n",
            "127.0.0.1:62190\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2091.txt\n",
            "\n",
            "An agent-based model of a Virtual Power Plant (VPP)\n",
            "\n",
            "Client: A Leading Energy Firm in the USA\n",
            "Industry Type: Energy\n",
            "Services: Power, Energy, Distribution\n",
            "Organization Size: 5000+\n",
            "To create an agent based model of a virtual power plant in Netlogo. To see the function of multiple such power plants that worked simultaneously. These power plants created and supplied energy based on a demand parameter that can be controlled by the observer\n",
            "-Netlogo\n",
            "– python\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2092.txt\n",
            "\n",
            "Transform API into SDK library and widget\n",
            "\n",
            "Client: A Leading Tech Firm in the USA\n",
            "Industry Type: IT\n",
            "Services: Consulting, Marketing, Healthtech\n",
            "Organization Size: 500+\n",
            "Convert API documentation into SDK library and widget. Expected deliverables are SDK library and widgets for\n",
            "API documentation is available for a tool that allows customers to type in their medication and find the cheapest price near them. For partners who want to have it on their own site, currently using the API documentation but would like to ultimately be able to send them an embeddable widget that incorporates the tool on their site\n",
            "We created a flutter widget that uses  SDK libraries that allows the customer to type their medication and find the cheapest price near them.\n",
            "This widget can be embedded in their web, android and IOS applications\n",
            "1)SDK Library/Widget\n",
            "2)Sample flutter application\n",
            "Flutter\n",
            "Dart\n",
            "1)Knowledge of dart language\n",
            "2)flutter app developing\n",
            "1 )Problems while fetching details of drugs and pharmacies\n",
            "2) Showing details of drugs and pharmacies in the widget\n",
            "All technical challenges are solved by proper communication with the client and by logical analyzing of data\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2093.txt\n",
            "\n",
            "Integration of a product to a cloud-based CRM platform\n",
            "\n",
            "Client: A Leading Logistics Firm Worldwide\n",
            "Industry Type: Logistics\n",
            "Services: Import, Export, Supply Chain, Logistics, Trades\n",
            "Organization Size: 500+\n",
            "The main challenge faced by the team was the integration of the two systems themselves.\n",
            "Since one-by-one entering of records into each module is a mundane task and a waste of valuable time we proposed the automation using APIs.\n",
            "The challenge was divided into two milestones and sub-tasks for each.\n",
            "1. First was the ingestion of existing data into the cloud-based CRM platform.\n",
            "2. Second was the question of automating the process of adding newer records to the cloud platform.\n",
            "The client has been provided with python scripting handling bulk data ingestion to CRM and also the script to handle daily synchronization of data.\n",
            "– Python\n",
            "– MySQL Database\n",
            "– Postman\n",
            "– TeamViewer\n",
            "– Automation\n",
            "– 3rd party APIs\n",
            "– Authentication methods\n",
            "– Multi-Threading of function calls\n",
            "– bat Scripts for easier running of scripts for the client\n",
            "Python Frameworks like requests to build own custom client for consumption of APIs.\n",
            "Python Programming, Mult-threading, APIs\n",
            "The client provided a MySQL instance.\n",
            "Zoho\n",
            "– Writing own client-side API-consumption code handling API calls from Authentication and Other Operations as per task requirements.\n",
            "– Debugging of API responses was messy.\n",
            "– Multiple alternatives were discussed and implemented in python like conditional refreshing of API tokens.\n",
            "– Automation of daily synchronization handled by use of time deltas.\n",
            "– Logging of all operations to efficiently handle errors in the future.\n",
            "– Automated workflow of the client\n",
            "– No need for dull tasks like data entry to CRM modules everything is taken care of using logic.\n",
            "\n",
            "https://www.exportgenius.in/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2094.txt\n",
            "\n",
            "A web-based dashboard for the filtered data retrieval of land records\n",
            "\n",
            "Client: A Leading Real Estate Firm in the USA\n",
            "Industry Type: Real Estate\n",
            "Services: Land, Infrastructure, Real Estate, Investment\n",
            "Organization Size: 100+\n",
            "The client’s own raw database needed to be converted into a dynamic web application with modern features like user management and subscription where users could explore land records as per their wish.\n",
            "Created the web application as per client needs.\n",
            "Added user functionality to handle signup/logins and added authorization middlewares to protect routes from unwanted access.\n",
            "Transformed raw data into a meaningful NoSQL-based database with a proper schema being served as an instance on a cloud service named\n",
            "‘ MongoDB Atlas ‘.\n",
            "Pushed code to the required GitHub repository.\n",
            "– Vanilla javascript\n",
            "– Javascript Frameworks ( Nodejs, express , cors )\n",
            "– Postman\n",
            "– JavsScript\n",
            "– Backend Service setup ( express, cors , js )\n",
            "– Fronted logic setup ( HTML , CSS , JavaScript , Jquery )\n",
            "Backend: An API service created to handle land records database and queries made by users.\n",
            "Frontend: A frontend client is available as a web application where users can signup and access land records.\n",
            "JavaScript Programming, APIs, JavaScript Frameworks ( NodeJS, Express  , cors ) , Web Design, NoSQL querying in MongoDB.\n",
            "MongoDB (NoSQL)\n",
            "MongoDB Atlas\n",
            "– UI component creation\n",
            "– User authorization middleware creation\n",
            "– Querying data in NoSQL\n",
            "– Created and extended UI components to handle filters like owners, date fields, and area ranges on land records.\n",
            "– API and Frontend are separately built for easier team management of tasks.\n",
            "– Using a cloud-based MongoDB instance provided support for teams to work without any problems with accessibility.\n",
            "– Created a platform for clients’ business.\n",
            "– Transformed his raw data into meaningful business applications.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2095.txt\n",
            "\n",
            "Integration of video-conferencing data to the existing web app\n",
            "\n",
            "Client: A Leading Tech Firm in the USA\n",
            "Industry Type: IT & Consulting\n",
            "Services: Software, Business Solutions, Consulting\n",
            "Organization Size: 200+\n",
            "Integration of 3rd party APIs to client’s platform.Client required meeting/conference data from sites like gotomeeting/zoom.\n",
            "Using APIs fetched data from different platform and rendered data into client’s application.\n",
            "Modifed web application with a UI to handle form data accepting dates as a timeframe – which then makes a request to the API being handled at server end and returns the meeting data from the required source.\n",
            "Pushed code to client’s github repository.\n",
            "– Python\n",
            "– Postman\n",
            "– Automation\n",
            "– 3rd party APIs\n",
            "– Authenication methods\n",
            "– Multi-Threading of function calls ( authentication of api client )\n",
            "– UI component design to get dates from user-end\n",
            "Python Framework- Django , requests\n",
            "Python Programming, APIs , Multi-threading , Web Developement\n",
            "Default project postgreSQL\n",
            "Heroku\n",
            "– UI creation for handling form data\n",
            "– Managing and Validating form data to process request at server end\n",
            "– Created autmated functions as views in django to handle requests made to video-conferencing platform.\n",
            "– Which then returns meeting data as per user’s wish.\n",
            "– Instead of extracting meeting data and adding it to all users\n",
            "any authorized user can get meeting data as his wish.\n",
            "https://www.codanalytics.net/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2096.txt\n",
            "\n",
            "Design & develop an app in retool which shows the progress of the added video\n",
            "\n",
            "Client: A Leading Tech Firm in the USA\n",
            "Industry Type: IT & Consulting\n",
            "Services: Software, Business Solutions, Consulting\n",
            "Organization Size: 200+\n",
            "\n",
            "The objective was to develop a progress bar that can help costumes to estimate the analytics of the video.\n",
            "App in retool\n",
            "Retool\n",
            "SQL\n",
            "SQL\n",
            "SQL Database\n",
            "Client wanted date filter and a video category filter but this data was not there in added video table\n",
            "We had to join multiple data so that we can get category column and date column for applying filter\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2097.txt\n",
            "\n",
            "Auvik, Connectwise integration in Grafana\n",
            "\n",
            "Client: A Leading Tech Firm in the USA\n",
            "Industry Type: IT & Consulting\n",
            "Services: Software, Business Solutions, Consulting\n",
            "Organization Size: 200+\n",
            "Get statistics such as uptime,  availability, cpu throughput etc. from Auvik and Connectwise and make a dashboard from it in Grafana.\n",
            "Unlike many technologies for which plugins are readily available in Grafana, there are none for auvik and Connectwise. So our task was to device a solution through which all the data from Auvik and Connectwise can be fed to Grafana. This data then would be used to plot graphs in Grafana.\n",
            "Grafana\n",
            "Postgres\n",
            "Vs Code\n",
            "AWS\n",
            "Postman\n",
            "Python\n",
            "bash\n",
            "Python\n",
            "networking\n",
            "Data visualisation\n",
            "Postgres\n",
            "Amazon Web Services (AWS)\n",
            "Since, the data received from Auvik was in Json fromat, our first approach was to use Grafana’s built-in Json plugin. But this wasn’t working since, the data received from Auvik was multi-dimensional when the Json plugin required One dimensional data.\n",
            "The above challenge was addressed by transforming the multi- dimensional data into one dimensional when it was store in a python variable. This transformed data was then inserted into Postgres.\n",
            "https://github.com/AjayBidyarthy/Henry-Pardo\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2098.txt\n",
            "\n",
            "Data integration and big data performance using Elasticsearch\n",
            "\n",
            "Client: A Leading Tech Firm in the USA\n",
            "Industry Type: IT & Consulting\n",
            "Services: Software, Business Solutions, Consulting\n",
            "Organization Size: 200+\n",
            "Migrate existing databases from Postgres to elastic search since Elasticserach performs better in search operations. In addition to this, all of the backend javascript also needed to be changed in order to query the new elasticsearch database.\n",
            "The client’s website was a visualization tool. It also had GUI to add filters. To make the visualizations, at least 50,000 records needed to be pulled from the Postgres database whose size would be around 200mbs. This would take a lot of time (nearly 20-30 secs). Adding filters would take additional time. So our task was to move the entire database over to Elasticsearch from postgres since it is way more faster in search operations and also filtering data. Since the database was changed, we also had to write new backend code that would now query the Elasticsearch database.\n",
            "Elasticsearch\n",
            "Postman\n",
            "Kibana\n",
            "Logstash\n",
            "Python\n",
            "Javascript\n",
            "Amazon Web Services\n",
            "Postgres\n",
            "Docker\n",
            "Git Bucket\n",
            "Github\n",
            "Javascript\n",
            "Json\n",
            "Domain-Specific Language for elasticsearch\n",
            "bash\n",
            "Elasticsearch query knowledge\n",
            "Postgres query knowledge\n",
            "Networking\n",
            "Javascript\n",
            "Backend web stack\n",
            "Postgres\n",
            "Elasticsearch\n",
            "Amazon Web Services (AWS)\n",
            "To solve the above mentioned problem, we used gzip in the request url’s header. This significantly reduced the execution times.\n",
            "Earlier postgres infrastructure which took around 20-30 secs now too consistently less than 10 secs to perform filter and search operations. This would contribute to a better user experience.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2099.txt\n",
            "\n",
            "Web Data Connector\n",
            "\n",
            "Client: A Leading Marketing Tech Firm in Australia\n",
            "Industry Type: Marketing\n",
            "Services: Marketing Solutions\n",
            "Organization Size: 50+\n",
            "To make a software code that takes data from a source and ingests it into a database present on a server. The scripts should automatically execute after regular intervals of time.\n",
            "The client had several data sources that were updated with new data regularly. The client wanted software that triggers itself automatically and takes data from those data sources and ingests it into a database that is hosted on a Linode server. Also, the date parameters in the query should be changed dynamically using the current date. Further, we had to assist in setting up the Tableau BI tool on the client’s PC and connect the Postgres database to the tableau.\n",
            "Linode server\n",
            "VS Code\n",
            "Python\n",
            "Bash\n",
            "PSQL.\n",
            "Python programming\n",
            "Postgres SQL\n",
            "Linux scripting\n",
            "Postgres\n",
            "Linode\n",
            "This solution helps in maintaining a copy of all data sources inside our Postgres database. Also, the data is 24/7 available. Since data inside the Postgres is updated regularly, graphs in the tableau are also up to date.\n",
            "https://github.com/X360pro/Web-connector-for-tableu\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2100.txt\n",
            "\n",
            "An app for updating the email id of the user and stripe refund tool using retool\n",
            "\n",
            "Client: A Leading Healthcare Tech Firm in the USA\n",
            "Industry Type: Healthcare\n",
            "Services: Healthcare Solutions\n",
            "Organization Size: 200+\n",
            "The client needed two apps in retool\n",
            "We create the following two apps in retool\n",
            "Apps in retool\n",
            "We have not used any models\n",
            "The main challenge was creating a full payment option using stripe API. If the customer has already received a partial amount then while performing a full refund the refund amount was always greater than the balance amount\n",
            "To solve the full payment option issue, we calculate the balance amount and provided that amount to the full payment event in retool\n",
            "Using this apps it’s easy for the client to update the email id of the customer and refund the customers client can refund into two option full payment and partial payment\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2101.txt\n",
            "\n",
            "An AI ML-based web application that detects the correctness of text in a given video\n",
            "\n",
            "Client: A Design & Media firm in the USA\n",
            "Industry Type: Marketing\n",
            "Services: Consulting, Software, Marketing Solutions\n",
            "Organization Size: 100+\n",
            "Create a python web application that detects the text and checks the spelling of written text in the videos and prints the count of wrong spelling in the end\n",
            "Developing a dockerized Django web application for detecting the text and checking the spelling of written text in the video and printing the count of wrong spelling in the end and deploying the application on google cloud\n",
            "We have created a python web application with Django framework when user uploads the video the application run keras-ocr model on each frame of the video and keep the count of the wrong words at the end it provides the video with the bounding box around the words. For correct words it creates green bounding box and for wrong words it creates red bounding box and also it provides the summation of count of wrong words.\n",
            "Deployed dockerized web application on google cloud which generate video with bounding box around texts\n",
            "We have used keras-ocr model for detecting the text form the video and creating the bounding box around the words\n",
            "Google cloud\n",
            "http://34.68.134.64/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2102.txt\n",
            "\n",
            "Website Tracking and Insights using Google Analytics, & Google Tag Manager\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type: Marketing\n",
            "Services: Consulting, Software, Marketing Solutions\n",
            "Organization Size: 400+\n",
            "The project objectives are as follows:\n",
            "This project includes assisting businesses with digital analysis for their marketing.Digital analytics allows you to stand back, get the big picture, and see what is working and what isn’t in your overall strategy so you can adjust. The importance of digital analytics is that it allows for a data-driven approach to marketing, and as such it can produce better results.\n",
            "The primary objective of the project is to help the businesses in knowing their target audience, understanding the trends in digital marketing, and providing insights on the analytics part of their website performance. Use the digital analytical data to determine if your business’ aims are in line with the customer’s wants and needs. As the picture of the customer’s needs unfolds, adjust the objectives accordingly.\n",
            "The main aim of this project is to assist the businesses to improve their website performance with the use of technologies like Google Analytics, Google Tag Manager and dashboards built on Whatagraph.\n",
            "Google Analytics:\n",
            "Google Analytics is integral to tracking and measuring data from a number of digital platforms, but especially web metrics and customer behaviour. For example, through Google Analytics, you can see when people drop out of the buying process, perhaps they abandon while on the cart page, which would then inform your decisions on how to improve the check-out process.\n",
            "Because Google Analytics measures traffic from a variety of devices and sources and integrates with other online platforms, such as Google Ads, it is a handy tool to get an overview of your business’s digital analytics.\n",
            "Google Tag Manager:\n",
            "Google Tag Manager is a tag management system (TMS) that allows you to quickly and easily update measurement codes and related code fragments collectively known as tags on your website or mobile app. Once the small segment of Tag Manager code has been added to your project, you can safely and easily deploy analytics and measurement tag configurations from a web-based user interface.\n",
            "When Tag Manager is installed, your website or app will be able to communicate with the Tag Manager servers. You can then use Tag Manager’s web-based user interface to set up tags, establish triggers that cause your tag to fire when certain events occur, and create variables that can be used to simplify and automate your tag configurations.A Tag Manager container can replace all other manually-coded tags on a site or app, including tags from Google Ads, Google Analytics, Floodlight, and 3rd party tags.\n",
            "Whatagraph Dashboards:\n",
            "The whatgraph dashboards previews the important metrics related to the website including conversions, events, number of users and performance about ads and campaigns by the website. This dashboard helps in drawing some of the useful insights for the website notifying the strengths,gains and areas of improvement.\n",
            "Main deliverables for the project are:\n",
            "The main technical challenge faced was that any changes in Google Analytics are operational after 24 hrs. Thus, we can’t judge if the setup works as per required.\n",
            "We had to wait for 24 hours to check the setup. We could use real-time report as well to check the setup on-the spot.\n",
            "This analysis helps to improve website performance, understanding user behavior, understanding the impact of business campaigns and improvising the UI/UX to increase their potential users.\n",
            "Having insight into your clients’ behaviour and demographics can help you make decisions about serving them the right products at the right time for maximum chances of a sale. Such data could include a client’s persona, such as their age, location, and areas of interest.\n",
            "Some of the common metrics that are important in digital analytics include:\n",
            "Dashboard metrics:\n",
            "Some examples are pages per visit, bounce rate, and average duration of each visit.\n",
            "Most exited pages:\n",
            "Pages with an exit rate of 75–100% show that you need to examine the problem with the content and improve upon it.\n",
            "Most visited pages:\n",
            "These pages will make the customers either exit or explore the website further.\n",
            "Referring websites:\n",
            "These are other websites that link to your website.\n",
            "Conversion rate:\n",
            "This indicates whether the goal of your website was achieved, be it a sale of a product, a free giveaway, or a subscription to a newsletter.\n",
            "Frequency of visitors:\n",
            "This tells you about the loyalty of the customers.\n",
            "Days to the last transaction:\n",
            "This refers to the time lapse between the first visit and the sale. The shorter the time taken, the better it is for your business.\n",
            "Figure 1: Google Tag Manager Domains\n",
            "Figure 2: Google Tags\n",
            "Figure  3: Google Analytics\n",
            "Figure 4: Google Analytics\n",
            "Figure 5: Tracking Facebook Pixels for a website\n",
            "Figure 6: Whatagraph dashboard\n",
            "Figure 7: Whatagraph Dashboard(Conversions)\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2103.txt\n",
            "\n",
            "Dashboard to track the analytics of the website using Google Analytics and Google Tag Manager\n",
            "\n",
            "Client: A Automobile firm in India\n",
            "Industry Type: Automobile\n",
            "Services: Retail, Automobile\n",
            "Organization Size: 1000+\n",
            "The project objectives are as follows:\n",
            "This project includes assisting the client to study the user flow and behaviour flow of the users on the websites. It had one main website and three other sub websites to analyse the button clicks, impressions and understanding the user’s behaviour on the website. Many events were to be tracked and converted to a dashboard in Google Data Studio to make it simpler to understand.\n",
            "This project was created to give this data in a way that companies can readily understand through the use of visualisations. The graphs will show the increase/decrease in any of the metrics, as well as the manner in which the increase/decrease occurs. It will display all of the crucial data monthly or even by date range to help you keep track of the changes that occur.\n",
            "The main aim of this project is to display the event flow, user flow and behaviour flow through dashboards and analyse them to work on the areas of improvements.\n",
            "Google Analytics:\n",
            "Google Analytics is integral to tracking and measuring data from a number of digital platforms, but especially web metrics and customer behaviour. For example, through Google Analytics, you can see when people drop out of the buying process, perhaps they abandon while on the cart page, which would then inform your decisions on how to improve the check-out process.\n",
            "Because Google Analytics measures traffic from a variety of devices and sources and integrates with other online platforms, such as Google Ads, it is a handy tool to get an overview of your business’s digital analytics.\n",
            "Google Tag Manager:\n",
            "Google Tag Manager is a tag management system (TMS) that allows you to quickly and easily update measurement codes and related code fragments collectively known as tags on your website or mobile app. Once the small segment of Tag Manager code has been added to your project, you can safely and easily deploy analytics and measurement tag configurations from a web-based user interface.\n",
            "When Tag Manager is installed, your website or app will be able to communicate with the Tag Manager servers. You can then use Tag Manager’s web-based user interface to set up tags, establish triggers that cause your tag to fire when certain events occur, and create variables that can be used to simplify and automate your tag configurations.A Tag Manager container can replace all other manually-coded tags on a site or app, including tags from Google Ads, Google Analytics, Floodlight, and 3rd party tags.\n",
            "Google Data Studio Dashboards:\n",
            "The dashboards preview the important metrics related to the websites using graphs, tables to understand the trends, patterns in the users.\n",
            "The following steps were carried out for the project:\n",
            "The main deliverable for this project were dashboards on Google Data Studio depicting important metrics related to website performance. There were three sub websites for which there were two types of views each. Each of the views had several buttons related to the product. The project was about finding the user flow and event flow on the views.\n",
            "The main technical challenge faced was that there were multiple events setup in Google Analytics for one event and thus identifying a particular one was difficult.\n",
            "We had to communicate with the client to clarify about the event names. Although this took some time but it was necessary since accurateness of data is very essential for the project.\n",
            "This analysis helps to improve website performance, understanding user behavior, understanding the impact of business campaigns and improvising the UI/UX to increase their potential users.\n",
            "Having insight into your clients’ behaviour and demographics can help you make decisions about serving them the right products at the right time for maximum chances of a sale. Such data could include a client’s persona, such as their age, location, and areas of interest.\n",
            "Some of the common metrics that are important in digital analytics include:\n",
            "Dashboard metrics:\n",
            "Some examples are pages per visit, bounce rate, and average duration of each visit.\n",
            "Conversion rate:\n",
            "This indicates whether the goal of your website was achieved, be it a sale of a product, a free giveaway, or a subscription to a newsletter.\n",
            "Source/Medium Analysis:\n",
            "This analysis helps in understanding the traffic sources and medium on the website. This helps the businesses to work on strengthening the traffic sources to get better reach to the target audience.\n",
            "Traffic Analysis:\n",
            "The overall traffic analysis for the website provides information regarding the important metrics like users,avg. session duration and goal completions according to different source/medium. This will help the  business to analyse different traffic channels performances.\n",
            "Figure 1: Tracking of Buttons for Triber Virtual Studio\n",
            "Figure 2: Triber Goal Conversions\n",
            "Figure 3: Kiger 360 Experience Website Tracking\n",
            "\n",
            "Figure 4: Traffic Medium Analysis\n",
            "Figure 5: Overview of Dashboard Metrics\n",
            "Figure 6: Kiger Studio Experience Website\n",
            "Website URL:\n",
            "https://www.renault.co.in/\n",
            "Dashboard URL:\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2104.txt\n",
            "\n",
            "Power BI Dashboard on Operations, Transactions, and Marketing Data, embedding the Dashboard to Web App\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type: IT Services\n",
            "Services: Consulting, Software, Marketing Solutions\n",
            "Organization Size: 100+\n",
            "Create a dashboard with Assets Performance With react App. So users can evaluate with Key metrics from data analytics and forecasting.\n",
            "The client requires two pages:\n",
            "Asset Report Page\n",
            "Investor Page\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2105.txt\n",
            "\n",
            "NFT Data Automation (looksrare), and ETL tool\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type: IT Services\n",
            "Services: Blockchain, NFT\n",
            "Organization Size: 10+\n",
            "To scrape all the desired information regarding the NFTs from a website and store them in a database to be accessed later on.\n",
            "Matthew Brown – extract all events, all time from this https://looksrare.org/explore/activity . We can then pay you weekly to keep them up to date. You can choose any technology you like, as long as it’s updated into an SQL database. Additional tasks may be to make an alert or dashboard from data, later access API when it becomes available.\n",
            "We provided a robust solution which returned the NFT data every 8 hours into the google big query database. To do this we used selenium web driver to scrape all events as the website was dynamic and did not have a format data structure to scrape data using AJAX POST calls. After automating the scarper the data was manipulated and constructed into a desired format into pandas dataframe, which was later used to push the dataframe into the google big query database using Google cloud api and credentials. The data was getting collected every day and about 50M distinct rows were created.\n",
            "SQL\n",
            "Google BigQuery\n",
            "Google BigQuery\n",
            "The only technical challenge faced during this project was that the website used to keep changing the elements on their webpage and used to cause error. Though it did not use to happen regularly, it happened 3 times in 5 weeks. Also AJAX calls were not proper.\n",
            "Identifying the elements solved the issue. Also remote access to a better desktop enabled me to keep working as well as keep the code running all the time.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2106.txt\n",
            "\n",
            "Optimize the data scraper program to easily accommodate large files and solve OOM errors\n",
            "\n",
            "Client: A leading tech firm in India\n",
            "Industry Type: IT Services\n",
            "Services: SAAS services, Marketing services, Business consultant\n",
            "Organization Size: 100+\n",
            "Building a large data warehouse that houses projects and tenders data from all over the world that is to be collected from official government websites, multilateral banks, state and local government agencies, data aggregating websites, etc.\n",
            "We had tried multiple solutions to prevent the program from running out of memory. We used python pandas techniques to control the use of memory which worked for some files and did not work for others. Provided more solutions using vaex ,dask module and datatables.\n",
            "Desired changes to the code and committing them to github.\n",
            "System specs requirement was the main issue during this project because the RAM available was too less and got used up quickly.\n",
            "Team viewer to use remote desktop which had higher specs would be sufficient enough to solve the problem.\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2107.txt\n",
            "\n",
            "Making a robust way to sync data from airtables to mongoDB using python – ETL Solution\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type: IT Services\n",
            "Services: SAAS services, Marketing services, Business consultant\n",
            "Organization Size: 100+\n",
            "Equilo is a social impact start-up focused on gender equality and social inclusion. We need to link data in Airtable (1 million+ records spread across 20+ bases) to MongoDB (v3.x.x).Most of the data is backend data for our app, in which case the flow is only AT to MDB.Need to create a code that can calculate a scores by pulling from indicators in many different bases and putting result in new database.\n",
            "Used Python and MongoDB module along with Airtable API to fetch all the data from airtables and push them to the database. Stayed in touch with the client through slack and asana completing daily tasks and applying a cronjob for the program to run on a scheduled time.\n",
            "Python code for sync into their staging server and then to production.\n",
            "Airtable\n",
            "Main challenge faced was regarding the new concept of Airtables and syncing up the data into mongodb in a very complex schema as proposed by the client. Dissimilar columns in mongoDB and Airtables for 100s of tables took lot of time.\n",
            "Also insufficient information provided by client while coding and the previous versions codes that had been written only to discover them on a later stage caused a lot of problem.\n",
            "Not proper code management which could help next coders like me to complete the remaining stuff quickly.\n",
            "These issues were solved by lot of self study and evaluation and then asking the exact question to client which they would then answer. For eg: whereabouts of the previous codes and people who run that code.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2108.txt\n",
            "\n",
            "Incident Duration Prediction – Infrastructure and Real Estate\n",
            "\n",
            "Client: A leading research institution in the middle east\n",
            "Industry Type:  Research\n",
            "Services: R&D\n",
            "Organization Size: 1000+\n",
            "To complete a Research Paper draft by training various Machine Learning models which can predict the Incident Duration based on various parameters given in the dataset and summarising the results.\n",
            "Given a set of researches, need to analyse and compare various machine learning and deep learning models to predict the Incident Duration for the given dataset. The dataset contained Short durations as well as Long durations. Build models for each set of durations, compare and get the best out of all.\n",
            "Here, we had to predict the traffic incident duration with some machine learning tools and techniques i.e. XGBoost, SVR and Deep Learning algorithm using tensor flow. First two models were run on Python Interpreter whereas Deep learning model was run on R studio, all the three with the same dataset and then we had compared these models based on their MAE (mean absolute error). Initially, we had done a preliminary analysis of the collected incident duration data, to collect the statistical characteristics of all the variables used in our research.\n",
            "Python Interpreter\n",
            "Language Used: Python\n",
            "Libraries Used: pandas, sklearn, numpy, keras, pickle\n",
            "Programming, Statistical Analysis\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2109.txt\n",
            "\n",
            "Statistical Data Analysis of Reinforced Concrete\n",
            "\n",
            "Client: A leading research institution in the middle east\n",
            "Industry Type:  Research\n",
            "Services: R&D\n",
            "Organization Size: 1000+\n",
            "\n",
            "Conducting statistical data analysis on the data provided for different types of reinforced concrete (using 3 different fibers – Steel, Date Palm and Polypropylene fibers) and also helping in preparing good research paper based on laboratory data.\n",
            "The project had two phase:\n",
            "Phase 1:\n",
            "In this phase, we had to do a comprehensive analysis on the data given and finally build statistical models for the variables present. The main motive was to understand the behaviour of concrete based on various parameters – Compressive strength, Flexural strength, water absorption capabilities of the concrete and many more. The analysis should include, but was not limited to:\n",
            "Phase 2:\n",
            "In this phase, we had to develop a structure for the research paper based on the results and analysis. The paper included sections – Abstract, Introduction ( literature, background and objective), Experimental program ( materials and methods), Results and discussion ( analysis and interpretation) and Conclusion ( summary, insights and remarks).\n",
            "Providing a Comprehensive analysis for the concrete data – showcasing the key insights from it based on the parameters (compressive strength, etc). On the basis of results from the analysis, research paper was drafted which included all the deliverable.\n",
            "A manuscript (drafted article) with the following:\n",
            "Tools used:\n",
            "Statistical models – linear, polynomial, exponential and logarithmic models build for showcasing behavior of concrete mixes due to mixing of different fiber content and its effect on different parameters specified above.\n",
            "Coding – Python\n",
            "Performing statistical analysis – extracting inferences\n",
            "Building statistical models – through python or through Excel and its counterparts.\n",
            "No database was used.\n",
            "No Cloud server was used.\n",
            "The Challenges faced during project execution are:\n",
            "I had to use different libraries for building the models, later on turned to MS excel and spreadsheet because they were building models and were also able to showcase it on the data itself. For this, I learned how to build models on the aforementioned software through YouTube and blogs.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2110.txt\n",
            "\n",
            "Database Normalization & Segmentation with Google Data Studio Dashboard Insights\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type:  Market Research\n",
            "Services: Marketing, Consultancy\n",
            "Organization Size: 60+\n",
            "To combine the different datasets.\n",
            "To make dashboards for each and every dataset individually.\n",
            "Phase – 1: In this project first of all we have to combine different datasets individually to make single file for each source.\n",
            "Phase – 2: Make Good looking reports for each file individually.\n",
            "We used pandas dataframe to combine different files to make single file for each source. We used Google Data Studio to make good looking and better reports with good UI.\n",
            "We have provided a Google Data Studio report file as deliverable for the project.\n",
            "Python, Google Data Studio, Google Chrome\n",
            "Python Programming and SQL queries editor.\n",
            "SDLC model used in this project. We have used the SDLC model as analysis, design, implementation, testing and maintenance.\n",
            "Data cleaning, Data Pre-processing, Data Visualisation are used in this project.\n",
            "We have used the traditional file systems as database storage.\n",
            "I used pandas dataframe to combine different datasets and made a single file of every individual source. I used Google Data Studio to make dashboard for the project.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2111.txt\n",
            "\n",
            "Power BI dashboard to drive insights from complex data to generate business insights\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type:  Market Research\n",
            "Services: Marketing, Consultancy\n",
            "Organization Size: 100+\n",
            "Phase – 1: In this project first of all we have made heatmap between two columns named Author and Data Source. Then after two combining two tables named NY_data and nodeid_views made the report of all of the data.\n",
            "Phase – 2: Success of story was given by if pageviews is more than 35000, if pageviews lies between 3500-35000 the story was labelled as needs improvement and if it was below 3500 the story was labelled as failure.\n",
            "Phase – 3: The powerbi report was made to find different insights in the data like different tables were drawn between different attributes of data like pie chart, time series chart, comparison charts. The data is updated every week and the report is generated automatically.\n",
            "We provided them Phase 1 in the powerbi sql editor by combining two tables using sql queries. For phase 2 we just used the power bi program tool and written a script in Python to calculate the success of story. For Phase 3 we used the internal features of Power BI to find insights of the data.\n",
            "We have provided a PowerBI report file as deliverable for the project.\n",
            "Python, PowerBI, Google Chrome\n",
            "Python Programming and SQL queries editor.\n",
            "Waterfall model used in this project.\n",
            "Data cleaning, Data Pre-processing, Data Visualisation are used in this project.\n",
            "We have used the traditional file systems as database storage.\n",
            "We installed a new add on in the PowerBI to draw heatmap for the project and used the SQL editor to combine the tables on the basis of page views. We used python programming to convert the time series data to 5 minute time gap format.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2112.txt\n",
            "\n",
            "Real-time dashboard to monitor infrastructure activity and Machines\n",
            "\n",
            "Client: A leading tech firm in Europe\n",
            "Industry Type:  IT\n",
            "Services: Software Services\n",
            "Organization Size: 30+\n",
            "For the current project, we hope to develop a real-time dashboard (* it updates every several minutes). Currently, we have multiple Ubuntu machines that are sending messages every minute to Apache Pulsar.\n",
            "Developing a realtime updating dashboard to display the metadata of various machines on a server from pandio queue.\n",
            "The dahboard must display the count of “inactive” , “active” and “down” servers with a table displaying the details of all the machines in different color scheme for each type of server/machine.\n",
            "Development hosted URL\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2113.txt\n",
            "\n",
            "Electric Vehicles (EV) Load Management System to Forecast Energy Demand\n",
            "\n",
            "Client: A leading energy consulting firm in the USA\n",
            "Industry Type:  Energy\n",
            "Services: Energy solutions, Consultancy\n",
            "Organization Size: 100+\n",
            "Create a Machine learning solution to manage electricity for electric vehicles.\n",
            "Main Tasks:\n",
            "We need to calculate the date and time probability that the user will plugin his vehicle today based on his plugin date and plugin time history. We also need to decrease time probability based on the user’s past time range.\n",
            "We converted the user’s plugin data into binary values like 0 if the user hasn’t plugged-in his vehicle on that day and 1 if he plugged-in. We identified the driven distance based on the amount of charge used between two plug-in times. Then we trained the Ridge Regression ML model for identifying each day driven kilometer. From these kilometres we have identified the probability that user’s will plug-in today and it will increase day by day till the user does not plug-in his vehicle.\n",
            "For time probability we have used Probability Distribution Function (PDF) and Cumulative Distribution Function  (CDF). These functions will decrease probability according to the user’s time range.\n",
            "2 python scripts to:\n",
            "Google Colab, VS Code, Google Drive, and MS Excel.\n",
            "Python programming language, Data Analytics with numpy and pandas, Data Visualization with matplotlib, Statistics and Mathematics, Machine learning with SKlearn.\n",
            "Ridge Regression Model\n",
            "Data Analytics, Data Visualization, Machine learning, Python, Statistics\n",
            "local data from MS Excel Sheet\n",
            "There are a lot of challenges faced during project execution\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2114.txt\n",
            "\n",
            "Power BI Data-Driven Map Dashboard\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type:  Market Research\n",
            "Services: Marketing, Consultancy\n",
            "Organization Size: 60+\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2115.txt\n",
            "\n",
            "Google Local Service Ads (LSA) Leads Dashboard\n",
            "\n",
            "Client: A leading law firm in USA\n",
            "Industry Type:  Law\n",
            "Services: Law practice\n",
            "Organization Size: 40+\n",
            "Local Service Ads is a newer program by Google that allows advertisers to achieve a “Google Guaranteed” status in search engines when a visitor makes a search. Advertisers who participate in Google Local Service Ads will receive a larger ad space with their competitor’s local services ads and they will be able to feature their local businesses throughout organic search queries.\n",
            "There are various aspects that firms must concentrate on in order to win the Google services ad and so raise their ranking. These enhancements may be implemented if companies obtain current data about their leads and analyse it in order to take appropriate actions in the future.\n",
            "This project was created to give this data in a way that companies can readily understand through the use of visualisations. The graphs will show the increase/decrease in any of the metrics, as well as the manner in which the increase/decrease occurs. It will display all of the crucial data monthly or even by date range to help you keep track of the changes that occur.\n",
            "The solution for the project includes data insights through visualisations which will help businesses to better analyse the available data. This solution will help the businesses in improvising the factors to increase their potential customers and raise their respective ranks.\n",
            "It is divided into two parts: databases and data dashboard. The databases will store the important data retrieved from the LSA dashboard and use them to calculate some important metrics. The data dashboard will represent those metrics in form of graphs and data in form of tables.\n",
            "The project deliverables can be divided into two parts:\n",
            "For extracting the data from the LSA Dashboard, we have made our own tool by python scripts. The automation tool will store data in the excel sheets and google bigquery for respective businesses on a day to day basis. PyCharm for compiling and running the code. JsonViewer for processing\n",
            "We have used the LSA API to extract data from the LSA Dashboard. Google Sheets API to store data in excel sheets. Bigquery API for storing data in google bigquery. The scripts for the automation tool were written in the Python programming language.\n",
            "Software Model: RAD(Rapid Application Development model) Model\n",
            "In the RAD paradigm, less emphasis is placed on planning and more emphasis is placed on development activities. It aims to create software in a short period of time.\n",
            "Advantages of RAD Model:\n",
            "Two types of databases: Google excel sheets and google bigquery.\n",
            "Google BigQuery Cloud Database with up to 1 TB of free storage is being used.\n",
            "Some minor technical challenges were faced for clients with minimum data. For those, plotting graphs became difficult.\n",
            "We tried to process the data, remove the blank data spaces and plotted the graph with available data.\n",
            "It’s undeniable that Google’s Local Services ads (LSA) have changed the way home service businesses advertise online.\n",
            "The pay per lead system designed to provide the end-user with a quick, clean and trusted experience, gives small and medium-sized businesses a better shot at competing with national brands and massive budget operations.\n",
            "To win with the Local Services the businesses need to take care of some factors where data comes to help.\n",
            "Fig.1: Data Dashboard for individual businesses-1\n",
            "Fig.2: Data Dashboard for individual businesses-2\n",
            "Fig.3: Consolidated Dashboard\n",
            "Fig.4: Historical Account Data\n",
            "Fig.5: CPA and CPL datasheet\n",
            "Fig.6: Lead Dispute Status\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2116.txt\n",
            "\n",
            "AWS Lex Voice and Chatbot\n",
            "\n",
            "Client: A leading tech firm in USA\n",
            "Industry Type:  IT\n",
            "Services: eCommerce\n",
            "Organization Size: 40+\n",
            "Create a Voice and chatbot using AWS lex which can book flights, hotels, cars and book some fun activities in a city.\n",
            "We need to create a voice and chatbot using AWS lex and lambda function. The bot should book a flight, a hotel, and a car by asking some relevant questions to the user like destination, origin, date, etc. We also need to create a combination of all these which can plan the whole trip, flight, hotel, car and book some fun activities.\n",
            "We have created aws lex intents and lambda functions for all bookings. Intents manage front ends like utterances (user can ask to the bot) and slots (bot replies with relevant questions). Lambda functions manage backend parts like which intent should be triggered if the user says “ book a flight” or “book a hotel” or “book a car”. For search results we have used some external APIs like Amadeus for flight, sabre for hotels and blablacar for car booking.  We have modified search results by using Data Analytics (for getting the cheapest and good star flight and hotel), Machine learning (for getting user’s preferences by analyzing user’s history) and NLP (Differentiate search results by text analysis) techniques so users can get the best search results.\n",
            "An aws lex voice and chatbot which can book flight, hotel, car and fun activities. This can be integrated with IOS applications.\n",
            "AWS Lex, AWS Lambda, AWS Cognito, AWS EC2, Google colab, VS code, FAST API, Uvicorn.\n",
            "python, machine learning, data analytics, NLP.\n",
            "TfIdf-Vectorizer and cosine similarity\n",
            "Data Analytics, Machine learning, NLP, Python, AWS, REST APIs.\n",
            "MySQL\n",
            "AWS\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2117.txt\n",
            "\n",
            "MetaBridges API Decentraland Integration – AR, VR\n",
            "\n",
            "Client: A leading tech firm in the USA\n",
            "Industry Type:  IT\n",
            "Services: Consulting, Software, Blockchain, Metaverse\n",
            "Organization Size: 20+\n",
            "To integrate with Metaverse environments with the help of EC2, S3 bucket and the Decentraland SDK.\n",
            "Move 3D model files from EC2 instance to S3 bucked using aws-sdk.\n",
            "Configure  s3 bucket in aws account, create an user for s3 bucket api keys, and\n",
            "api secret. Put the api key, aapi secret, bucket name and bucket region in\n",
            "environment variable to use them in app. Install aws-sdk to implement s3 bucket.\n",
            "Create a function to send file from nodejs server to s3 bucket.\n",
            "Aws ec2 instance credentials, s3 bucket credentials. Code used in the project\n",
            "vs code editor, git bash terminal, google chrome web browser.  Metamask wallet, cryptocurrency, blockchain, bitcoin,  metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality\n",
            "Javascript language is used.  Metamask wallet, cryptocurrency, blockchain, bitcoin,  metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality\n",
            "dcl SDK (Decentraland sdk for nodejs), aws-sdk, awscli.\n",
            "Node js project setup, Dcl sdk setup, Aws ec2 instance setup with aws cli,\n",
            "S3 bucket connection with aws-sdk. cryptocurrency, blockchain, bitcoin, metamask, metaverse, VR, AR, Virtual Reality, Augmented Reality\n",
            "No database is used\n",
            "AWS cloud server is used\n",
            "Making the application port in ec2 instance available globaly.\n",
            "Search few blogs and videos for the solution. And make it done by doing some change in\n",
            "Security group in ec2 instance.\n",
            "As Decentraland is a platform based of NFT so main part of business is related to NFT and cryptocurrency.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2118.txt\n",
            "\n",
            "Microsoft Azure chatbot with LUIS (Language Understanding)\n",
            "\n",
            "Client: A leading retail firm in the USA\n",
            "Industry Type:  Retail\n",
            "Services: e-commerce, retail business\n",
            "Organization Size: 100+\n",
            "To create an advanced chatbot using Microsoft Azure cognitive service to take orders from customer on behalf of a pizza restaurant and give order summary as end result to the user.\n",
            "The project uses MS Azure LUIS service for language understanding to receive order details from a customer and provide an order summary. Also display various menu options to the customer in a dynamic method.\n",
            "Our solution is to create a chatbot on MS Azure platform using their LUIS service in bot-framework composer environment. Use dynamic hero cards to display menu so that user can get a better experience.\n",
            "Microsoft Azure web platform\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2119.txt\n",
            "\n",
            "Impact of news, media, and press on innovation, startups, and investments\n",
            "\n",
            "Client: A leading research institution in the word\n",
            "Industry Type:  Research, R&D\n",
            "Services: R&D\n",
            "Organization Size: 1000+\n",
            "Make data ready for predictive modelling.\n",
            "Making Google Data Studio dashboard.\n",
            "Phase – 1: In this project first of all we have to clean the data as the data was very noisy, we have to filter out only the needed columns of the data.\n",
            "Phase – 2: Finding co-relation between the pitchbook data and the other output files.\n",
            "Phase – 3: Making dashboard in Google Data Studio for the project.\n",
            "We used pandas and numpy to clean the data and make useful for it to be used in predictive modelling. We have found the co-relation between the tempa msa pitchbook data and the output files like textual file, ai_ml_tm file etc. We have made the dashboard using the Google Data Studio.\n",
            "We have provided a excel file consisting of clean data and the Google Data Studio report.\n",
            "Python, Google Data Studio, Google Chrome\n",
            "Python Programming\n",
            "Waterfall model used in this project.\n",
            "Data cleaning, Data Pre-processing, Data Visualisation are used in this project.\n",
            "We have used the traditional file systems as database storage.\n",
            "Cleaning the data was the major challenge faced while executing the project. The data has a lot of noise. It was difficult to find which data was useful and which data is not useful in this project. Secondly the co relation between the output files and pitchbook data. There was nothing common between both the datasets. So was difficult to find co-relation between them.\n",
            "We used pandas dataframe to clean the data and make it ready for predictive modelling and used the Google Data studio to find insights between the different datasets.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2120.txt\n",
            "\n",
            "AWS QuickSight Reporting Dashboard\n",
            "\n",
            "OverviewAs a Singapore and Australia based startup, Drive lah (known as Drive mate in Australia) is a peer-to-peer car sharing platform where you can rent a large variety of cars, always nearby at great value. All trips on Drive lah are comprehensively insured through our insurance partners so car owners don’t have to worry about their insurance. The idea is simple: car ownership is expensive in Singapore (per month yet only use the car 5% of the time – cars are mostly parked. With Drive lah you can reduce the cost of ownership by renting it out when you don’t need it in a safe way. Renters can rent those cars when they are not used by their owners at good value.In a fast-growing non-ownership economy where taxi, food, beauty is available on-demand, Drive lah is envisioning to take the lead in distance travel and simplifying car access\n",
            "Websitehttp://www.drivelah.sgCompany size11-50 employeesFounded2019\n",
            "Automating the process to get updated Metrics every week.\n",
            "Evaluate the following Performance Metrics which will be used on AWS Quick Sight for Performance Evaluations:\n",
            "Build Code for extracting Daily Agent Activity Report on Daily Basis.\n",
            "For Performance Metrics, we suggested that we will Code for each Metric & will store them in a Table on AWS RDS which will be directly synced to the AWS Quick Sight for Performance Evaluations.\n",
            "For Automating the process to get updated Tables of Metrics every week, we suggested to use a Virtual Machine on which we can upload all code files & can run a Cron Job for each file to automatically get updated on specified time every week.\n",
            "Python\n",
            "Amazon Relational Database Service (RDS)\n",
            "Tried with AWS Lambda Function to update tables on AWS RDS but Lambda Function was unable to run complete code.\n",
            "Suggested to use a Virtual Machine on which we can upload our Code Files & can run Cron Job for automatically updating tables on regularly basis.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2121.txt\n",
            "\n",
            "Google Data Studio Dashboard for Marketing, ads and Traction data\n",
            "\n",
            "OverviewBankiom – the super banking app for MENA on a mission to make managing your finances easier.\n",
            "☞ Open an account on your phone and get a virtual card in 3 minutes or less☞ Manage all your bank accounts from one app and one control panel☞ Save money and grow your wealth\n",
            "Websitehttp://www.bankiom.comCompany size2-10 employeesFounded2019SpecialtiesBanking, Financial Services, Card Payments, Mobile Payments, Digital Bank, and FinTech\n",
            "Build a dashboard unifying all the platforms that we use: Google Ads, FB ads, Appsflyer, Mixpanel\n",
            "We want to be able to track everything in the funnel from traffic source to total installs (paid, organic and by channel):– App settings in Appsflyer– SDK Installation, test it (+ instruction for devs)– Ad sources setup in ad accounts (Facebook, Google Ads, etc)– Ad sources setup in Appsflyer– In-app conversions mapping– Conversion set up in ads sources– One link, smart script, and deep link setup– SKAD Network for IOS app\n",
            "Built dashboard for each data source like Google Ads, Facebook Ads for tracking installs, channel spend, cost per install for both Android and IOS.\n",
            "Then, we made a dashboard for tracking the retention rates of customers and other events that they execute on the app like transfer money, user registration, connect banks. The data for these events was fetched from MixPanel.\n",
            "These dashboards were made using Google Data Studio.\n",
            "We need to deliver dashboards for tracking the ads data from Google and Facebook and to track the events which the users perform on their app and for this data was collected from MixPanel.\n",
            "Following Tools were used for successful execution of the project\n",
            "Code was written to create the pipeline to fetch MixPanel data through mixpanel Api and store it in bigquery. So, the code was written in Python.\n",
            "Following Skills were used to complete the project\n",
            "For storing the data of the project Google Sheets and Google BigQuery were used.\n",
            "Web Cloud server used in this project was Google Cloud Platform.\n",
            "Technical Challenges faced during the execution of the project was to understand how the api of the mixpanel works and how to connect it to Google BiqQuery. Another technical challenge that we faced was to find a free resource to connect the facebook ads data to data studio.\n",
            "To solve the technical challenges we went through the documentation of the mixpanel api to get a understanding of how the things work. Based on that we built the pipeline to connect the mixpanel data to big query. The other technical challenge of finding a free resource to connect the facebook ads to datastudio for free was solved by researching for the various connectors available and we found an add on named ‘Adveronix’ which could connect the facebook ads data to google sheets which can eaily be connected to data studio.\n",
            "https://datastudio.google.com/reporting/8af163c1-b328-4ed3-91fc-cf8a026d0d9f\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2122.txt\n",
            "\n",
            "Gangala.in: E-commerce Big Data ETL / ELT Solution and Data Warehouse\n",
            "\n",
            "Client: A leading eCommerce firm in the USA, Columbia, India, and Latin America\n",
            "Gangala promotes local shops selling a wide variety of products at great prices. Easily find the best offers using our price comparison tool. It’s a WIN WIN for …\n",
            "Industry Type:  eCommerce\n",
            "Services: e-commerce, retail business\n",
            "Organization Size: 100+\n",
            "Gangala.in: E-commerce site gathering data of different products from various sources and providing it on a single platform\n",
            "A platform in which users can get price data of any product from multiple sites. The client provided us with raw data. We were tasked with building a pipeline for the data, build API’s to get product data such as price and update them and make sure that all the data is available for the front end team to access.\n",
            "We built them a pipeline to process and clean the raw data provided. We built API’s to fetch the updated data of the products. Neo4j was used as the intermediary data and mongoDB was used as our primary database. We also process the images of each product and remove any unwanted texts from it and add the client’s watermark.\n",
            "A fully-updated database with up to date data on all the products and each product having atleast 3-5 prices from different sites.\n",
            "Linode cloud servers\n",
            "We were asked to process 3million products per day and this was a challenge as the VM’s we used were not able to handle the load.\n",
            "We were able to overcome the challenge by using Asynchronous processing of the data thereby increasing the speed of the processing reducing the cost on the client side as well\n",
            "https://gangala.in/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2123.txt\n",
            "\n",
            "Big Data solution to an online multivendor marketplace eCommerce business\n",
            "\n",
            "Client: A leading eCommerce firm in the USA, Columbia, India, and Latin America\n",
            "Gangala promotes local shops selling a wide variety of products at great prices. Easily find the best offers using our price comparison tool. It’s a WIN WIN for …\n",
            "Industry Type:  eCommerce\n",
            "Services: e-commerce, retail business\n",
            "Organization Size: 100+\n",
            "To give User experience of easy and convenient Shopping by searching all the products like any medicines , Clothes , Gadgets etc in a single Website without going through all the E-Commerce Sites and make shopping easy and get the most affordable and best product.\n",
            "It’s an E-Commerce Sites that’s helps customer to compare different  products that were available on different E-Commerce Sites like Flipkart , Amazon , Netmeds etc.It’s helps the user to visit only one sites to get what they need and find the perfect product without visiting all the sites.The gives the user a great and friendly Experience in Buying any Products.It’s Also have some Unique Similar Products Recommendation Based on user search and also have a ChatBot That’s solves User Query .It’s uses Big data and Rest API that’s help the projects for regular updates and regular fetching of the new products.\n",
            "Python script for performing ETL and Cypher Query for big data Handling.\n",
            "https://gangala.in/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2124.txt\n",
            "\n",
            "Creating a custom report and dashboard using the data got from Atera API\n",
            "\n",
            "Client: A leading Marketing firm in USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing, consulting, ads, business solutions\n",
            "Organization Size: 20+\n",
            "Atera.com is used as our RMM, we have an agent on every machine. Which tracks the if a machine goes down, initial response time etc.., The website doesn’t provide any standard reports, So we needed to create a custom report.\n",
            "\n",
            "https://datastudio.google.com/reporting/5e61aecb-a420-41cc-afba-d0ca37f69132\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2125.txt\n",
            "\n",
            "Azure Data Lake and Power BI Dashboard\n",
            "\n",
            "OverviewStone is a video bibliographic tool for journalists and other researchers.\n",
            "It allows users to capture, annotate and share their journeys through digital and physical space, producing verifiable logs and generating monetizeable video highlight reels that can be embedded in digital and other media – showcasing key moments and telling the story behind the story.\n",
            "Our mission is to address distrust and disinformation with transparency and authenticity, while simultaneously tilting the information ecosystem in favour of quality original work.\n",
            "Research is valuable. Make it Visible.\n",
            "Write In Stone.\n",
            "Websitehttp://www.writeinstone.comCompany size2-10 employeesHeadquartersBlackheath, New South WalesFounded2017SpecialtiesResearch Transparency, Trust, Video Content, Journalism, Proof Of Work, and Bibliographic Standards\n",
            "Built a Power BI dashboard as per the requirement. Also built a separate dashboard for the metric data from Azure.\n",
            "Power BI dashboard which contains indicators funnels, new indicators(Research logged, Average number of Highlights per projects, Total hours of content watched etc), visualizations  extracted from metric data.\n",
            "Difficulty in data collection.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2126.txt\n",
            "\n",
            "Google Data Studio Pipeline with GCP/MySQL\n",
            "\n",
            "Client: A leading IT firm in Europe\n",
            "Industry Type:  IT\n",
            "Services: e-commerce, retail business, marketing, Consulting\n",
            "Organization Size: 100+\n",
            "Creating a Data Pipeline to sync live data from FieldPulse to Google Data Studio using GCP/MySQL.\n",
            "There is a Virtual Machine up and running and MySQL in Google Cloud(GCP). Get the following live data from FieldPulse to Google Data Studio(GDS) for making Business Dashboard in GDS –\n",
            "Such that if data changes in FieldPulse , GDS Dashboard should update automatically.\n",
            "For fetching data from FieldPulse –\n",
            "For getting data from GCP MySQL to Google Data Studio(GDS) :\n",
            "Below are the services that we provided to client after completion of this project –\n",
            "Google Colab\n",
            "MySQL\n",
            "Google Cloud Platform (GCP)\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2127.txt\n",
            "\n",
            "QuickBooks dashboard to find patterns in finance, sales, and forecasts\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: e-commerce, retail business, marketing\n",
            "Organization Size: 100+\n",
            "Build a fully Integrated BI Platform in PowerBI using native connectors and APIs(QuickBooks and Airtable) to pull real time data from many sources.\n",
            "For building a fully integrated BI Platform , the data has to come from the following sources to feed it to PowerBI –\n",
            "·         QuickBooks :  An accounting software that accepts real-time business payments ,  manage and pay bills, manage organisation’s deposits/expenses , customers ,and payroll functions. The following data/tables has to be fetched from Quickbooks –\n",
            "o   Customer\n",
            "o   Invoices\n",
            "o   Product & Services\n",
            "o   Payments\n",
            "o   Expenses\n",
            "o   Deposits\n",
            "o   Accounts\n",
            "o   Vendors\n",
            "o   Departments\n",
            "o   Classes\n",
            "·         Airtable : An online database hybrid platform for creating and sharing relational databases with friendly user interfaces. The following databases with multiple data table has to be fetched from Airtable –\n",
            "o   Marketing Data Analytics Base (Google Ads , Facebook Ads)\n",
            "o   Payroll Tracking (Payroll , Hours Log)\n",
            "This Quickbook and Airtable real time data has to go to the powerBI service (https://app.powerbi.com). Then create useful visualisation and dashboards based on plan and feedback from the executive team. All visuals in dashboards should automatically update without any intervention to make it fully integrated.\n",
            "Collecting data tables from data sources :\n",
            "After getting these raw data tables , pipeline converts it into DataFrame , then writes/updates it into Airtable.\n",
            "The Pipeline is deployed in a server that runs every night , it fetches the data from QuickBooks API and writes/updates to Airtable.\n",
            "Scheduled Refresh :  To refresh visualization/dashboard (If incoming data from Airtable API has updated) , set refresh time in powerBI service.\n",
            "Below are the services that we provided to client after completion of this project –\n",
            "PowerBI\n",
            "\n",
            "This is how we get all the data of any size from Airtable bases.\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2128.txt\n",
            "\n",
            "Marketing, sales, and financial data business dashboard (Wink Report)\n",
            "\n",
            "Client: A leading retail firm in Australia\n",
            "Industry Type:  Retail\n",
            "Services: e-commerce, retail business, marketing\n",
            "Organization Size: 100+\n",
            "Bringing in data from many sources(Google Analytics , ServiceM8 and Xero etc.) and making Business Dashboard KPIs in Wink Report.\n",
            "For building Business Dashboards in Wink Report , collect data from the following sources –\n",
            "Explore/analyze the underlying data tables from each Data Source. Make useful reports using different tables from different data sources based on client’s requirement. Set up formulas in each report to calculate desired fields. Add a custom visualization to each report for making dashlets. Add dashlets to newly created dashboards.\n",
            "For collecting the data from the sources (ServiceM8 , Xero , Facebook , Google Ad) native connectors have been used , available in the Wink Report. It fetches the following data/tables from around the given data sources –\n",
            "Data Pipeline : For collecting data from Communiqa website (https://www.communiqa.com.au/) , web scraping has been used as there is no connector available for Communiqa to Wink Report. By scraping Communiqa , we get the following data –\n",
            "Then , we have merged different tables from different sources to get desired reports. Store all reports belonging to the same dashboard in a separate folder. Do this for all the dashboard , then setup formula for calculating desired fields. Add appropriate visualization to each report for each folder. Then , finally add all dashlets belonging to the same folder to a newly created dashboard.\n",
            "Below are the services that we provided to client after completion of this project –\n",
            "Wink Report\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2129.txt\n",
            "\n",
            "React Native Apps in the Development Portfolio\n",
            "\n",
            "Here are the list of react native apps developed by the team and the resources:\n",
            "https://itunes.apple.com/us/app/truckmap-truck-gps-routes/id1198422047?mt=8\n",
            "https://play.google.com/store/apps/details?id=com.truckmap.truckmap\n",
            "https://play.google.com/store/apps/details?id=com.verifai.standalone\n",
            "https://apps.apple.com/nl/app/verifai/id1504214033\n",
            "https://apps.apple.com/de/app/meetlist-lokale-aktivit%C3%A4ten/id1439183715\n",
            "https://play.google.com/store/apps/details?id=de.mlug.meetlist\n",
            "https://play.google.com/store/apps/details?id=com.payroo.employee\n",
            "https://play.google.com/store/apps/details?id=com.vahcare\n",
            "https://play.google.com/store/apps/details?id=com.candorivf\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2130.txt\n",
            "\n",
            "A Leading Law Firm in the USA, Website SEO & Optimization\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing consulting\n",
            "Organization Size: 100+\n",
            "Connect website to Search Console, Google Analytics and Facebook Pixel through Google Tag Manager.\n",
            "Fix SEO of the website.\n",
            "Connecting website to Google Search Console, Google Analytics and Facebook Pixel through Google Tag Manager.\n",
            "Fixing SEO of the website.\n",
            "Website connected to Google Search Console, Google Analytics and Facebook Pixel successfully.\n",
            "Fixed the\n",
            "Project Snapshots\n",
            "https://www.keepingorlandomoving.com/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2131.txt\n",
            "\n",
            "A Leading Hospitality Firm in the USA, Website SEO & Optimization\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing consulting\n",
            "Organization Size: 100+\n",
            "Working On-page SEO of the pages to make it user-friendly and feasible for crawlers to make the site indexing better.\n",
            "Firstly, exploring the Liverez as it was a new platform then, performing intermediate SEO like page titles and description, completing word count, alt. text and removing duplicate page title and description.\n",
            "To increase the organic traffic of the site and improve the insights.\n",
            "There was a bit of improvement in the traffic of the site.\n",
            "Brightlocal.com, Yoast SEO, Grammarly\n",
            "Basic HTML\n",
            "ON-page SEO\n",
            "https://www.missionbeach.com/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2132.txt\n",
            "\n",
            "A Leading Firm in the USA, Website SEO & Optimization\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing consulting\n",
            "Organization Size: 100+\n",
            "Fixing On-Page SEO of the website\n",
            "Fixing On-Page SEO contains things like title, meta description, image-alt text, broken links, 404 error page, multiple h1 tag in one page, duplicate title/description, dynamic URL, sparse content page (word count <500), etc.\n",
            "Project Snapshots\n",
            "URL https://www.jupiteroutdoorcenter.com/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2133.txt\n",
            "\n",
            "A Leading Musical Instrumental, Website SEO & Optimization\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing consulting\n",
            "Organization Size: 100+\n",
            "Connect website to Google Tag Manager.\n",
            "Remove error.\n",
            "Remove all previously added code and add new code for connecting to Google Tag Manager.\n",
            "Remove 5xx error from the website.\n",
            "Website connected to Google Tag Manager successfully.\n",
            "Removed 5xx error.\n",
            "WordPress\n",
            "Google Tag Manager\n",
            "\n",
            "URL: https://www.hamiltonpianoco.com/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2134.txt\n",
            "\n",
            "A Leading Firm in the USA, SEO and Website Optimization\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing consulting\n",
            "Organization Size: 100+\n",
            "Connect website to Search Console. Add Call Rail Code\n",
            "Connecting website to Google Search Console through Google Tag Manager.\n",
            "Connect website with CallRail.\n",
            "Website connected to Google Search Console successfully.\n",
            "Added CallRail code to the website.\n",
            "https://www.12stonesnwa.com/\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2135.txt\n",
            "\n",
            "Immigration Datawarehouse & AI-based recommendations\n",
            "\n",
            "Client: A leading business school worldwide\n",
            "Industry Type:  R&D\n",
            "Services: R&D, Innovation\n",
            "Organization Size: 100+\n",
            "Objective of this project is to research and collect news article data sourcing from Canada, based on the keyword.\n",
            "There were 3 phases of the project.\n",
            "We provide them with completed Phase 1 in an excel sheet and ongoing samples for Phase 2. Also work for Phase 3 has been started in between to complete the Project as soon as possible in a best way.\n",
            "There is a file containing excel sheet and a word file containing a summary of the dataset and folders of text files containing samples of data from Phase 2.\n",
            "Python, PyCharm, Jupyter Notebook, Microsoft Excel, Google Chrome is used to complete different phases of this project\n",
            "Python programming language is used to do Web Scraping, Automation, Data Engineering in this project.\n",
            "SDLC is a process followed for a software project, within a software organization. It consists of a detailed plan describing how to develop, maintain, replace and alter or enhance specific software. The life cycle defines a methodology for improving the quality of software and the overall development process.\n",
            "We are using Iterative Waterfall SDLC Model as we have to follow our development of software in phases and we also need feedback on every step of the development of our project so as to keep track of the occurring changes with every step.\n",
            "Figure 1 SDLC Iterative Waterfall Model\n",
            "Data scraping, cleaning, pre-processing and creating data pipelines are used in this project.\n",
            "We used the traditional way of storing the data i.e file systems.\n",
            "There were a lot of challenges we faced during the project execution.\n",
            "Below are the points used to solve the above technical challenges-\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2136.txt\n",
            "\n",
            "Lipsync Automation for Celebrities and Influencers\n",
            "\n",
            "Client: A leading tech firm in India\n",
            "Industry Type:  Entertainment\n",
            "Services: B2C\n",
            "Organization Size: 100+\n",
            "To change the lipsing of the original video with the new replaced audio.\n",
            "We needed to create an output video that will have the new lipsing according to the new replaced audio. Also we will have to change the actual audio with the new audio with automated editing.\n",
            "We have created two different files which will perform 2 different operations 1st will replace the original audio with new and extract only video from original. 2nd will take the muted video and replaced audio and we will get the output of the new replaced audio lipsync. This is done by pre-defined model Wav2Lip on github.\n",
            "2  google colab notebooks\n",
            "github\n",
            "Google drive\n",
            "Python 3.6\n",
            "moviepy\n",
            "ffmpeg\n",
            "Wav2lip\n",
            "Python programming\n",
            "Data science\n",
            "Provided by the company (Hrithik Roshan video files)\n",
            "https://colab.research.google.com/drive/18mlREpLmV9hj-uDfufkGJ_-m_E37Hct9?usp=sharing\n",
            "https://colab.research.google.com/drive/1FZHvcVKyJxOUkUFI2auPt3vTOu4jh09K?usp=sharing\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2137.txt\n",
            "\n",
            "Key Audit Matters Predictive Modeling\n",
            "\n",
            "Client: A leading business school worldwide\n",
            "Industry Type:  R&D\n",
            "Services: Research & Innovation\n",
            "Organization Size: 10000+\n",
            "Do regression modeling on the data provided, cross-country determinants of Key Audit Matters (KAMs) and its usefulness to Investors and Debt Market Participants\n",
            "USEFULNESS TO EQUITY MARKETS\n",
            "In order to do the analysis and hypothesis testing, create a mapping to divide the audits into sub category and category according to the sub category and category provided in the question document. Clean the data before proceeding and calculate variables ABRET, ABVOL, CAR and CAAR according to the description provided.\n",
            "Created a mapping for key audit matters to label the sub category and category of the audit for further analysis and merging with other datasets on the basis of the unique keys to create a final dataset we can use to calculate and do the hypothesis testing.\n",
            "Calculation of variable ABRET and ABVOL is proceeded by firstly arranging the data by unique key and then the date of the data to get the sorted data. Cleaning is done on the data by removing the repetitive entries from the dataset and then selected the data around the date for which the variable is to be calculated. Similarly, calculated ABVOL in which extracted the data around the annual report filing date and mean value for 40 days interval that ends 21 days before earning announcement dates.\n",
            "Couldn’t proceed because dataset provided by the client was incomplete in order to calculate ABRET.\n",
            "R language to create mapping for the key audit matters and save data set for question 1.\n",
            "Python pandas library to deal with dates and extract data around annual report filing date.\n",
            "Data mapping, data cleaning, data manipulation, debugging\n",
            "Key audit matter\n",
            "GDP rule law\n",
            "Audit fee\n",
            "Trading data\n",
            "Earning date\n",
            "Report filing date\n",
            "Dataset provided by the client was too big and made my system slow when the data is loaded in the environment. Too many datasets and variables made it bit difficult to understand and time taking.\n",
            "Calculated the number of unique identifiers in the large dataset and sorted those. Then selected the data for 1 unique identifier and sorted dates for it and append it to the dataframe and saved group of such unique identifiers to reduce the size of the dataset and performed the calculations in loop.\n",
            "To tackle the difficulty of understanding the data I made a document tracking all the columns or variables present in the data.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2138.txt\n",
            "\n",
            "Splitting of Songs into its Vocals and Instrumental\n",
            "\n",
            "Client: A leading Entertainment firm in the USA\n",
            "Industry Type:  Entertainment\n",
            "Services: Music\n",
            "Organization Size: 100+\n",
            "The objective of this project is to split a song into its vocals and instrumental.\n",
            "The project aims at taking a Hindi language song as input and separating the vocals(lyrics) from the instrumental music of the song. Save both the vocals and instrumental files separately as output.\n",
            "I have used Python programming language for this project. The use of a Python library called Spleeter developed by Deezer has been made to achieve our goal.\n",
            "Spleeter is Deezer source separation library with pretrained models written in Python and uses Tensorflow. It makes it easy to train source separation model (assuming you have a dataset of isolated sources), and provides already trained state of the art model for performing various flavor of separation :\n",
            "2 stems and 4 stems models have high performance on the musdb dataset. Spleeter is also very fast as it can perform separation of audio files to 4 stems 100x faster than real-time when run on a GPU.\n",
            "Python tool that takes Hindi song as input and gives two audio files as output: vocals file and instrumental file.\n",
            "Python\n",
            "2 Stems model\n",
            "Advanced Python programming\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2139.txt\n",
            "\n",
            "AI and ML technologies to Evaluate Learning Assessments\n",
            "\n",
            "Client: A leading EduTech firm in the USA\n",
            "Industry Type:  EduTech\n",
            "Services: Educations. Training\n",
            "Organization Size: 1000+\n",
            "It is a culture management platform that uses learning as the fundamental mode of communication. The platform requires an Analytics portion that captures a variety of data related to the interaction of the learner with content, assessments, engagements and forums to create personalized learning plans for each user to increase the effectiveness of learning and its retention which together make an impact on the overall productivity of the learner and the organization.\n",
            "We helped the client in deciding the data required for the analysis process. We came up with the appropriate models for various tasks and interpretations of how the data will be collected and analysed for the initial response, final response, retention, proficiency, and learning intent of the user. We designed the models in such a way that one can perform seamlessly grading for each question type (based on difficulty level) and at a different hierarchical level (sub-section, section, training, and so on). We knew that each user has its unique aptitude level (basic, intermediate, and advanced) and keeping that in my mind, we incorporated those aptitude levels in our analytics too. Moreover, we integrated the grade and time factor into the analysis so that more points are allotted for comparatively tough questions and quick responses, respectively.\n",
            "MS Excel sheet, Google spreadsheets with proper tables and visualizations.\n",
            "Jupyter notebook, MS Excel, Google Spreadsheets.\n",
            "Python.\n",
            "Data science and analytics.\n",
            "Generated our data through data simulation.\n",
            "Data analytics is all about analysing and finding patterns in the data that already exist or are getting generated in real-time. However, this project is in the budding stage, and we had no data to start our analysis. Moreover, this project is novel, and the dataset that meets our requirements was nearly impossible to find online.\n",
            "We performed data simulation techniques and tried to generate the data as authentic as possible using some libraries in python and random functions in spreadsheets. We also generated the data manually at a small scale, but we made sure that we are including every human factor in it.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2140.txt\n",
            "\n",
            "Datawarehouse, and Recommendations Engine for AirBNB\n",
            "\n",
            "Client: A leading hotels chain in the USA\n",
            "Industry Type:  Real Estate, Hospitality\n",
            "Services: Hostpitality\n",
            "Organization Size: 1000+\n",
            "To download the data from the servers using Cyberduck on the daily basis and perform data engineering on it.\n",
            "We created a Python Script which performs the task and create property and forward master files, which we deliver to client on weekly basis.\n",
            "Two csv files named property master file and forward master file to be delivered weekly after applying various steps.\n",
            "PyCharm, PowerBi, Cyberduck, Microsoft Excel.\n",
            "Python Programming Language is used to create scripts performing Data Manipulation in different files.\n",
            "SDLC is a process followed for a software project, within a software organization. It consists of a detailed plan describing how to develop, maintain, replace and alter or enhance specific software. The life cycle defines a methodology for improving the quality of software and the overall development process.\n",
            "We are using Iterative Waterfall SDLC Model as we have to follow our development of software in phases and we also need feedback on every step of the development of our project so as to keep track of the occurring changes with every step.\n",
            "Figure 1 SDLC Iterative Waterfall Model\n",
            "Skills such as Data Pre-processing, cleaning, and data manipulation are used in this project.\n",
            "We used traditional way of storing the data i.e file systems.\n",
            "Cyberduck, which is a libre server and cloud storage browser for Mac and Windows with support for FTP, SFTP, WebDAV, Amazon S3 etc, was used in this project with Amazon S3 servers.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2141.txt\n",
            "\n",
            "Real Estate Data Warehouse\n",
            "\n",
            "Client: A leading Real Estate firm in the EU\n",
            "Industry Type:  Real Estate\n",
            "Services: Real Estate\n",
            "Organization Size: 1000+\n",
            "The objective of this project is to build a data warehouse from a website given search and filter criteria.\n",
            "The objective of this project is to collect data from a website given search and filter criteria.\n",
            "Data Brief:\n",
            "Filters:\n",
            "Contains a list of the federal states in Germany to Crawl:\n",
            "https://en.wikipedia.org/wiki/States_of_Germany\n",
            "\n",
            "\n",
            "We have developed a Python tool that crawls and scrapes all the apartment listings for all the states in Germany under each category namely: mieten wohnungen, kaufen wohnungen, kaufen anlageobjekte and kaufen grundstuck. The Scrapy library has been used to crawl and scrape. Beautiful soup could have also been used for the scraping purpose, but for the sake of consistency, Scrapy has been used for both purposes.\n",
            "Scrapy is an application framework for crawling web sites and extracting structured data which can be used for a wide range of useful applications, like data mining, information processing or historical archival.\n",
            "Even though Scrapy was originally designed for web scraping, it can also be used to extract data using APIs (such as Amazon Associates Web Services) or as a general purpose web crawler.\n",
            "Four Spiders have been created for each category to be scraped. Every spider crawls all the states in Germany and scrapes all the apartment listings for important data. Every spider creates a separate JSON file to store all its data. This data is then converted to CSV using another python script called “conversion”.\n",
            "The python tool has been completely automated and only needs the “Controller” script to be run. The script also has the capability of running every two weeks automatically.\n",
            "Four CSV files (one for each category):\n",
            "Mieten Wohnungen.csv\n",
            "Kaufen Wohnungen.csv\n",
            "Kaufen Anlageobjekte.csv\n",
            "Kaufen Grundstuck.csv\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2142.txt\n",
            "\n",
            "Traction Dashboards of Marketing Campaigns and Posts\n",
            "\n",
            "Client: A leading Marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing consulting\n",
            "Organization Size: 100+\n",
            "We are testing AWS Comprehend. I performed a key phrase analysis of our LinkedIn posts. We have an output file. Now we need your help to visualize the data so that we can interpret it.I also have the original export file from LinkedIn. I want to answer this business question:For the LinkedIn posts that received the highest engagement, which keywords, phrases, and hashtags were most commonly used?\n",
            "I want to match up Engagement Rate with key phrase analysis. The business question is this: For the LinkedIn posts that received the highest engagement, what were the most common keywords, phrases and hashtags used?\n",
            "Beyond matching to Engagement Rate, please check if there is a way to also view the data according to Impressions and Likes.\n",
            "Data Driven Dashboards which will give the summary of Most used words, keywords, Phrases and also Analysis of Posts as per their interaction with their audience.\n",
            "Two Dashboard Links in which\n",
            "First dashboard represents Key Phrase analysis of the output by AWS Comprehend.\n",
            "Second Dashboard represents the Linked In data Analysis\n",
            "Python, Google Data studio\n",
            "Python\n",
            "Python and Data Studio\n",
            "MongoDB\n",
            "Google Data Studio\n",
            "One of the major problem was to match the output of AWS Comprehend data with the data of excel sheet to find out which posts received maximum interactions and make a dashboard out of it.\n",
            "Working on the output.json file in code editor and comparing it to the Linked In data sheet to check the accuracy of the output file with each post.\n",
            "1 Key Phrase Analysis Dashboard\n",
            "https://datastudio.google.com/reporting/efbabbff-55ba-4326-8133-78ae304aeb99\n",
            "2 Linked IN Data Analysis Dashboard\n",
            "https://datastudio.google.com/reporting/3525e1c1-6c4f-4613-b260-d6e975fe1652\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2143.txt\n",
            "\n",
            "Google Local Service Ads (LSA) Data Warehouse\n",
            "\n",
            "Client: A leading Marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing consulting\n",
            "Organization Size: 100+\n",
            "Automated tool to extract daily review data from Local Service Ads dashboard for all clients.\n",
            "Get list of companies to monitor along with their LSA URL\n",
            "Use Selenium automated browsing to open the review page for each company.\n",
            "Web scrape the data from the review page\n",
            "Prepare report\n",
            "Upload to database\n",
            "An automated tool that runs daily and extracts and uploads review data for all companies.\n",
            "Selenium\n",
            "Heroku\n",
            "Sheets API\n",
            "BigQuery\n",
            "Python\n",
            "Data extraction, cleaning and summarising. Web scraping.\n",
            "BigQuery –  LSA_Review_db\n",
            "Heroku\n",
            "Using Selenium to automate web browsing since it takes a large amount of RAM.\n",
            "Using the proper type of dynos and managing their allotment to lower both costs as well as memory usage.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2144.txt\n",
            "\n",
            "Google Local Service Ads Missed Calls and Messages Automation Tool\n",
            "\n",
            "Client: A leading Marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing consulting\n",
            "Organization Size: 100+\n",
            "A real time tool to send a report of missed calls and messages to the client.\n",
            "Extracts data from CallRail database for the last 5 minutes\n",
            "To provide data real time, schedule the tool to check for data every 5 minutes.\n",
            "Extract data from CallRail\n",
            "Filter out all answered calls\n",
            "Prepare report\n",
            "Get email ids from sheets\n",
            "Send email through SendGrid\n",
            "An automated tool which provides real time updates to the client along with all information about the call.\n",
            "Heroku\n",
            "CallRail API\n",
            "SendGrid\n",
            "Sheets API\n",
            "Python\n",
            "Data extraction, cleaning and summarising\n",
            "Google Big Query\n",
            "Heroku\n",
            "Sending correct reports only to the companies which are active\n",
            "Using Google Sheet’s cell formatting in Python\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2145.txt\n",
            "\n",
            "Marketing Ads Leads Call Status Data Tool to BigQuery\n",
            "\n",
            "Client: A leading Marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing consulting\n",
            "Organization Size: 100+\n",
            "Prepare a daily report for the companies and upload it to BigQuery database. Data is from callrail and contains all call information about a company.\n",
            "Use CallRail API to get data from database.\n",
            "Run script daily\n",
            "Filter out excess data\n",
            "Prepare report\n",
            "Upload to BigQuery\n",
            "A working deployed automated tool that runs once a day in the morning hours and uploads the data to BigQuery database. Tool is monitored daily.\n",
            "Heroku\n",
            "CallRail API\n",
            "BigQuery\n",
            "Sheets API\n",
            "Python\n",
            "Data extraction, cleaning, and summarising\n",
            "BigQuery –  Call_Status_From_CallRail\n",
            "Heroku\n",
            "Ensuring proper data upload to database\n",
            "Proper monitoring of tool post-deployment.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2146.txt\n",
            "\n",
            "Marketing Analytics to Automate Leads Call Status and Reporting\n",
            "\n",
            "Client: A leading Marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing consulting\n",
            "Organization Size: 100+\n",
            "Prepare a daily report for the companies and upload it to Google Sheets. Data is from callrail and contains all call information about a company.\n",
            "Use CallRail API to get data from database.\n",
            "Run script daily\n",
            "Filter out excess data\n",
            "Prepare report\n",
            "Upload to Google Sheets\n",
            "A working deployed automated tool that runs once a day in the morning hours and uploads the data to Google Sheets. Tool is monitored daily.\n",
            "Heroku\n",
            "CallRail API\n",
            "BigQuery\n",
            "Sheets API\n",
            "Python\n",
            "Data extraction, cleaning and summarising\n",
            "Google Sheets –   Call status record\n",
            "Heroku\n",
            "Ensuring proper amendment of data to sheets without overwrite\n",
            "Proper monitoring before final deployment\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2147.txt\n",
            "\n",
            "CallRail, Analytics & Leads Report Alert\n",
            "\n",
            "Client: A leading Marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing consulting\n",
            "Organization Size: 100+\n",
            "Prepare an annual report for the companies and upload it to database. Data is from callrail and contains call analytics.\n",
            "Use CallRail API to get data from database.\n",
            "Set time window to be one year.\n",
            "Filter out excess data\n",
            "Prepare report\n",
            "Upload to BigQuery\n",
            "A working deployed automated tool that runs once a year in the morning hours and uploads the data to BigQuery. Tool is in prototype phase and hence is operational for 2 companies.\n",
            "Heroku\n",
            "CallRail API\n",
            "BigQuery\n",
            "Python\n",
            "Data extraction, cleaning and summarising\n",
            "BigQuery –  lead_report_alert_callrail\n",
            "Heroku\n",
            "Working on a large amount of data since a year’s data contains hundred of thousands of records\n",
            "Optimized code for faster processing.\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2148.txt\n",
            "\n",
            "Marketing Tool to Notify Leads to Clients over Email and Phone\n",
            "\n",
            "Client: A leading Marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing consulting\n",
            "Organization Size: 100+\n",
            "Prepare a daily report for data from Local Service Ads dashboard and email to client.\n",
            "A working deployed automated tool that runs everyday in the morning hours and sends a report to the client. Tool is monitored everyday.\n",
            "Heroku\n",
            "LSA API\n",
            "SendGrid\n",
            "Sheets API\n",
            "Python\n",
            "Data extraction, cleaning, and summarising\n",
            "Data is not stored and is sent directly to the client\n",
            "Heroku\n",
            "Ensuring a company’s data does not go to another company\n",
            "Testing on multiple dummy email ids\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2149.txt\n",
            "\n",
            "Data ETL: Local Service Ads Leads to BigQuery\n",
            "\n",
            "Client: A leading Marketing firm in the USA\n",
            "Industry Type:  Marketing\n",
            "Services: Marketing consulting\n",
            "Organization Size: 100+\n",
            "Upload daily data from Google Local Service Ads dashboard to BigQuery database.\n",
            "A working deployed automated tool that runs everyday in the morning hours and uploads a report to database. Tool is monitored everyday.\n",
            "Heroku\n",
            "LSA API\n",
            "BigQuery API\n",
            "Sheets API\n",
            "Python\n",
            "Data extraction, cleaning and summarising\n",
            "BigQuery –  lsa_lead_daily_data\n",
            "Heroku\n",
            "Making sure that the data uploaded is for the right company.\n",
            "Monitoring daily logs and uploads for some time and making sure data was correct\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2150.txt\n",
            "\n",
            "Marbles Stimulation using python\n",
            "\n",
            "Client: A leading consulting firm in the USA\n",
            "Industry Type: IT Consulting\n",
            "Services: Consultanting\n",
            "Organization Size: 100+\n",
            "For all 4 cases, use a random number generator that will give you numbers between 1 & a million [1,000,000].  Whatever generator you use, make sure to adjust the numbers so that they are between 1 & 1,000,000 distributed randomly.\n",
            "For all tasks, we will have 5 colors, for example in Task 1, when the random number selected is between 1 & 5857 choose a bright color that is easily visible [I have called it Br. Clr. 1], for numbers between 5858 & 8678 choose another bright color [Br. Clr. 2], for numbers between 8679 & 11500 choose B (Blue), for numbers between 11501 & 50,000 choose R (Red), and > 50,000 choose G (Green). Simulate these 4 Task scenarios and represent them in a Table (1000 x 32) and collect statistics at the end. Replicate the simulation exercises for each Task with 3 different initial seed numbers. Likewise for 16 other Tasks.\n",
            "No Software model is being Used to Solve this Project\n",
            "No database were used stored complete data in MS Excel\n",
            "No cloud servers were used for this project\n",
            "Figure 1: Sample Output File for Task 12 stimulation 3\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2151.txt\n",
            "\n",
            "Stocktwits Data Structurization\n",
            "\n",
            "Client: A leading financial institution in the USA\n",
            "Industry Type: Financial services & Consulting\n",
            "Services: Financial consultant\n",
            "Organization Size: 100+\n",
            "\n",
            ">To process two json file stocktwits_legacy_msg_2015_10.txt (file size = 2 GB) & stocktwits_legacy_msg_2015_10.txt (file size = 3.5 GB).\n",
            ">To handle Nested Json for both files and after conversion into one merged Data Frame need to perform Data Structurization.\n",
            ">While accessing a Json file in JupyterNB, I need to perform Chunking as the file size is bigger and it is in json format to avoid PC standstill.\n",
            ">After Data Preprocessing I need to perform Exploratory Data Analysis on that Data.\n",
            "> Conditional Programming to deal with Data Transferring to a particular folder based on the column values.\n",
            "During the training period I was involved with 2 live projects, One project named ‘Stocktwits Data Structurization’ in which I have to process huge JSON Data which was already obtained the size of data was nearly 5 GB need to process the data by chunking with chunk size = 20000 rows at a time. The file has nested JSON data within it’s attributes so abstracts data from the nested columns into a new dataframe. Completed handling complex nested json formed columns abstracted from nested json. Then need to Handle the missing data by mapping it with another index dataset further missing values for certain attributes were handled by mean value and 0 substitution. This task involves numerous pandas operations along with multiple python functions. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes.\n",
            "Worked on Accessing Json Data, done tree Analysis on Json Sample data.\n",
            "Both the File was too big for reading and applying some Python Code in JupyterNb, so performed chunking of stocktwits_legacy_messages_2015_10.txt  with chunk size = 20000 rows at a time. Similarly trying for the other file.\n",
            "Created a list of all the chunked files of Json Data & Concat all the files in that list.\n",
            "The File has Nested Json data within it’s attributes so abstracted data from the nested columns into a new DataFrame. Completed handling complex nested json formed columns abstracted from nested json.\n",
            "Renamed the columns with identification. (Eg: ‘id’ as ‘entities_id’) likewise for others. So that while merging the data doesn’t create any issue. Completed forming Preprocessed csv file for 1st json file which  Output2015.csv.\n",
            "For Second file size was > 3gb so splitted the file into ten parts and then individually solved nested json for all these parts like done in the 1st file finally concat them into one, then handled columns arrangements and removed unwanted columns and finally removed dictionary representation from entity_sentiments column. Completed forming Preprocessed csv file for 2nd json file which is Output_Stocktwits_2017.csv.\n",
            "The cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Further done Exploratory Data Analysis on the cleaned dataset finding correlation matrix and plotting certain seaborn graphs between strong correlated attributes. Conditional Programming to deal with Data Transferring to a particular folder based on the column values.\n",
            "● Jupyter Notebook\n",
            "● Anaconda\n",
            "● Notepad++\n",
            "● Sublime Text\n",
            "● Brackets\n",
            "● JsonViewer\n",
            "● Python Programming\n",
            "My project ‘Stocktwits Data Structurization’ developed with a software model which makes the project high quality, reliable and cost effective.\n",
            "● Software Model : RAD(Rapid Application Development model) Model\n",
            "● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.\n",
            "● Advantages of RAD Model:\n",
            "o Changing requirements can be accommodated.\n",
            "o Progress can be measured.\n",
            "o Iteration time can be short with use of powerful RAD tools.\n",
            "o Productivity with fewer people in a short time.\n",
            "o Reduced development time.\n",
            "o Increases reusability of components.\n",
            "o Quick initial reviews occur.\n",
            "o Encourages customer feedback.\n",
            "o Integration from very beginning solves a lot of integration issues\n",
            "● Data Mining\n",
            "● Data Wrangling\n",
            "● Data Visualization\n",
            "● Python Programming including OOPs and Exception Handling\n",
            "No Databases were used, all the data was stored on Google Drive and Local Device.\n",
            "No Cloud Server were used\n",
            "● Handling Huge Data and Data Cleaning\n",
            "● JSON Data Serialization.\n",
            "● Solving Complex Nested JSON among the data provided.\n",
            "● Handling Huge Data and Data Cleaning\n",
            "Solved by Breaking the Dataset into 10 stream parts as the data was too huge and was not able to read easily in Jupyter NB.\n",
            "● JSON Data Serialization\n",
            "Solved by Data Chunking with chunk_size=20000 which means serialization of data with processing 20000 rows at a time.\n",
            "● Solving Complex Nested JSON among the data provided.\n",
            "Viewed the Structure of the part of data in JSON Viewer then Changed the data in proper standard JSON Format. After Reading JSON Data Performing Normalization of Nested JSON data setting maximum level of normalization with specifying proper orient form. Then After Normalization remaining Unsolved Nested JSON was solved using Dictionary Conversions and Structuring the data.\n",
            "Figure 1 Sample Input Dataframe After Converting Outer JSON\n",
            "Figure 2 Sample Output Dataframe After Solving Nested JSON and Data Preprocessing\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2152.txt\n",
            "\n",
            "Sentimental Analysis on Shareholder Letter of Companies\n",
            "\n",
            "Client: A leading financial firm in the USA\n",
            "Industry Type: Financial services & Consulting\n",
            "Services: Financial consultant\n",
            "Organization Size: 100+\n",
            "Project “Sentimental Analysis on Shareholder Letter of Companies” objective was to Predict the Sentiments columns Shareholder Letter in terms of Polarity and Subjectivity finally classification of data into positive, negative and neutral tone.\n",
            "The project ‘Sentimental Analysis on Shareholder Letter of US Companies’ task involved data cleaning on shareholder letters of different companies which includes lemmatization, lower case conversion, removing special character, \\n , \\t , punctuations, numbers & single character and tokenization. To generate polarity and subjectivity columns for the letter 1 & letter 2 columns using the Textblob library of NLTK. Based on the polarity categorizing it into positive, neutral  &  negative.\n",
            "i.  Lemmatisation\n",
            "ii. lower case conversion\n",
            "iii.  Removing Special character\n",
            "iv.  Removing \\n , \\t etc\n",
            "v.  remove punctuations, numbers & single character removal\n",
            "vi.  forming list of letter data using tqdm\n",
            "● Jupyter Notebook\n",
            "● Anaconda\n",
            "● Notepad++\n",
            "● Sublime Text\n",
            "● Brackets\n",
            "● Python 3.4\n",
            "My project ‘Sentimental Analysis on Shareholder Letter of Companies’ developed with a software model which makes the project high quality, reliable and cost effective.\n",
            "● Software Model : Waterfall Model\n",
            "● For Project ‘Sentimental Analysis on Shareholder Letter of US Companies’ is a Waterfall Model as our model is not forming the loop from end to the start using Textblob which predicts Sentiments, Polarity and Subjectivity as the output following the Waterfall Model.\n",
            "No Database is used to complete this project.\n",
            "No Web cloud Server was required for this work.\n",
            "I have worked before on tasks similar to this so there were no challenges faced but the data cleaning was a bit different and required time to complete.\n",
            "As Discussed no technical Challenges were faced during this project.\n",
            "Figure 1: Input Data Schema\n",
            "Figure 2: Output Data Schema\n",
            "Figure 3: Sample Input Dataset\n",
            "figure 3 is pandas dataframe which was fetched from google cloud database there were 7 columns and 13290 rows.\n",
            "Figure 4: Sample Output Dataset\n",
            "figure 4 is output pandas dataframe after data cleaning and modeling of sentiment identification there are 13 columns and 13290 rows.\n",
            "Figure 5: Sentiments assignment based on polarity\n",
            "figure 5 represents the identification of sentiments and tone based on polarity and subjectivity. polarity>0 then sentiment type is positive,  if the polarity<0 sentiment type is negative and if the polarity=0 sentiment type is neutral.\n",
            "Figure 6:  Histogram Representation of Length of Shareholder Letter 1\n",
            "figure 6 is histogram plot between length of shareholder letter 1 among the final output dataset.\n",
            "Figure 7:  Histogram Representation of Length of Shareholder Letter 2\n",
            "figure 7 is Histogram plot between length of shareholder letter 2 among the final output dataset.\n",
            "Figure 8: Flow Chart\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2153.txt\n",
            "\n",
            "Population and Community Survey of America\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type: Marketing services & Consulting\n",
            "Services: Marketing consultant\n",
            "Organization Size: 100+\n",
            "Project ‘Population and Community Survey of America’ objective were to perform Data Abstraction, Data Structurization, Data Preprocessing, Data Cleaning, and Combining Data from all the years listed and finally presenting insights of the data by Exploratory Data Analysis.\n",
            "For Project ‘Population and Community Survey of America’ task involved fetching json and unformatted csv data from numerous web links further needed to process data, handling nested JSON, data conversion of JSON data in dataframe, performing certain pandas operation for feature selection and structuring data. Concat all this data into one csv file then handle missing value by mapping with another dataset finally perform certain data visualization and exploratory data analysis.\n",
            "Module 1: Data Abstraction\n",
            "The process of data abstraction involves collecting data from numerous web links from Year 2005 to 2017 and viewing the data using JSON viewer in tree format.\n",
            "Module 2: Data Chunking and Integration\n",
            "Was unable to process data in pandas so performed data chunking with chunksize 10000 rows at a time for year 2005 likewise performed for all other years data till 2017 and finally combined all the dataframes into one containing all data from year 2005 to 2017.\n",
            "Module 3: Handling Complexity of Nested Data & format the Unformatted CSV Files\n",
            "Handling unformatted CSV in proper comma separated format so that data frame can be formed. Dataframe produced after merging for all the years from 2005 to 2017 contains a lot of nested JSON data among certain attributes so performed normalization of nested Json forming new_columns naming them based on their attributes key.\n",
            "2.2.4 Module 4: Data Cleaning and Preprocessing\n",
            "Involves handling missing value, contraction mapping with another dataset to fill the missing State_Zip_Code column, handling inf and -inf within the dataset for some attributes and forming a new column population_ratio based on passing formula among other attributes.\n",
            "2.2.5 Module 5: Data Analysis\n",
            "This step involves forming a correlation matrix to understand the relation between numeric attributes. performed Exploratory Data Analysis on strong correlated attributes to understand pattern/relation between them.\n",
            "After completion of Project we provided:\n",
            "● Jupyter Notebook\n",
            "● Anaconda\n",
            "● Notepad++\n",
            "● Sublime Text\n",
            "● Brackets\n",
            "● Python 3.4\n",
            "● JSON Viewer\n",
            "● Python\n",
            "● ETL Techniques\n",
            "● Advanced Excel Formatting\n",
            "My project ‘Population and Community Survey of America’ developed with a software model which makes the project high quality, reliable and cost effective.\n",
            "● Software Model : RAD(Rapid Application Development model) Model\n",
            "● This Project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.\n",
            "● Advantages of RAD Model:\n",
            "o Changing requirements can be accommodated.\n",
            "o Progress can be measured.\n",
            "o Iteration time can be short with use of powerful RAD tools.\n",
            "o Productivity with fewer people in a short time.\n",
            "o Reduced development time.\n",
            "o Increases reusability of components.\n",
            "o Quick initial reviews occur.\n",
            "o Encourages customer feedback.\n",
            "o Integration from very beginning solves a lot of integration issues\n",
            "No Database is used in this project, only used Google Drive for Storing and Transferring Data.\n",
            "No Web Server is Used\n",
            "Data Cleaning and Filling out Missing Values by Data mapping with another dataset as the Data was not in proper format in the another dataset.\n",
            "Data Cleaning was done using a few built in pandas operations to deal with Missing Values, Ordering Data Columns, Data Formatting, Changing of data types and many more. Filling of remaining Missing Data from columns using Outer Join among the datasets and using Map Function of Python.\n",
            "Figure 1: Input Data Schema for Year 2008\n",
            "Figure 2: Output Data Schema from Year 2005 to 2017\n",
            "Figure 3: Dataset for Year 2008\n",
            "figure 3 is pandas dataset of year 2008 which has 169595 rows and 25 columns which was fetched from authenticated survey web portal, data obtained were in JSON format which were converted into pandas dataframe likewise there are dataframes created from year 2005 to 2017.\n",
            "Figure 4:  Output Preprocessed Dataset\n",
            "figure 4 is an output preprocessed dataset from 2005 to 2017 which has 26,41,363 rows and 25 columns.\n",
            "Figure 5: Describing Numeric Data of Preprocessed Dataset\n",
            "Figure 6: Bar plot of attribute state_name\n",
            "figure 6 represents the bar plot among the state_name on the final output dataset from year 2005 till 2017.\n",
            "Figure 7: KDE Graph for all numeric population data column of dataset\n",
            "figure 7 represents the Kernel Density Estimate Plot(KDE) among all Population estimate data columns for the Preprocessed Dataset. KDE plot is a method for visualizing the distribution of observations in a dataset, analogous to a histogram. KDE represents the data using a continuous probability density curve in one or more dimensions. Plotted many more graphs apart this between highly correlated attributes like pair plot, box plot, line plot etc.\n",
            "Figure 8: Flow Chart\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2154.txt\n",
            "\n",
            "Google LSA API Data Automation and Dashboarding\n",
            "\n",
            "Client: A leading marketing firm in the USA\n",
            "Industry Type: Marketing services & Consulting\n",
            "Services: Marketing consultant\n",
            "Organization Size: 100+\n",
            "For this project objective was to perform API Data Abstraction using Google LSA API in GCP, Automation of data fetching and storing in BigQuery on daily basis, Storing Historical data for all active companies, Fetching Customer Report then storing data on daily basis in BigQuery also storing Historical data for all companies, Perform Linear Regression Modelling on Historical data for all companies and storing the modeling Summary in google sheet in a structured manner, Basecamp Automation with LSA Daily Data, Creating 4 BI Dashboard in Data Studio for Live, Historical, Modelling and Customer Report data for all companies.\n",
            "For this project task was to obtain an account report and detailed lead report for a specific dates and customer_id using Google Local Service Ads API Service in Google Cloud Platform. Further need to integrate with Google BigQuery database storing MCC data for all companies on a daily basis then storing Historical data for all active companies. Also notifying clients through email and passing messages containing daily account data in a message format to BaseCamp Message Board and Campfire of respective company projects through its API all with python programming, further deploying the script on Heroku Server for automating all this task. Then Creating BI Dashboard in Data Studio connecting with BigQuery and Creating Live Dashboard, Historical Dashboard for all companies.\n",
            "On historical data for all companies, Linear Regression Modelling needs to perform and to create Modelling Dashboard for all companies in Data Studio. Further needs to do  Exploratory Data Analysis for all companies on Historical Data.\n",
            "To Store Customer Account Report for message lead and phone lead on a daily basis, Script needs to be created and deployed in Heroku and also need to store Historical data for these companies and Finally Create Data Studio Dashboard on it.\n",
            "Creating Sales Representation Dashboard for two Companies which involves multiple Reports and blending of multiple data sources from Big Query.\n",
            ">> Module 1: API Data Abstraction\n",
            "Which first includes generation of the access token and refresh token with the scope of Google AdWord API for the authentication and connecting with Google LSA API. Then fetching daily data in JSON format for particular account name based on customer_id assigned in API URL while fetching data. Likewise generating a script that would Handle data generation for all other active accounts based on their customer id.\n",
            ">> Module 2: Data Imputation and Storing\n",
            "Converting the JSON data to the pandas data frame forming a list of data frame for all the active accounts by looping them then deriving certain more attributes based on their handling the missing and inf values. Finally storing the data in Google Big Query database within the respective table for all accounts using Bigquery API.\n",
            ">> Module 3: Data Storing in BigQuery and Notification Automation\n",
            "The task was to automate notifications sent to email and to Basecamp and the data transferred to the database on a daily basis by deploying the script to Heroku Server setting time parameters based on the New York time zone.\n",
            ">> Module 4: Automation tools created till now:\n",
            "i. LSA_AccountReport_daily_BigQuery tool: For Automation of Account Report for all companies on a daily basis. Scheduling it at 1:00 am in the Los Angeles Timezone.\n",
            "ii. LSA_AccountReport_Historical_API tool:  For Storing Historical Data for companies for the last few Years till the end date which we set.\n",
            "iii. Basecamp_lsa_automation: This is used to pass the lsa data in a message format to Campfire for respective companies groups and store lsa data combined for all companies to Messageboard and Campfire at one Automation Python Group in Basecamp.\n",
            "iv. LSA_DateRange Tool: Used to store missed out data for all the companies for a few sets of days or months as per the need.\n",
            "v. LSA_MainSheet_AutoUpdation tool: For Auto updation of main sheet  ‘LSA Client Lead’  Google Sheet. As Daily Data are fetched on the basis of this list so it is required to auto update this sheet for all the new companies entered would store information of those like company name, account id and database name.\n",
            "vi. LSA_daily_CustomerReport tool: Created to Store LSA Customer Report for all companies in database ‘CustomerReport_PhoneLead’ & ‘CustomerReport_MessageLead’ on daily basis.\n",
            "vii. Historical_LSA_CustomerReport tool:  Created to Store LSA Customer Report for all companies in database ‘CustomerReport_PhoneLead’ & ‘CustomerReport_MessageLead’ storing historical data for year 2021.\n",
            ">> Module 5: Data Studio BI Dashboards Created:\n",
            "i. Historical Dashboard\n",
            "ii. Live Dashboard\n",
            "ii. Customer Report Dashboard\n",
            "iii. Modelling Report Dashboard\n",
            "iv. Sales Representation Dashboard\n",
            "● PyCharm\n",
            "● Jupyter Notebook\n",
            "● Anaconda\n",
            "● Heroku\n",
            "● Notepad++\n",
            "● Google Sheet API\n",
            "● Google LSA API on GCP\n",
            "● Google BigQuery\n",
            "● Sublime Text\n",
            "● Brackets\n",
            "● JsonViewer\n",
            "● Python\n",
            "● SQL\n",
            "My project ‘Google Adword LSA API Reports automation into Google Big Query database and Basecamp’ developed with a software model which makes the project high quality, reliable and cost-effective.\n",
            "● Software Model: RAD(Rapid Application Development model) Model\n",
            "● This project follows a RAD Model as our model is not forming the loop from end to the start, also my project was based on prototyping without any specific planning. In the RAD model, there is less attention paid to the planning and more priority is given to the development tasks. It targets developing software in a short span of time.\n",
            "● Advantages of RAD Model:\n",
            "o Changing requirements can be accommodated.\n",
            "o Progress can be measured.\n",
            "o Iteration time can be short with the use of powerful RAD tools.\n",
            "o Productivity with fewer people in a short time.\n",
            "o Reduced development time.\n",
            "o Increases reusability of components.\n",
            "o Quick initial reviews occur.\n",
            "o Encourages customer feedback.\n",
            "o Integration from the very beginning solves a lot of integration issues\n",
            "● API Data Abstraction\n",
            "● Data Mining and Statistical Modelling\n",
            "● Data Wrangling\n",
            "● Deployment for Automation\n",
            "● Data Visualization\n",
            "● SQL\n",
            "● Machine Learning\n",
            "● Python Programming including OOPs and Exception Handling\n",
            "● Google Firestore (Just for Testing Purpose)\n",
            "● Google BigQuery\n",
            "Google BigQuery Cloud Database with up to 1 TB of free storage is being used.\n",
            "● Scheduling Automation of Python Script.\n",
            "● Data Exceptions and Duplication in BigQuery Tables.\n",
            "● Refresh token Expiration After 7 Days.\n",
            "● Data Exception due to Inactive companies or not Updation of LSA Main sheet.\n",
            "● Basecamp ProjectId Issue for transferring Data to multiple companies projects.\n",
            "● Data Studio Time Series Plot data mismatch due to multiple account id.\n",
            "● Scheduling Automation of Python Script.\n",
            "Python Library BlockingScheduler were used and the Timezone variable ‘TZ’ was set to Los Angeles in Heroku\n",
            "● Data Exceptions and Duplication in BigQuery Tables.\n",
            "Structuring SQL Query to deal with all the database issues which were being used in BigQuery to solve those issues.\n",
            "● Refresh token Expiration after 7 Days.\n",
            "Initially ‘Auth Playground’ was used for generating Refresh token which was getting expired after every 7 Days so to last it longer for more than a year we are now using the refresh token which was generated using Python script where proper token endpoints and many other headers were defined before generating the refresh token.\n",
            "● Data Exception due to Inactive companies or not Updation of LSA Main sheet.\n",
            "Data Exception occurred while API data abstraction for few of the companies which were solved by adding more nested try and except statements after understanding issues also ‘LSA Clients Lead’ main sheet was not being updated by other members due to which we missed out data for few of the companies which were solved by creating script which will automatically update the mainsheet when an error occurred.\n",
            "● Basecamp ProjectId Issue for transferring Data to multiple companies projects.\n",
            "This issue was solved by creating Basecamp Main sheet where data was fetched now by mapping the account id of fetched data using LSA Main sheet and project id of all the basecamp companies.\n",
            "● Data Studio Time Series Plot data mismatch due to multiple account id.\n",
            "Solved by adding many parameters like setting the metrics which will do a summation of all the companies on a particular day for all the account id.\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2155.txt\n",
            "\n",
            "Healthcare Data Analysis\n",
            "\n",
            "Client: A leading healthcare tech firm in the USA\n",
            "Industry Type: Healthcare Consulting\n",
            "Services: Management consultant\n",
            "Organization Size: 100+\n",
            "The main objective of this project is to find the pattern in the vital signs of patients who were admitted to the hospital in past. And from this pattern, we get some ranges that help us to give early warnings.\n",
            "We are more interested in non-survivor patients’ vital signs as compare to survivor patients. we find patterns in vital signs that could better determine that patient died (ex. if Sp02 is below 70, patient in 95% of cases died, if Sp02 is below 50%, the death rate is 99.9%) or we can take correlations which can help us to find better patterns to define death cases.\n",
            "Data The dataset which was used for analysis here is taken from the mimic website. But the dataset is not in the correct format which we want, after some manipulation, we get the data ready for the analysis.\n",
            "Approach\n",
            "I can’t go with 1st option because a major part of the data has missing values. so, I decided to go with the second option and fill missing values with the average of upper and lower values. But before that, I filtered the data and take only those patients’ data who died in a hospital or survive.\n",
            "SQL\n",
            "MongoDB\n",
            "Google Cloud\n",
            "https://colab.research.google.com/drive/1mo7i32BoEVb0Ac6_CWwJd7_HVbliktx0?usp=sharing\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2156.txt\n",
            "\n",
            "Budget, Sales KPI Dashboard using Power BI\n",
            "\n",
            "\n",
            "PresentationMapDashboardAPI Integration\n",
            "\n",
            "KibanaGoogle Data StudioMicrosoft ExcelMicrosoft Power BI\n",
            "JavaScriptSQLPythonDAX\n",
            "\n",
            "\n",
            "--------------------------------------------------\n",
            "File: bctech2157.txt\n",
            "\n",
            "Amazon Buy Bot, an Automation AI tool to Auto-Checkouts\n",
            "\n",
            "Client: A leading consulting firm in the USA\n",
            "Industry Type: Consulting\n",
            "Services: Management consultant\n",
            "Organization Size: 100+\n",
            "The main objective of this project is to build the automation tool to buy product on amazon.\n",
            "This project is basically completed using selenium and Python. All we have done is write a python script for automation using Selenium.\n",
            "Make some clicks use logics to check item is in stock or not. If the item is in stock then it buys the product otherwise repeat the process again.\n",
            "A simple python code which uses selenium web driver to do all work.\n",
            "Python Code\n",
            "Selenium Webdriver\n",
            "Python\n",
            "Web Scraping\n",
            "Selenium\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the Excel file, handling potential FileNotFoundError\n",
        "try:\n",
        "    df = pd.read_excel('/content/drive/MyDrive/Blackcoffer/Input.xlsx')\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Input.xlsx file not found. Please check the file path.\")\n",
        "    # You might want to add more error handling here, like exiting the script or prompting the user for the correct path.\n",
        "\n",
        "# Specify the output directory in your Google Drive\n",
        "output_dir = '/content/drive/MyDrive/Blackcoffer'\n",
        "\n",
        "# Create a list to store the extracted data\n",
        "extracted_data = []\n",
        "\n",
        "# Iterate through each row\n",
        "for index, row in df.iterrows():\n",
        "    url_id = row['URL_ID']\n",
        "    url = row['URL']\n",
        "\n",
        "    # Fetch the webpage content\n",
        "    try:\n",
        "        response = requests.get(url)\n",
        "        response.raise_for_status()  # Raise an exception for bad status codes (4xx and 5xx)\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        continue  # Skip to the next URL if there's an error\n",
        "\n",
        "    # Parse the HTML content\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extract the article title\n",
        "    title_tag = soup.find('h1', class_='entry-title')\n",
        "    if title_tag:\n",
        "        title = title_tag.text.strip()\n",
        "    else:\n",
        "        title = \"Title not found\"\n",
        "\n",
        "    # Find the main content area\n",
        "    main_content = soup.find('div', class_='td-post-content')\n",
        "\n",
        "    # Remove unwanted elements within the main content\n",
        "    for unwanted_tag in main_content.find_all(['header', 'footer', 'nav', 'aside', 'div', 'figure']):\n",
        "        unwanted_tag.decompose()\n",
        "\n",
        "    # Specifically target paragraphs within the main content\n",
        "    paragraphs = main_content.find_all('p')\n",
        "\n",
        "    # Filter out paragraphs that might contain extraneous information\n",
        "    filtered_paragraphs = [p for p in paragraphs if not ('blackcoffer' in p.text.lower() or 'summarized' in p.text.lower())]\n",
        "\n",
        "    # Extract the article text from the filtered paragraphs, joining with spaces\n",
        "    article_content = \" \".join([p.text.strip() for p in filtered_paragraphs])\n",
        "\n",
        "    # Append the extracted data to the list\n",
        "    extracted_data.append({'URL_ID': url_id, 'Title': title, 'Article_text': article_content})\n",
        "\n",
        "# Create a DataFrame from the extracted data\n",
        "output_df = pd.DataFrame(extracted_data)\n",
        "\n",
        "# Construct the full file path for saving with the new filename\n",
        "file_path = os.path.join(output_dir, \"extracted_data.csv\")\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "output_df.to_csv(file_path, index=False)\n",
        "\n",
        "print(f\"Extracted data saved to {file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P-LZ0L_Yspe",
        "outputId": "dd1c1a33-55bd-4f94-b43e-74f34342fd47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted data saved to /content/drive/MyDrive/Blackcoffer/extracted_data.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Blackcoffer/extracted_data.csv')"
      ],
      "metadata": {
        "id": "YhY1dvAsnKdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "7LL4YfQqrO07",
        "outputId": "c274763f-d159-48d0-a27d-b6cf184df917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       URL_ID                                              Title  \\\n",
              "0  bctech2011  ML and AI-based insurance premium model to pre...   \n",
              "1  bctech2012  Streamlined Integration: Interactive Brokers A...   \n",
              "2  bctech2013  Efficient Data Integration and User-Friendly I...   \n",
              "3  bctech2014  Effective Management of Social Media Data Extr...   \n",
              "4  bctech2015  Streamlined Trading Operations Interface for M...   \n",
              "\n",
              "                                        Article_text  \n",
              "0  Client: A leading insurance firm worldwide Ind...  \n",
              "1  Client: A leading fintech firm in the USA Indu...  \n",
              "2  Client: A leading tech firm in the USA Industr...  \n",
              "3  Client: A leading tech firm in the USA Industr...  \n",
              "4  Client: A leading fintech firm in the USA Indu...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-28bb8e89-69d9-486a-8993-a08eac185675\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL_ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Article_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bctech2011</td>\n",
              "      <td>ML and AI-based insurance premium model to pre...</td>\n",
              "      <td>Client: A leading insurance firm worldwide Ind...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bctech2012</td>\n",
              "      <td>Streamlined Integration: Interactive Brokers A...</td>\n",
              "      <td>Client: A leading fintech firm in the USA Indu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bctech2013</td>\n",
              "      <td>Efficient Data Integration and User-Friendly I...</td>\n",
              "      <td>Client: A leading tech firm in the USA Industr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bctech2014</td>\n",
              "      <td>Effective Management of Social Media Data Extr...</td>\n",
              "      <td>Client: A leading tech firm in the USA Industr...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bctech2015</td>\n",
              "      <td>Streamlined Trading Operations Interface for M...</td>\n",
              "      <td>Client: A leading fintech firm in the USA Indu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-28bb8e89-69d9-486a-8993-a08eac185675')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-28bb8e89-69d9-486a-8993-a08eac185675 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-28bb8e89-69d9-486a-8993-a08eac185675');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-ce783263-bef4-4c31-9da1-185339d2030b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ce783263-bef4-4c31-9da1-185339d2030b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-ce783263-bef4-4c31-9da1-185339d2030b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 147,\n  \"fields\": [\n    {\n      \"column\": \"URL_ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 147,\n        \"samples\": [\n          \"bctech2136\",\n          \"bctech2062\",\n          \"bctech2149\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 147,\n        \"samples\": [\n          \"Lipsync Automation for Celebrities and Influencers\",\n          \"An ETL solution for an Internet Publishing firm\",\n          \"Data ETL: Local Service Ads Leads to BigQuery\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Article_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 147,\n        \"samples\": [\n          \"Client:\\u00a0A leading tech firm in India Industry Type:\\u00a0\\u00a0Entertainment Services:\\u00a0B2C Organization Size:\\u00a0100+ To change the lipsing of the original video with the new replaced audio. We needed to create an output video that will have the new lipsing according to the new replaced audio. Also we will have to change the actual audio with the new audio with automated editing. We have created two different files which will perform 2 different operations 1st will replace the original audio with new and extract only video from original. 2nd will take the muted video and replaced audio and we will get the output of the new replaced audio lipsync. This is done by pre-defined model Wav2Lip on github. 2\\u00a0 google colab notebooks github Google drive Python 3.6 moviepy ffmpeg Wav2lip Python programming Data science Provided by the company (Hrithik Roshan video files) https://colab.research.google.com/drive/18mlREpLmV9hj-uDfufkGJ_-m_E37Hct9?usp=sharing https://colab.research.google.com/drive/1FZHvcVKyJxOUkUFI2auPt3vTOu4jh09K?usp=sharing\",\n          \"Client:\\u00a0A leading internet publishing firm in Singapore and Australia Industry Type:\\u00a0\\u00a0Internet Publishing Services:\\u00a0peer-to-peer car sharing platform where you can rent a large variety of cars, always nearby at great value Organization Size:\\u00a0100+ We need to fetch last month\\u2019s call details (from user, to user, call_time, call_status ) using zendesk api. Then we need to analyse all call logs and need to identify the number of calls made by a particular user to the company and the most recent call timing from the company server. To fetch all call logs using zendesk api we used python language in programming. When we checked call details in the zendesk api, the details were in json format which is very tough to understand the calls details. So first we have fetched only needed details (call made from person, to person and call timing) converted into tabular format. In tabular format it was easy to identify call details. After that we need to identify the number of calls made by the user to the company in the last month.\\u00a0 We used the python pandas module here which is very fast and effective to handle tabular data. First we separated the user who made a call to the company last month and then counted each unique user\\u2019s call records. For recent dates we used python\\u2019s datetime module which can easily identify recent date time. 2 python scripts VS Code, Google Drive, and MS Excel. Python programming language, Data Analytics with numpy and pandas, python datetime. Data Analytics,, Python, Mathematics local data from MS Excel Sheet Here are my contact details: Skype: asbidyarthy WhatsApp: +91 9717367468 Telegram: @asbidyarthy For project discussions and daily updates, would you like to use Slack, Skype, Telegram, or Whatsapp? Please recommend, what would work best for you.\",\n          \"Client:\\u00a0A leading Marketing firm in the USA Industry Type:\\u00a0 Marketing Services:\\u00a0Marketing consulting Organization Size:\\u00a0100+ Upload daily data from Google Local Service Ads dashboard to BigQuery database. A working deployed automated tool that runs everyday in the morning hours and uploads a report to database. Tool is monitored everyday. Heroku LSA API BigQuery API Sheets API Python Data extraction, cleaning and summarising BigQuery \\u2013\\u00a0 lsa_lead_daily_data Heroku Making sure that the data uploaded is for the right company. Monitoring daily logs and uploads for some time and making sure data was correct\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOWUXmB2rP4v",
        "outputId": "4ad9e57e-4958-4403-9ebe-a7d3f652f7ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 147 entries, 0 to 146\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count  Dtype \n",
            "---  ------        --------------  ----- \n",
            " 0   URL_ID        147 non-null    object\n",
            " 1   Title         147 non-null    object\n",
            " 2   Article_text  147 non-null    object\n",
            "dtypes: object(3)\n",
            "memory usage: 3.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSg3Ls4U2aNH",
        "outputId": "79673074-73f2-4bbd-fbc9-d8311ae367cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(147, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/Blackcoffer/extracted_data.csv')\n",
        "\n",
        "# Define paths to StopWords and MasterDictionary folders\n",
        "stopwords_folder_path = '/content/drive/MyDrive/Blackcoffer/StopWords'\n",
        "master_dictionary_folder_path = '/content/drive/MyDrive/Blackcoffer/Master dictionary'\n",
        "\n",
        "# Create sets to store stop words, positive words, and negative words\n",
        "all_stop_words = set()\n",
        "positive_words = set()\n",
        "negative_words = set()\n",
        "\n",
        "# Read stop words from all files in the StopWords folder\n",
        "for filename in os.listdir(stopwords_folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        with open(os.path.join(stopwords_folder_path, filename), 'r', encoding='latin-1') as f:\n",
        "            all_stop_words.update(f.read().splitlines())\n",
        "\n",
        "# Read positive and negative words from files in the MasterDictionary folder\n",
        "for filename in os.listdir(master_dictionary_folder_path):\n",
        "    if filename.endswith('.txt'):\n",
        "        with open(os.path.join(master_dictionary_folder_path, filename), 'r', encoding='latin-1') as f:\n",
        "            words = f.read().splitlines()\n",
        "            if 'positive' in filename.lower():\n",
        "                positive_words.update(words)\n",
        "            elif 'negative' in filename.lower():\n",
        "                negative_words.update(words)\n",
        "\n",
        "# Remove stop words from positive and negative word sets\n",
        "positive_words = positive_words - all_stop_words\n",
        "negative_words = negative_words - all_stop_words\n",
        "\n",
        "# Create the final sentiment dictionary\n",
        "sentiment_dict = {\n",
        "    'positive': list(positive_words),\n",
        "    'negative': list(negative_words)\n",
        "}\n",
        "\n",
        "# Assuming the text column to be cleaned is named 'text'\n",
        "def clean_text(text):\n",
        "    words = text.lower().split()\n",
        "    cleaned_words = [word for word in words if word not in all_stop_words]\n",
        "    return ' '.join(cleaned_words)\n",
        "\n",
        "# Apply the cleaning function to the text column\n",
        "df['Article_text_cleaned'] = df['Article_text'].astype(str).apply(clean_text)\n",
        "\n",
        "# Function to calculate the scores\n",
        "def calculate_scores(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    positive_score = sum([1 for word in tokens if word in sentiment_dict['positive']])\n",
        "    negative_score = sum([1 for word in tokens if word in sentiment_dict['negative']])\n",
        "\n",
        "    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)\n",
        "    subjectivity_score = (positive_score + negative_score) / (len(tokens) + 0.000001)\n",
        "\n",
        "    return positive_score, negative_score, polarity_score, subjectivity_score\n",
        "\n",
        "# Apply the calculation function to the cleaned text column\n",
        "df[['POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE']] = df['Article_text_cleaned'].apply(calculate_scores).apply(pd.Series)"
      ],
      "metadata": {
        "id": "47_kBnupOLnF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the first few rows to get a glimpse of the results\n",
        "print(df[['URL_ID', 'Article_text_cleaned', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO04i3UhO6r9",
        "outputId": "5e31500c-c660-46e4-fe78-a201bb3508b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       URL_ID                               Article_text_cleaned  \\\n",
            "0  bctech2011  client: leading insurance firm worldwide indus...   \n",
            "1  bctech2012  client: leading fintech firm usa industry type...   \n",
            "2  bctech2013  client: leading tech firm usa industry type: p...   \n",
            "3  bctech2014  client: leading tech firm usa industry type: p...   \n",
            "4  bctech2015  client: leading fintech firm usa industry type...   \n",
            "\n",
            "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \n",
            "0            34.0             8.0        0.619048            0.078212  \n",
            "1             1.0             0.0        0.999999            0.043478  \n",
            "2             1.0             0.0        0.999999            0.055556  \n",
            "3             1.0             0.0        0.999999            0.045455  \n",
            "4             1.0             0.0        0.999999            0.043478  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate average sentence length using regex\n",
        "def calculate_avg_sentence_length(text):\n",
        "  sentences = re.split(r'[.?!]+ ', text)\n",
        "  num_words = len(text.split())\n",
        "\n",
        "  # Handle cases where there are no real sentences or only one sentence\n",
        "  if len(sentences) <= 1:\n",
        "    return num_words  # If there's only one sentence, the avg sentence length is the total number of words\n",
        "\n",
        "  avg_sentence_length = num_words / (len(sentences) - 1)\n",
        "  return avg_sentence_length\n",
        "\n",
        "# Apply the function to the 'Article_text' column\n",
        "df['AVG SENTENCE LENGTH'] = df['Article_text'].astype(str).apply(calculate_avg_sentence_length)\n",
        "\n",
        "# Print the first 5 rows with the required columns\n",
        "print(df[['URL_ID', 'Article_text', 'AVG SENTENCE LENGTH']].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0gFT8DwQc4w",
        "outputId": "e0a334b0-a46d-4a21-8d09-e215ada64625"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| URL_ID     | Article_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | AVG SENTENCE LENGTH   |\n",
            "|:-----------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------|\n",
            "| bctech2011 | Client: A leading insurance firm worldwide Industry Type: BFSI Products & Services: Insurance Organization Size: 10000+ The insurance industry, particularly in the context of providing coverage to Public Company Directors against Insider Trading public lawsuits, faces a significant challenge in accurately determining insurance premiums. Traditional methods of premium calculation may lack precision, and there is a growing need for more sophisticated and data-driven approaches. The integration of Artificial Intelligence (AI) and Machine Learning (ML) models in predicting insurance premiums for this specialized coverage is essential to enhance accuracy, fairness, and responsiveness in adapting to evolving risk factors. The problem at hand involves developing robust AI and ML models that can effectively analyze a multitude of dynamic variables influencing the risk profile of Public Company Directors. These variables include market conditions, regulatory changes, historical legal precedents, financial performance of the insured company, and individual directorial behaviors. The goal is to create a predictive model that not only accurately assesses the risk associated with potential insider trading public lawsuits but also adapts in real-time to new information, ensuring that the insurance premiums charged by the global insurance firm are reflective of the current risk landscape. Key Challenges: Addressing these challenges will not only improve the accuracy of insurance premium predictions but also contribute to the overall efficiency and effectiveness of the insurance services provided to Public Company Directors by the leading global insurance firm. To develop an ML and AI-based insurance premium prediction model for Public Company Directors in the USA, safeguarding them against insider trading public lawsuits, we propose a comprehensive solution leveraging advanced machine learning techniques. The goal is to create a model that accurately assesses the risk associated with individual directors and adapts to dynamic market conditions. By adopting this ML and AI-based approach, the insurance company can enhance its ability to predict insurance premiums accurately, adapt to changing risk landscapes, and provide tailored coverage for Public Company Directors against insider trading public lawsuits in the dynamic environment of the USA.   Building an ML and AI-based insurance premium prediction model involves the use of various tools and technologies across different stages of development. Here’s a list of tools and technologies that can be employed for creating such a model for a leading insurance firm in the USA, specifically targeting Public Company Directors against insider trading public lawsuits: It’s important to note that the choice of specific tools may vary based on the preferences of the data science team, the complexity of the model, and the existing technology stack of the insurance company. Additionally, compliance with regulatory requirements and industry standards should be considered in the selection of tools and technologies. The deliverables for an ML and AI-based insurance premium model for Public Company Directors in the USA, aiming to predict premiums for protection against insider trading public lawsuits, would encompass various stages of the development and deployment process. Here is a comprehensive list of deliverables: 1.1 Project Proposal: 1.2 Requirements Document: 2.1 Data Collection Report: 2.2 Cleaned and Preprocessed Dataset: 3.1 Feature Selection and Engineering Report: 4.1 Trained ML Models: 4.2 Model Evaluation Report: 5.1 Real-Time Integration Component: 5.2 Scenario Analysis Module: 6.1 Fairness Assessment Report: 6.2 Explainability Module: 7.1 Deployed API: 7.2 User Interface (UI): 7.3 Documentation for Integration: 8.1 Monitoring Dashboard: 8.2 Automated Model Update Pipeline: 9.1 Model Architecture Document: 9.2 Technical User Manual: 10.1 Training Sessions: 10.2 Knowledge Transfer Documentation: 11.1 Regulatory Compliance Report: 11.2 Data Privacy and Security Documentation: 12.1 Support and Maintenance Plan: By delivering these items, the insurance firm can ensure a thorough and transparent development process, facilitating successful integration and utilization of the ML and AI-based insurance premium prediction model. The implementation of an ML and AI-based insurance premium model for Public Company Directors in the USA, specifically tailored to protect them from insider trading public lawsuits, can have significant business impacts for the leading insurance firm. Here are several potential business impacts: By recognizing and leveraging these business impacts, the leading insurance firm can derive significant value from the implementation of an ML and AI-based insurance premium model tailored for Public Company Directors in the USA. | 40.2941               |\n",
            "| bctech2012 | Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Banking, Financing Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 20                    |\n",
            "| bctech2013 | Client: A leading tech firm in the USA Industry Type: IT Products & Services: IT Consulting Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 19                    |\n",
            "| bctech2014 | Client: A leading tech firm in the USA Industry Type: IT Products & Services: Consulting, Product & Services Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 21                    |\n",
            "| bctech2015 | Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Investment, Financing Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 20                    |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count complex words (words with more than two syllables)\n",
        "def count_complex_words(text):\n",
        "    # Define a helper function to count syllables in a word (approximate)\n",
        "    def count_syllables(word):\n",
        "        vowel_groups = re.findall(r'[aeiouy]+', word.lower())\n",
        "        return len(vowel_groups)\n",
        "\n",
        "    # Count words with more than two syllables\n",
        "    words = text.split()\n",
        "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
        "    return len(complex_words)\n",
        "\n",
        "# Apply the function to calculate the number of complex words for each row\n",
        "df['Complex Word Count'] = df['Article_text'].astype(str).apply(count_complex_words)\n",
        "\n",
        "# Calculate the total number of words in each row\n",
        "df['Total Word Count'] = df['Article_text'].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "# Calculate the percentage of complex words\n",
        "df['PERCENTAGE OF COMPLEX WORDS'] = (df['Complex Word Count'] / df['Total Word Count']) * 100\n",
        "\n",
        "# Print the first 5 rows with relevant columns\n",
        "print(df[['URL_ID', 'Article_text', 'PERCENTAGE OF COMPLEX WORDS']].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VEOxpoFfzyl",
        "outputId": "85409fb9-5a91-4d7c-de11-51df7ba62133"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| URL_ID     | Article_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | PERCENTAGE OF COMPLEX WORDS   |\n",
            "|:-----------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------------------|\n",
            "| bctech2011 | Client: A leading insurance firm worldwide Industry Type: BFSI Products & Services: Insurance Organization Size: 10000+ The insurance industry, particularly in the context of providing coverage to Public Company Directors against Insider Trading public lawsuits, faces a significant challenge in accurately determining insurance premiums. Traditional methods of premium calculation may lack precision, and there is a growing need for more sophisticated and data-driven approaches. The integration of Artificial Intelligence (AI) and Machine Learning (ML) models in predicting insurance premiums for this specialized coverage is essential to enhance accuracy, fairness, and responsiveness in adapting to evolving risk factors. The problem at hand involves developing robust AI and ML models that can effectively analyze a multitude of dynamic variables influencing the risk profile of Public Company Directors. These variables include market conditions, regulatory changes, historical legal precedents, financial performance of the insured company, and individual directorial behaviors. The goal is to create a predictive model that not only accurately assesses the risk associated with potential insider trading public lawsuits but also adapts in real-time to new information, ensuring that the insurance premiums charged by the global insurance firm are reflective of the current risk landscape. Key Challenges: Addressing these challenges will not only improve the accuracy of insurance premium predictions but also contribute to the overall efficiency and effectiveness of the insurance services provided to Public Company Directors by the leading global insurance firm. To develop an ML and AI-based insurance premium prediction model for Public Company Directors in the USA, safeguarding them against insider trading public lawsuits, we propose a comprehensive solution leveraging advanced machine learning techniques. The goal is to create a model that accurately assesses the risk associated with individual directors and adapts to dynamic market conditions. By adopting this ML and AI-based approach, the insurance company can enhance its ability to predict insurance premiums accurately, adapt to changing risk landscapes, and provide tailored coverage for Public Company Directors against insider trading public lawsuits in the dynamic environment of the USA.   Building an ML and AI-based insurance premium prediction model involves the use of various tools and technologies across different stages of development. Here’s a list of tools and technologies that can be employed for creating such a model for a leading insurance firm in the USA, specifically targeting Public Company Directors against insider trading public lawsuits: It’s important to note that the choice of specific tools may vary based on the preferences of the data science team, the complexity of the model, and the existing technology stack of the insurance company. Additionally, compliance with regulatory requirements and industry standards should be considered in the selection of tools and technologies. The deliverables for an ML and AI-based insurance premium model for Public Company Directors in the USA, aiming to predict premiums for protection against insider trading public lawsuits, would encompass various stages of the development and deployment process. Here is a comprehensive list of deliverables: 1.1 Project Proposal: 1.2 Requirements Document: 2.1 Data Collection Report: 2.2 Cleaned and Preprocessed Dataset: 3.1 Feature Selection and Engineering Report: 4.1 Trained ML Models: 4.2 Model Evaluation Report: 5.1 Real-Time Integration Component: 5.2 Scenario Analysis Module: 6.1 Fairness Assessment Report: 6.2 Explainability Module: 7.1 Deployed API: 7.2 User Interface (UI): 7.3 Documentation for Integration: 8.1 Monitoring Dashboard: 8.2 Automated Model Update Pipeline: 9.1 Model Architecture Document: 9.2 Technical User Manual: 10.1 Training Sessions: 10.2 Knowledge Transfer Documentation: 11.1 Regulatory Compliance Report: 11.2 Data Privacy and Security Documentation: 12.1 Support and Maintenance Plan: By delivering these items, the insurance firm can ensure a thorough and transparent development process, facilitating successful integration and utilization of the ML and AI-based insurance premium prediction model. The implementation of an ML and AI-based insurance premium model for Public Company Directors in the USA, specifically tailored to protect them from insider trading public lawsuits, can have significant business impacts for the leading insurance firm. Here are several potential business impacts: By recognizing and leveraging these business impacts, the leading insurance firm can derive significant value from the implementation of an ML and AI-based insurance premium model tailored for Public Company Directors in the USA. | 36.3504                       |\n",
            "| bctech2012 | Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Banking, Financing Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 25                            |\n",
            "| bctech2013 | Client: A leading tech firm in the USA Industry Type: IT Products & Services: IT Consulting Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 21.0526                       |\n",
            "| bctech2014 | Client: A leading tech firm in the USA Industry Type: IT Products & Services: Consulting, Product & Services Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 23.8095                       |\n",
            "| bctech2015 | Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Investment, Financing Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 30                            |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Fog Index\n",
        "df['FOG INDEX'] = 0.4 * (df['AVG SENTENCE LENGTH'] + df['PERCENTAGE OF COMPLEX WORDS'])\n",
        "\n",
        "# Print the first 5 rows with relevant columns (including Fog Index)\n",
        "print(df[['URL_ID', 'Article_text', 'AVG SENTENCE LENGTH', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX']].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6O7SdlpisfP",
        "outputId": "61570618-3f7e-4562-d2c1-b21464fe889e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| URL_ID     | Article_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | AVG SENTENCE LENGTH   | PERCENTAGE OF COMPLEX WORDS   | FOG INDEX   |\n",
            "|:-----------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:----------------------|:------------------------------|:------------|\n",
            "| bctech2011 | Client: A leading insurance firm worldwide Industry Type: BFSI Products & Services: Insurance Organization Size: 10000+ The insurance industry, particularly in the context of providing coverage to Public Company Directors against Insider Trading public lawsuits, faces a significant challenge in accurately determining insurance premiums. Traditional methods of premium calculation may lack precision, and there is a growing need for more sophisticated and data-driven approaches. The integration of Artificial Intelligence (AI) and Machine Learning (ML) models in predicting insurance premiums for this specialized coverage is essential to enhance accuracy, fairness, and responsiveness in adapting to evolving risk factors. The problem at hand involves developing robust AI and ML models that can effectively analyze a multitude of dynamic variables influencing the risk profile of Public Company Directors. These variables include market conditions, regulatory changes, historical legal precedents, financial performance of the insured company, and individual directorial behaviors. The goal is to create a predictive model that not only accurately assesses the risk associated with potential insider trading public lawsuits but also adapts in real-time to new information, ensuring that the insurance premiums charged by the global insurance firm are reflective of the current risk landscape. Key Challenges: Addressing these challenges will not only improve the accuracy of insurance premium predictions but also contribute to the overall efficiency and effectiveness of the insurance services provided to Public Company Directors by the leading global insurance firm. To develop an ML and AI-based insurance premium prediction model for Public Company Directors in the USA, safeguarding them against insider trading public lawsuits, we propose a comprehensive solution leveraging advanced machine learning techniques. The goal is to create a model that accurately assesses the risk associated with individual directors and adapts to dynamic market conditions. By adopting this ML and AI-based approach, the insurance company can enhance its ability to predict insurance premiums accurately, adapt to changing risk landscapes, and provide tailored coverage for Public Company Directors against insider trading public lawsuits in the dynamic environment of the USA.   Building an ML and AI-based insurance premium prediction model involves the use of various tools and technologies across different stages of development. Here’s a list of tools and technologies that can be employed for creating such a model for a leading insurance firm in the USA, specifically targeting Public Company Directors against insider trading public lawsuits: It’s important to note that the choice of specific tools may vary based on the preferences of the data science team, the complexity of the model, and the existing technology stack of the insurance company. Additionally, compliance with regulatory requirements and industry standards should be considered in the selection of tools and technologies. The deliverables for an ML and AI-based insurance premium model for Public Company Directors in the USA, aiming to predict premiums for protection against insider trading public lawsuits, would encompass various stages of the development and deployment process. Here is a comprehensive list of deliverables: 1.1 Project Proposal: 1.2 Requirements Document: 2.1 Data Collection Report: 2.2 Cleaned and Preprocessed Dataset: 3.1 Feature Selection and Engineering Report: 4.1 Trained ML Models: 4.2 Model Evaluation Report: 5.1 Real-Time Integration Component: 5.2 Scenario Analysis Module: 6.1 Fairness Assessment Report: 6.2 Explainability Module: 7.1 Deployed API: 7.2 User Interface (UI): 7.3 Documentation for Integration: 8.1 Monitoring Dashboard: 8.2 Automated Model Update Pipeline: 9.1 Model Architecture Document: 9.2 Technical User Manual: 10.1 Training Sessions: 10.2 Knowledge Transfer Documentation: 11.1 Regulatory Compliance Report: 11.2 Data Privacy and Security Documentation: 12.1 Support and Maintenance Plan: By delivering these items, the insurance firm can ensure a thorough and transparent development process, facilitating successful integration and utilization of the ML and AI-based insurance premium prediction model. The implementation of an ML and AI-based insurance premium model for Public Company Directors in the USA, specifically tailored to protect them from insider trading public lawsuits, can have significant business impacts for the leading insurance firm. Here are several potential business impacts: By recognizing and leveraging these business impacts, the leading insurance firm can derive significant value from the implementation of an ML and AI-based insurance premium model tailored for Public Company Directors in the USA. | 40.2941               | 36.3504                       | 30.6578     |\n",
            "| bctech2012 | Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Banking, Financing Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 20                    | 25                            | 18          |\n",
            "| bctech2013 | Client: A leading tech firm in the USA Industry Type: IT Products & Services: IT Consulting Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 19                    | 21.0526                       | 16.0211     |\n",
            "| bctech2014 | Client: A leading tech firm in the USA Industry Type: IT Products & Services: Consulting, Product & Services Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 21                    | 23.8095                       | 17.9238     |\n",
            "| bctech2015 | Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Investment, Financing Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 20                    | 30                            | 20          |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count sentences using regex (you already have this)\n",
        "\n",
        "def count_sentences(text):\n",
        "    # Improved regex to handle more cases\n",
        "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
        "\n",
        "    # Filter out empty sentences\n",
        "    sentences = [sentence for sentence in sentences if sentence.strip()]\n",
        "\n",
        "    return len(sentences)\n",
        "# Apply the function to create the 'total_sentences' column\n",
        "df['total_sentences'] = df['Article_text'].astype(str).apply(count_sentences)\n",
        "\n",
        "# Calculate the total number of words in each article\n",
        "df['the total number of words'] = df['Article_text'].astype(str).apply(lambda x: len(x.split()))\n",
        "\n",
        "# Calculate the average number of words per sentence\n",
        "df['AVG NUMBER OF WORDS PER SENTENCE'] = df['the total number of words'] / df['total_sentences']\n",
        "\n",
        "# Print the first 5 rows with the required columns\n",
        "print(df[['URL_ID', 'Article_text', 'total_sentences', 'the total number of words', 'AVG NUMBER OF WORDS PER SENTENCE']].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUp_m4zsq8ET",
        "outputId": "aec82592-9463-497b-9968-1dc3be55d64f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| URL_ID     | Article_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | total_sentences   | the total number of words   | AVG NUMBER OF WORDS PER SENTENCE   |\n",
            "|:-----------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:------------------|:----------------------------|:-----------------------------------|\n",
            "| bctech2011 | Client: A leading insurance firm worldwide Industry Type: BFSI Products & Services: Insurance Organization Size: 10000+ The insurance industry, particularly in the context of providing coverage to Public Company Directors against Insider Trading public lawsuits, faces a significant challenge in accurately determining insurance premiums. Traditional methods of premium calculation may lack precision, and there is a growing need for more sophisticated and data-driven approaches. The integration of Artificial Intelligence (AI) and Machine Learning (ML) models in predicting insurance premiums for this specialized coverage is essential to enhance accuracy, fairness, and responsiveness in adapting to evolving risk factors. The problem at hand involves developing robust AI and ML models that can effectively analyze a multitude of dynamic variables influencing the risk profile of Public Company Directors. These variables include market conditions, regulatory changes, historical legal precedents, financial performance of the insured company, and individual directorial behaviors. The goal is to create a predictive model that not only accurately assesses the risk associated with potential insider trading public lawsuits but also adapts in real-time to new information, ensuring that the insurance premiums charged by the global insurance firm are reflective of the current risk landscape. Key Challenges: Addressing these challenges will not only improve the accuracy of insurance premium predictions but also contribute to the overall efficiency and effectiveness of the insurance services provided to Public Company Directors by the leading global insurance firm. To develop an ML and AI-based insurance premium prediction model for Public Company Directors in the USA, safeguarding them against insider trading public lawsuits, we propose a comprehensive solution leveraging advanced machine learning techniques. The goal is to create a model that accurately assesses the risk associated with individual directors and adapts to dynamic market conditions. By adopting this ML and AI-based approach, the insurance company can enhance its ability to predict insurance premiums accurately, adapt to changing risk landscapes, and provide tailored coverage for Public Company Directors against insider trading public lawsuits in the dynamic environment of the USA.   Building an ML and AI-based insurance premium prediction model involves the use of various tools and technologies across different stages of development. Here’s a list of tools and technologies that can be employed for creating such a model for a leading insurance firm in the USA, specifically targeting Public Company Directors against insider trading public lawsuits: It’s important to note that the choice of specific tools may vary based on the preferences of the data science team, the complexity of the model, and the existing technology stack of the insurance company. Additionally, compliance with regulatory requirements and industry standards should be considered in the selection of tools and technologies. The deliverables for an ML and AI-based insurance premium model for Public Company Directors in the USA, aiming to predict premiums for protection against insider trading public lawsuits, would encompass various stages of the development and deployment process. Here is a comprehensive list of deliverables: 1.1 Project Proposal: 1.2 Requirements Document: 2.1 Data Collection Report: 2.2 Cleaned and Preprocessed Dataset: 3.1 Feature Selection and Engineering Report: 4.1 Trained ML Models: 4.2 Model Evaluation Report: 5.1 Real-Time Integration Component: 5.2 Scenario Analysis Module: 6.1 Fairness Assessment Report: 6.2 Explainability Module: 7.1 Deployed API: 7.2 User Interface (UI): 7.3 Documentation for Integration: 8.1 Monitoring Dashboard: 8.2 Automated Model Update Pipeline: 9.1 Model Architecture Document: 9.2 Technical User Manual: 10.1 Training Sessions: 10.2 Knowledge Transfer Documentation: 11.1 Regulatory Compliance Report: 11.2 Data Privacy and Security Documentation: 12.1 Support and Maintenance Plan: By delivering these items, the insurance firm can ensure a thorough and transparent development process, facilitating successful integration and utilization of the ML and AI-based insurance premium prediction model. The implementation of an ML and AI-based insurance premium model for Public Company Directors in the USA, specifically tailored to protect them from insider trading public lawsuits, can have significant business impacts for the leading insurance firm. Here are several potential business impacts: By recognizing and leveraging these business impacts, the leading insurance firm can derive significant value from the implementation of an ML and AI-based insurance premium model tailored for Public Company Directors in the USA. | 17                | 685                         | 40.2941                            |\n",
            "| bctech2012 | Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Banking, Financing Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 1                 | 20                          | 20                                 |\n",
            "| bctech2013 | Client: A leading tech firm in the USA Industry Type: IT Products & Services: IT Consulting Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 1                 | 19                          | 19                                 |\n",
            "| bctech2014 | Client: A leading tech firm in the USA Industry Type: IT Products & Services: Consulting, Product & Services Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 1                 | 21                          | 21                                 |\n",
            "| bctech2015 | Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Investment, Financing Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 1                 | 20                          | 20                                 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking once\n",
        "text = df['Article_text'].iloc[0]  # Get the first row's text\n",
        "sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?)\\s', text)\n",
        "\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nSplit Sentences:\")\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(f\"{i+1}. {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vy2wN-G1rAWR",
        "outputId": "5ee9a79f-ffea-43ee-a169-aa90c7e343ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "Client: A leading insurance firm worldwide Industry Type: BFSI Products & Services: Insurance Organization Size: 10000+ The insurance industry, particularly in the context of providing coverage to Public Company Directors against Insider Trading public lawsuits, faces a significant challenge in accurately determining insurance premiums. Traditional methods of premium calculation may lack precision, and there is a growing need for more sophisticated and data-driven approaches. The integration of Artificial Intelligence (AI) and Machine Learning (ML) models in predicting insurance premiums for this specialized coverage is essential to enhance accuracy, fairness, and responsiveness in adapting to evolving risk factors. The problem at hand involves developing robust AI and ML models that can effectively analyze a multitude of dynamic variables influencing the risk profile of Public Company Directors. These variables include market conditions, regulatory changes, historical legal precedents, financial performance of the insured company, and individual directorial behaviors. The goal is to create a predictive model that not only accurately assesses the risk associated with potential insider trading public lawsuits but also adapts in real-time to new information, ensuring that the insurance premiums charged by the global insurance firm are reflective of the current risk landscape. Key Challenges: Addressing these challenges will not only improve the accuracy of insurance premium predictions but also contribute to the overall efficiency and effectiveness of the insurance services provided to Public Company Directors by the leading global insurance firm. To develop an ML and AI-based insurance premium prediction model for Public Company Directors in the USA, safeguarding them against insider trading public lawsuits, we propose a comprehensive solution leveraging advanced machine learning techniques. The goal is to create a model that accurately assesses the risk associated with individual directors and adapts to dynamic market conditions. By adopting this ML and AI-based approach, the insurance company can enhance its ability to predict insurance premiums accurately, adapt to changing risk landscapes, and provide tailored coverage for Public Company Directors against insider trading public lawsuits in the dynamic environment of the USA.   Building an ML and AI-based insurance premium prediction model involves the use of various tools and technologies across different stages of development. Here’s a list of tools and technologies that can be employed for creating such a model for a leading insurance firm in the USA, specifically targeting Public Company Directors against insider trading public lawsuits: It’s important to note that the choice of specific tools may vary based on the preferences of the data science team, the complexity of the model, and the existing technology stack of the insurance company. Additionally, compliance with regulatory requirements and industry standards should be considered in the selection of tools and technologies. The deliverables for an ML and AI-based insurance premium model for Public Company Directors in the USA, aiming to predict premiums for protection against insider trading public lawsuits, would encompass various stages of the development and deployment process. Here is a comprehensive list of deliverables: 1.1 Project Proposal: 1.2 Requirements Document: 2.1 Data Collection Report: 2.2 Cleaned and Preprocessed Dataset: 3.1 Feature Selection and Engineering Report: 4.1 Trained ML Models: 4.2 Model Evaluation Report: 5.1 Real-Time Integration Component: 5.2 Scenario Analysis Module: 6.1 Fairness Assessment Report: 6.2 Explainability Module: 7.1 Deployed API: 7.2 User Interface (UI): 7.3 Documentation for Integration: 8.1 Monitoring Dashboard: 8.2 Automated Model Update Pipeline: 9.1 Model Architecture Document: 9.2 Technical User Manual: 10.1 Training Sessions: 10.2 Knowledge Transfer Documentation: 11.1 Regulatory Compliance Report: 11.2 Data Privacy and Security Documentation: 12.1 Support and Maintenance Plan: By delivering these items, the insurance firm can ensure a thorough and transparent development process, facilitating successful integration and utilization of the ML and AI-based insurance premium prediction model. The implementation of an ML and AI-based insurance premium model for Public Company Directors in the USA, specifically tailored to protect them from insider trading public lawsuits, can have significant business impacts for the leading insurance firm. Here are several potential business impacts: By recognizing and leveraging these business impacts, the leading insurance firm can derive significant value from the implementation of an ML and AI-based insurance premium model tailored for Public Company Directors in the USA.  \n",
            "\n",
            "Split Sentences:\n",
            "1. Client: A leading insurance firm worldwide Industry Type: BFSI Products & Services: Insurance Organization Size: 10000+ The insurance industry, particularly in the context of providing coverage to Public Company Directors against Insider Trading public lawsuits, faces a significant challenge in accurately determining insurance premiums.\n",
            "2. Traditional methods of premium calculation may lack precision, and there is a growing need for more sophisticated and data-driven approaches.\n",
            "3. The integration of Artificial Intelligence (AI) and Machine Learning (ML) models in predicting insurance premiums for this specialized coverage is essential to enhance accuracy, fairness, and responsiveness in adapting to evolving risk factors.\n",
            "4. The problem at hand involves developing robust AI and ML models that can effectively analyze a multitude of dynamic variables influencing the risk profile of Public Company Directors.\n",
            "5. These variables include market conditions, regulatory changes, historical legal precedents, financial performance of the insured company, and individual directorial behaviors.\n",
            "6. The goal is to create a predictive model that not only accurately assesses the risk associated with potential insider trading public lawsuits but also adapts in real-time to new information, ensuring that the insurance premiums charged by the global insurance firm are reflective of the current risk landscape.\n",
            "7. Key Challenges: Addressing these challenges will not only improve the accuracy of insurance premium predictions but also contribute to the overall efficiency and effectiveness of the insurance services provided to Public Company Directors by the leading global insurance firm.\n",
            "8. To develop an ML and AI-based insurance premium prediction model for Public Company Directors in the USA, safeguarding them against insider trading public lawsuits, we propose a comprehensive solution leveraging advanced machine learning techniques.\n",
            "9. The goal is to create a model that accurately assesses the risk associated with individual directors and adapts to dynamic market conditions.\n",
            "10. By adopting this ML and AI-based approach, the insurance company can enhance its ability to predict insurance premiums accurately, adapt to changing risk landscapes, and provide tailored coverage for Public Company Directors against insider trading public lawsuits in the dynamic environment of the USA.\n",
            "11.   Building an ML and AI-based insurance premium prediction model involves the use of various tools and technologies across different stages of development.\n",
            "12. Here’s a list of tools and technologies that can be employed for creating such a model for a leading insurance firm in the USA, specifically targeting Public Company Directors against insider trading public lawsuits: It’s important to note that the choice of specific tools may vary based on the preferences of the data science team, the complexity of the model, and the existing technology stack of the insurance company.\n",
            "13. Additionally, compliance with regulatory requirements and industry standards should be considered in the selection of tools and technologies.\n",
            "14. The deliverables for an ML and AI-based insurance premium model for Public Company Directors in the USA, aiming to predict premiums for protection against insider trading public lawsuits, would encompass various stages of the development and deployment process.\n",
            "15. Here is a comprehensive list of deliverables: 1.1 Project Proposal: 1.2 Requirements Document: 2.1 Data Collection Report: 2.2 Cleaned and Preprocessed Dataset: 3.1 Feature Selection and Engineering Report: 4.1 Trained ML Models: 4.2 Model Evaluation Report: 5.1 Real-Time Integration Component: 5.2 Scenario Analysis Module: 6.1 Fairness Assessment Report: 6.2 Explainability Module: 7.1 Deployed API: 7.2 User Interface (UI): 7.3 Documentation for Integration: 8.1 Monitoring Dashboard: 8.2 Automated Model Update Pipeline: 9.1 Model Architecture Document: 9.2 Technical User Manual: 10.1 Training Sessions: 10.2 Knowledge Transfer Documentation: 11.1 Regulatory Compliance Report: 11.2 Data Privacy and Security Documentation: 12.1 Support and Maintenance Plan: By delivering these items, the insurance firm can ensure a thorough and transparent development process, facilitating successful integration and utilization of the ML and AI-based insurance premium prediction model.\n",
            "16. The implementation of an ML and AI-based insurance premium model for Public Company Directors in the USA, specifically tailored to protect them from insider trading public lawsuits, can have significant business impacts for the leading insurance firm.\n",
            "17. Here are several potential business impacts: By recognizing and leveraging these business impacts, the leading insurance firm can derive significant value from the implementation of an ML and AI-based insurance premium model tailored for Public Company Directors in the USA.\n",
            "18.  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count complex words (words with more than two syllables)\n",
        "def count_complex_words(text):\n",
        "    # Define a helper function to count syllables in a word (approximate)\n",
        "    def count_syllables(word):\n",
        "        vowel_groups = re.findall(r'[aeiouy]+', word.lower())\n",
        "        return len(vowel_groups)\n",
        "\n",
        "    # Count words with more than two syllables\n",
        "    words = text.split()\n",
        "    complex_words = [word for word in words if count_syllables(word) > 2]\n",
        "    return len(complex_words)\n",
        "\n",
        "# Apply the function to calculate the number of complex words for each row\n",
        "df['COMPLEX WORD COUNT'] = df['Article_text'].astype(str).apply(count_complex_words)\n",
        "print(df[['URL_ID', 'Article_text', 'COMPLEX WORD COUNT']].head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTTpYFkmswfd",
        "outputId": "75fd9648-67ea-467c-aa6a-6c8a8b6538f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| URL_ID     | Article_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | COMPLEX WORD COUNT   |\n",
            "|:-----------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------|\n",
            "| bctech2011 | Client: A leading insurance firm worldwide Industry Type: BFSI Products & Services: Insurance Organization Size: 10000+ The insurance industry, particularly in the context of providing coverage to Public Company Directors against Insider Trading public lawsuits, faces a significant challenge in accurately determining insurance premiums. Traditional methods of premium calculation may lack precision, and there is a growing need for more sophisticated and data-driven approaches. The integration of Artificial Intelligence (AI) and Machine Learning (ML) models in predicting insurance premiums for this specialized coverage is essential to enhance accuracy, fairness, and responsiveness in adapting to evolving risk factors. The problem at hand involves developing robust AI and ML models that can effectively analyze a multitude of dynamic variables influencing the risk profile of Public Company Directors. These variables include market conditions, regulatory changes, historical legal precedents, financial performance of the insured company, and individual directorial behaviors. The goal is to create a predictive model that not only accurately assesses the risk associated with potential insider trading public lawsuits but also adapts in real-time to new information, ensuring that the insurance premiums charged by the global insurance firm are reflective of the current risk landscape. Key Challenges: Addressing these challenges will not only improve the accuracy of insurance premium predictions but also contribute to the overall efficiency and effectiveness of the insurance services provided to Public Company Directors by the leading global insurance firm. To develop an ML and AI-based insurance premium prediction model for Public Company Directors in the USA, safeguarding them against insider trading public lawsuits, we propose a comprehensive solution leveraging advanced machine learning techniques. The goal is to create a model that accurately assesses the risk associated with individual directors and adapts to dynamic market conditions. By adopting this ML and AI-based approach, the insurance company can enhance its ability to predict insurance premiums accurately, adapt to changing risk landscapes, and provide tailored coverage for Public Company Directors against insider trading public lawsuits in the dynamic environment of the USA.   Building an ML and AI-based insurance premium prediction model involves the use of various tools and technologies across different stages of development. Here’s a list of tools and technologies that can be employed for creating such a model for a leading insurance firm in the USA, specifically targeting Public Company Directors against insider trading public lawsuits: It’s important to note that the choice of specific tools may vary based on the preferences of the data science team, the complexity of the model, and the existing technology stack of the insurance company. Additionally, compliance with regulatory requirements and industry standards should be considered in the selection of tools and technologies. The deliverables for an ML and AI-based insurance premium model for Public Company Directors in the USA, aiming to predict premiums for protection against insider trading public lawsuits, would encompass various stages of the development and deployment process. Here is a comprehensive list of deliverables: 1.1 Project Proposal: 1.2 Requirements Document: 2.1 Data Collection Report: 2.2 Cleaned and Preprocessed Dataset: 3.1 Feature Selection and Engineering Report: 4.1 Trained ML Models: 4.2 Model Evaluation Report: 5.1 Real-Time Integration Component: 5.2 Scenario Analysis Module: 6.1 Fairness Assessment Report: 6.2 Explainability Module: 7.1 Deployed API: 7.2 User Interface (UI): 7.3 Documentation for Integration: 8.1 Monitoring Dashboard: 8.2 Automated Model Update Pipeline: 9.1 Model Architecture Document: 9.2 Technical User Manual: 10.1 Training Sessions: 10.2 Knowledge Transfer Documentation: 11.1 Regulatory Compliance Report: 11.2 Data Privacy and Security Documentation: 12.1 Support and Maintenance Plan: By delivering these items, the insurance firm can ensure a thorough and transparent development process, facilitating successful integration and utilization of the ML and AI-based insurance premium prediction model. The implementation of an ML and AI-based insurance premium model for Public Company Directors in the USA, specifically tailored to protect them from insider trading public lawsuits, can have significant business impacts for the leading insurance firm. Here are several potential business impacts: By recognizing and leveraging these business impacts, the leading insurance firm can derive significant value from the implementation of an ML and AI-based insurance premium model tailored for Public Company Directors in the USA. | 249                  |\n",
            "| bctech2012 | Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Banking, Financing Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | 5                    |\n",
            "| bctech2013 | Client: A leading tech firm in the USA Industry Type: IT Products & Services: IT Consulting Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | 4                    |\n",
            "| bctech2014 | Client: A leading tech firm in the USA Industry Type: IT Products & Services: Consulting, Product & Services Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | 5                    |\n",
            "| bctech2015 | Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Investment, Financing Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 6                    |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Function to clean and count words\n",
        "def clean_and_count_words(text):\n",
        "    # Remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    cleaned_text = text.translate(translator)\n",
        "\n",
        "    # Tokenize into words\n",
        "    words = cleaned_text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "    return len(filtered_words)\n",
        "\n",
        "# Apply the function to create the 'Word Count' column\n",
        "df['WORD COUNT'] = df['Article_text'].apply(clean_and_count_words)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLruVH4Z3Gnh",
        "outputId": "1e68aaf5-cad0-43cd-be52-00954ca1520b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         URL_ID                                              Title  \\\n",
            "0    bctech2011  ML and AI-based insurance premium model to pre...   \n",
            "1    bctech2012  Streamlined Integration: Interactive Brokers A...   \n",
            "2    bctech2013  Efficient Data Integration and User-Friendly I...   \n",
            "3    bctech2014  Effective Management of Social Media Data Extr...   \n",
            "4    bctech2015  Streamlined Trading Operations Interface for M...   \n",
            "..          ...                                                ...   \n",
            "142  bctech2153         Population and Community Survey of America   \n",
            "143  bctech2154    Google LSA API Data Automation and Dashboarding   \n",
            "144  bctech2155                           Healthcare Data Analysis   \n",
            "145  bctech2156         Budget, Sales KPI Dashboard using Power BI   \n",
            "146  bctech2157  Amazon Buy Bot, an Automation AI tool to Auto-...   \n",
            "\n",
            "                                          Article_text  \\\n",
            "0    Client: A leading insurance firm worldwide Ind...   \n",
            "1    Client: A leading fintech firm in the USA Indu...   \n",
            "2    Client: A leading tech firm in the USA Industr...   \n",
            "3    Client: A leading tech firm in the USA Industr...   \n",
            "4    Client: A leading fintech firm in the USA Indu...   \n",
            "..                                                 ...   \n",
            "142  Client: A leading marketing firm in the USA In...   \n",
            "143  Client: A leading marketing firm in the USA In...   \n",
            "144  Client: A leading healthcare tech firm in the ...   \n",
            "145   PresentationMapDashboardAPI Integration  Kiba...   \n",
            "146  Client: A leading consulting firm in the USA I...   \n",
            "\n",
            "                                  Article_text_cleaned  POSITIVE SCORE  \\\n",
            "0    client: leading insurance firm worldwide indus...            34.0   \n",
            "1    client: leading fintech firm usa industry type...             1.0   \n",
            "2    client: leading tech firm usa industry type: p...             1.0   \n",
            "3    client: leading tech firm usa industry type: p...             1.0   \n",
            "4    client: leading fintech firm usa industry type...             1.0   \n",
            "..                                                 ...             ...   \n",
            "142  client: leading marketing firm usa industry ty...            13.0   \n",
            "143  client: leading marketing firm usa industry ty...            21.0   \n",
            "144  client: leading healthcare tech firm usa indus...             6.0   \n",
            "145  presentationmapdashboardapi integration kibana...             0.0   \n",
            "146  client: leading consulting firm usa industry t...             2.0   \n",
            "\n",
            "     NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
            "0               8.0        0.619048            0.078212            40.294118   \n",
            "1               0.0        0.999999            0.043478            20.000000   \n",
            "2               0.0        0.999999            0.055556            19.000000   \n",
            "3               0.0        0.999999            0.045455            21.000000   \n",
            "4               0.0        0.999999            0.043478            20.000000   \n",
            "..              ...             ...                 ...                  ...   \n",
            "142             9.0        0.181818            0.034646            25.787879   \n",
            "143            19.0        0.050000            0.040445            20.621212   \n",
            "144             7.0       -0.076923            0.089655            22.600000   \n",
            "145             0.0        0.000000            0.000000             9.000000   \n",
            "146             0.0        1.000000            0.027778            17.666667   \n",
            "\n",
            "     Complex Word Count  Total Word Count  PERCENTAGE OF COMPLEX WORDS  \\\n",
            "0                   249               685                    36.350365   \n",
            "1                     5                20                    25.000000   \n",
            "2                     4                19                    21.052632   \n",
            "3                     5                21                    23.809524   \n",
            "4                     6                20                    30.000000   \n",
            "..                  ...               ...                          ...   \n",
            "142                 210               851                    24.676851   \n",
            "143                 296              1361                    21.748714   \n",
            "144                  32               226                    14.159292   \n",
            "145                   6                 9                    66.666667   \n",
            "146                  20               106                    18.867925   \n",
            "\n",
            "     FOG INDEX  total_sentences  the total number of words  \\\n",
            "0    30.657793               17                        685   \n",
            "1    18.000000                1                         20   \n",
            "2    16.021053                1                         19   \n",
            "3    17.923810                1                         21   \n",
            "4    20.000000                1                         20   \n",
            "..         ...              ...                        ...   \n",
            "142  20.185892               34                        851   \n",
            "143  16.947971               66                       1361   \n",
            "144  14.703717               11                        226   \n",
            "145  30.266667                1                          9   \n",
            "146  14.613836                7                        106   \n",
            "\n",
            "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \n",
            "0                           40.294118                 249         471  \n",
            "1                           20.000000                   5          16  \n",
            "2                           19.000000                   4          13  \n",
            "3                           21.000000                   5          15  \n",
            "4                           20.000000                   6          16  \n",
            "..                                ...                 ...         ...  \n",
            "142                         25.029412                 210         588  \n",
            "143                         20.621212                 296         900  \n",
            "144                         20.545455                  32         124  \n",
            "145                          9.000000                   6           9  \n",
            "146                         15.142857                  20          66  \n",
            "\n",
            "[147 rows x 18 columns]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to count syllables in a word\n",
        "def count_syllables(word):\n",
        "    # Remove any trailing 'es' or 'ed'\n",
        "    word = re.sub(r'(es|ed)$', '', word.lower())\n",
        "\n",
        "    # Count the vowels (assuming a simple rule: each vowel is a syllable)\n",
        "    return sum(1 for char in word if char in 'aeiouy')\n",
        "\n",
        "# Apply the function to create the 'SYLLABLE PER WORD' column\n",
        "df['SYLLABLE PER WORD'] = df['Article_text'].apply(lambda Article_text: sum(count_syllables(word) for word in Article_text.split()))\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HoLEAFaB86YM",
        "outputId": "89bd7032-b56a-4bf8-91b4-4daab41ada16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         URL_ID                                              Title  \\\n",
            "0    bctech2011  ML and AI-based insurance premium model to pre...   \n",
            "1    bctech2012  Streamlined Integration: Interactive Brokers A...   \n",
            "2    bctech2013  Efficient Data Integration and User-Friendly I...   \n",
            "3    bctech2014  Effective Management of Social Media Data Extr...   \n",
            "4    bctech2015  Streamlined Trading Operations Interface for M...   \n",
            "..          ...                                                ...   \n",
            "142  bctech2153         Population and Community Survey of America   \n",
            "143  bctech2154    Google LSA API Data Automation and Dashboarding   \n",
            "144  bctech2155                           Healthcare Data Analysis   \n",
            "145  bctech2156         Budget, Sales KPI Dashboard using Power BI   \n",
            "146  bctech2157  Amazon Buy Bot, an Automation AI tool to Auto-...   \n",
            "\n",
            "                                          Article_text  \\\n",
            "0    Client: A leading insurance firm worldwide Ind...   \n",
            "1    Client: A leading fintech firm in the USA Indu...   \n",
            "2    Client: A leading tech firm in the USA Industr...   \n",
            "3    Client: A leading tech firm in the USA Industr...   \n",
            "4    Client: A leading fintech firm in the USA Indu...   \n",
            "..                                                 ...   \n",
            "142  Client: A leading marketing firm in the USA In...   \n",
            "143  Client: A leading marketing firm in the USA In...   \n",
            "144  Client: A leading healthcare tech firm in the ...   \n",
            "145   PresentationMapDashboardAPI Integration  Kiba...   \n",
            "146  Client: A leading consulting firm in the USA I...   \n",
            "\n",
            "                                  Article_text_cleaned  POSITIVE SCORE  \\\n",
            "0    client: leading insurance firm worldwide indus...            34.0   \n",
            "1    client: leading fintech firm usa industry type...             1.0   \n",
            "2    client: leading tech firm usa industry type: p...             1.0   \n",
            "3    client: leading tech firm usa industry type: p...             1.0   \n",
            "4    client: leading fintech firm usa industry type...             1.0   \n",
            "..                                                 ...             ...   \n",
            "142  client: leading marketing firm usa industry ty...            13.0   \n",
            "143  client: leading marketing firm usa industry ty...            21.0   \n",
            "144  client: leading healthcare tech firm usa indus...             6.0   \n",
            "145  presentationmapdashboardapi integration kibana...             0.0   \n",
            "146  client: leading consulting firm usa industry t...             2.0   \n",
            "\n",
            "     NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
            "0               8.0        0.619048            0.078212            40.294118   \n",
            "1               0.0        0.999999            0.043478            20.000000   \n",
            "2               0.0        0.999999            0.055556            19.000000   \n",
            "3               0.0        0.999999            0.045455            21.000000   \n",
            "4               0.0        0.999999            0.043478            20.000000   \n",
            "..              ...             ...                 ...                  ...   \n",
            "142             9.0        0.181818            0.034646            25.787879   \n",
            "143            19.0        0.050000            0.040445            20.621212   \n",
            "144             7.0       -0.076923            0.089655            22.600000   \n",
            "145             0.0        0.000000            0.000000             9.000000   \n",
            "146             0.0        1.000000            0.027778            17.666667   \n",
            "\n",
            "     Complex Word Count  Total Word Count  PERCENTAGE OF COMPLEX WORDS  \\\n",
            "0                   249               685                    36.350365   \n",
            "1                     5                20                    25.000000   \n",
            "2                     4                19                    21.052632   \n",
            "3                     5                21                    23.809524   \n",
            "4                     6                20                    30.000000   \n",
            "..                  ...               ...                          ...   \n",
            "142                 210               851                    24.676851   \n",
            "143                 296              1361                    21.748714   \n",
            "144                  32               226                    14.159292   \n",
            "145                   6                 9                    66.666667   \n",
            "146                  20               106                    18.867925   \n",
            "\n",
            "     FOG INDEX  total_sentences  the total number of words  \\\n",
            "0    30.657793               17                        685   \n",
            "1    18.000000                1                         20   \n",
            "2    16.021053                1                         19   \n",
            "3    17.923810                1                         21   \n",
            "4    20.000000                1                         20   \n",
            "..         ...              ...                        ...   \n",
            "142  20.185892               34                        851   \n",
            "143  16.947971               66                       1361   \n",
            "144  14.703717               11                        226   \n",
            "145  30.266667                1                          9   \n",
            "146  14.613836                7                        106   \n",
            "\n",
            "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
            "0                           40.294118                 249         471   \n",
            "1                           20.000000                   5          16   \n",
            "2                           19.000000                   4          13   \n",
            "3                           21.000000                   5          15   \n",
            "4                           20.000000                   6          16   \n",
            "..                                ...                 ...         ...   \n",
            "142                         25.029412                 210         588   \n",
            "143                         20.621212                 296         900   \n",
            "144                         20.545455                  32         124   \n",
            "145                          9.000000                   6           9   \n",
            "146                         15.142857                  20          66   \n",
            "\n",
            "     SYLLABLE PER WORD  \n",
            "0                 1569  \n",
            "1                   41  \n",
            "2                   35  \n",
            "3                   38  \n",
            "4                   42  \n",
            "..                 ...  \n",
            "142               1635  \n",
            "143               2732  \n",
            "144                397  \n",
            "145                 44  \n",
            "146                203  \n",
            "\n",
            "[147 rows x 19 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the personal pronouns\n",
        "personal_pronouns = [\"I\", \"we\", \"my\", \"ours\", \"us\"]\n",
        "\n",
        "# Function to count personal pronouns\n",
        "def count_personal_pronouns(text):\n",
        "    # Convert text to lowercase for case-insensitive matching\n",
        "    text_lower = text.lower()\n",
        "\n",
        "    # Initialize count\n",
        "    count = 0\n",
        "\n",
        "    # Search for each personal pronoun using regex\n",
        "    for pronoun in personal_pronouns:\n",
        "        # Exclude \"us\" if it's part of the word \"US\"\n",
        "        if pronoun != \"us\":\n",
        "            count += len(re.findall(r'\\b' + pronoun + r'\\b', text_lower))\n",
        "\n",
        "    return count\n",
        "\n",
        "# Apply the function to create the 'Personal Pronouns' column\n",
        "df['PERSONAL PRONOUNS'] = df['Article_text'].apply(count_personal_pronouns)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUjfLCTz-VwD",
        "outputId": "5ff553c9-0042-4a05-f8e1-4941e66505bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         URL_ID                                              Title  \\\n",
            "0    bctech2011  ML and AI-based insurance premium model to pre...   \n",
            "1    bctech2012  Streamlined Integration: Interactive Brokers A...   \n",
            "2    bctech2013  Efficient Data Integration and User-Friendly I...   \n",
            "3    bctech2014  Effective Management of Social Media Data Extr...   \n",
            "4    bctech2015  Streamlined Trading Operations Interface for M...   \n",
            "..          ...                                                ...   \n",
            "142  bctech2153         Population and Community Survey of America   \n",
            "143  bctech2154    Google LSA API Data Automation and Dashboarding   \n",
            "144  bctech2155                           Healthcare Data Analysis   \n",
            "145  bctech2156         Budget, Sales KPI Dashboard using Power BI   \n",
            "146  bctech2157  Amazon Buy Bot, an Automation AI tool to Auto-...   \n",
            "\n",
            "                                          Article_text  \\\n",
            "0    Client: A leading insurance firm worldwide Ind...   \n",
            "1    Client: A leading fintech firm in the USA Indu...   \n",
            "2    Client: A leading tech firm in the USA Industr...   \n",
            "3    Client: A leading tech firm in the USA Industr...   \n",
            "4    Client: A leading fintech firm in the USA Indu...   \n",
            "..                                                 ...   \n",
            "142  Client: A leading marketing firm in the USA In...   \n",
            "143  Client: A leading marketing firm in the USA In...   \n",
            "144  Client: A leading healthcare tech firm in the ...   \n",
            "145   PresentationMapDashboardAPI Integration  Kiba...   \n",
            "146  Client: A leading consulting firm in the USA I...   \n",
            "\n",
            "                                  Article_text_cleaned  POSITIVE SCORE  \\\n",
            "0    client: leading insurance firm worldwide indus...            34.0   \n",
            "1    client: leading fintech firm usa industry type...             1.0   \n",
            "2    client: leading tech firm usa industry type: p...             1.0   \n",
            "3    client: leading tech firm usa industry type: p...             1.0   \n",
            "4    client: leading fintech firm usa industry type...             1.0   \n",
            "..                                                 ...             ...   \n",
            "142  client: leading marketing firm usa industry ty...            13.0   \n",
            "143  client: leading marketing firm usa industry ty...            21.0   \n",
            "144  client: leading healthcare tech firm usa indus...             6.0   \n",
            "145  presentationmapdashboardapi integration kibana...             0.0   \n",
            "146  client: leading consulting firm usa industry t...             2.0   \n",
            "\n",
            "     NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
            "0               8.0        0.619048            0.078212            40.294118   \n",
            "1               0.0        0.999999            0.043478            20.000000   \n",
            "2               0.0        0.999999            0.055556            19.000000   \n",
            "3               0.0        0.999999            0.045455            21.000000   \n",
            "4               0.0        0.999999            0.043478            20.000000   \n",
            "..              ...             ...                 ...                  ...   \n",
            "142             9.0        0.181818            0.034646            25.787879   \n",
            "143            19.0        0.050000            0.040445            20.621212   \n",
            "144             7.0       -0.076923            0.089655            22.600000   \n",
            "145             0.0        0.000000            0.000000             9.000000   \n",
            "146             0.0        1.000000            0.027778            17.666667   \n",
            "\n",
            "     Complex Word Count  Total Word Count  PERCENTAGE OF COMPLEX WORDS  \\\n",
            "0                   249               685                    36.350365   \n",
            "1                     5                20                    25.000000   \n",
            "2                     4                19                    21.052632   \n",
            "3                     5                21                    23.809524   \n",
            "4                     6                20                    30.000000   \n",
            "..                  ...               ...                          ...   \n",
            "142                 210               851                    24.676851   \n",
            "143                 296              1361                    21.748714   \n",
            "144                  32               226                    14.159292   \n",
            "145                   6                 9                    66.666667   \n",
            "146                  20               106                    18.867925   \n",
            "\n",
            "     FOG INDEX  total_sentences  the total number of words  \\\n",
            "0    30.657793               17                        685   \n",
            "1    18.000000                1                         20   \n",
            "2    16.021053                1                         19   \n",
            "3    17.923810                1                         21   \n",
            "4    20.000000                1                         20   \n",
            "..         ...              ...                        ...   \n",
            "142  20.185892               34                        851   \n",
            "143  16.947971               66                       1361   \n",
            "144  14.703717               11                        226   \n",
            "145  30.266667                1                          9   \n",
            "146  14.613836                7                        106   \n",
            "\n",
            "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
            "0                           40.294118                 249         471   \n",
            "1                           20.000000                   5          16   \n",
            "2                           19.000000                   4          13   \n",
            "3                           21.000000                   5          15   \n",
            "4                           20.000000                   6          16   \n",
            "..                                ...                 ...         ...   \n",
            "142                         25.029412                 210         588   \n",
            "143                         20.621212                 296         900   \n",
            "144                         20.545455                  32         124   \n",
            "145                          9.000000                   6           9   \n",
            "146                         15.142857                  20          66   \n",
            "\n",
            "     SYLLABLE PER WORD  PERSONAL PRONOUNS  \n",
            "0                 1569                  1  \n",
            "1                   41                  0  \n",
            "2                   35                  0  \n",
            "3                   38                  0  \n",
            "4                   42                  0  \n",
            "..                 ...                ...  \n",
            "142               1635                  3  \n",
            "143               2732                  5  \n",
            "144                397                  6  \n",
            "145                 44                  0  \n",
            "146                203                  1  \n",
            "\n",
            "[147 rows x 20 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate average word length\n",
        "def calculate_avg_word_length(text):\n",
        "    # Tokenize into words\n",
        "    words = text.split()\n",
        "\n",
        "    # Calculate the total number of characters in all words\n",
        "    total_char_count = sum(len(word) for word in words)\n",
        "\n",
        "    # Calculate the total number of words\n",
        "    total_word_count = len(words)\n",
        "\n",
        "    # Calculate the average word length\n",
        "    avg_word_length = total_char_count / total_word_count\n",
        "\n",
        "    return avg_word_length\n",
        "\n",
        "# Apply the function to create the 'AVG WORD LENGTH' column\n",
        "df['AVG WORD LENGTH'] = df['Article_text'].apply(calculate_avg_word_length)\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpfVkOgY-_tF",
        "outputId": "e9c529b5-d1cf-4edf-8345-2a31376cb024"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         URL_ID                                              Title  \\\n",
            "0    bctech2011  ML and AI-based insurance premium model to pre...   \n",
            "1    bctech2012  Streamlined Integration: Interactive Brokers A...   \n",
            "2    bctech2013  Efficient Data Integration and User-Friendly I...   \n",
            "3    bctech2014  Effective Management of Social Media Data Extr...   \n",
            "4    bctech2015  Streamlined Trading Operations Interface for M...   \n",
            "..          ...                                                ...   \n",
            "142  bctech2153         Population and Community Survey of America   \n",
            "143  bctech2154    Google LSA API Data Automation and Dashboarding   \n",
            "144  bctech2155                           Healthcare Data Analysis   \n",
            "145  bctech2156         Budget, Sales KPI Dashboard using Power BI   \n",
            "146  bctech2157  Amazon Buy Bot, an Automation AI tool to Auto-...   \n",
            "\n",
            "                                          Article_text  \\\n",
            "0    Client: A leading insurance firm worldwide Ind...   \n",
            "1    Client: A leading fintech firm in the USA Indu...   \n",
            "2    Client: A leading tech firm in the USA Industr...   \n",
            "3    Client: A leading tech firm in the USA Industr...   \n",
            "4    Client: A leading fintech firm in the USA Indu...   \n",
            "..                                                 ...   \n",
            "142  Client: A leading marketing firm in the USA In...   \n",
            "143  Client: A leading marketing firm in the USA In...   \n",
            "144  Client: A leading healthcare tech firm in the ...   \n",
            "145   PresentationMapDashboardAPI Integration  Kiba...   \n",
            "146  Client: A leading consulting firm in the USA I...   \n",
            "\n",
            "                                  Article_text_cleaned  POSITIVE SCORE  \\\n",
            "0    client: leading insurance firm worldwide indus...            34.0   \n",
            "1    client: leading fintech firm usa industry type...             1.0   \n",
            "2    client: leading tech firm usa industry type: p...             1.0   \n",
            "3    client: leading tech firm usa industry type: p...             1.0   \n",
            "4    client: leading fintech firm usa industry type...             1.0   \n",
            "..                                                 ...             ...   \n",
            "142  client: leading marketing firm usa industry ty...            13.0   \n",
            "143  client: leading marketing firm usa industry ty...            21.0   \n",
            "144  client: leading healthcare tech firm usa indus...             6.0   \n",
            "145  presentationmapdashboardapi integration kibana...             0.0   \n",
            "146  client: leading consulting firm usa industry t...             2.0   \n",
            "\n",
            "     NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
            "0               8.0        0.619048            0.078212            40.294118   \n",
            "1               0.0        0.999999            0.043478            20.000000   \n",
            "2               0.0        0.999999            0.055556            19.000000   \n",
            "3               0.0        0.999999            0.045455            21.000000   \n",
            "4               0.0        0.999999            0.043478            20.000000   \n",
            "..              ...             ...                 ...                  ...   \n",
            "142             9.0        0.181818            0.034646            25.787879   \n",
            "143            19.0        0.050000            0.040445            20.621212   \n",
            "144             7.0       -0.076923            0.089655            22.600000   \n",
            "145             0.0        0.000000            0.000000             9.000000   \n",
            "146             0.0        1.000000            0.027778            17.666667   \n",
            "\n",
            "     Complex Word Count  ...  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
            "0                   249  ...                    36.350365  30.657793   \n",
            "1                     5  ...                    25.000000  18.000000   \n",
            "2                     4  ...                    21.052632  16.021053   \n",
            "3                     5  ...                    23.809524  17.923810   \n",
            "4                     6  ...                    30.000000  20.000000   \n",
            "..                  ...  ...                          ...        ...   \n",
            "142                 210  ...                    24.676851  20.185892   \n",
            "143                 296  ...                    21.748714  16.947971   \n",
            "144                  32  ...                    14.159292  14.703717   \n",
            "145                   6  ...                    66.666667  30.266667   \n",
            "146                  20  ...                    18.867925  14.613836   \n",
            "\n",
            "     total_sentences  the total number of words  \\\n",
            "0                 17                        685   \n",
            "1                  1                         20   \n",
            "2                  1                         19   \n",
            "3                  1                         21   \n",
            "4                  1                         20   \n",
            "..               ...                        ...   \n",
            "142               34                        851   \n",
            "143               66                       1361   \n",
            "144               11                        226   \n",
            "145                1                          9   \n",
            "146                7                        106   \n",
            "\n",
            "     AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
            "0                           40.294118                 249         471   \n",
            "1                           20.000000                   5          16   \n",
            "2                           19.000000                   4          13   \n",
            "3                           21.000000                   5          15   \n",
            "4                           20.000000                   6          16   \n",
            "..                                ...                 ...         ...   \n",
            "142                         25.029412                 210         588   \n",
            "143                         20.621212                 296         900   \n",
            "144                         20.545455                  32         124   \n",
            "145                          9.000000                   6           9   \n",
            "146                         15.142857                  20          66   \n",
            "\n",
            "     SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
            "0                 1569                  1         6.093431  \n",
            "1                   41                  0         5.900000  \n",
            "2                   35                  0         5.105263  \n",
            "3                   38                  0         5.333333  \n",
            "4                   42                  0         6.050000  \n",
            "..                 ...                ...              ...  \n",
            "142               1635                  3         5.290247  \n",
            "143               2732                  5         5.235856  \n",
            "144                397                  6         4.964602  \n",
            "145                 44                  0        12.444444  \n",
            "146                203                  1         4.952830  \n",
            "\n",
            "[147 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        },
        "id": "ZbJAKQTeAGEl",
        "outputId": "7281b179-9437-450e-b56c-cab55dff8587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       URL_ID                                              Title  \\\n",
              "0  bctech2011  ML and AI-based insurance premium model to pre...   \n",
              "1  bctech2012  Streamlined Integration: Interactive Brokers A...   \n",
              "2  bctech2013  Efficient Data Integration and User-Friendly I...   \n",
              "3  bctech2014  Effective Management of Social Media Data Extr...   \n",
              "4  bctech2015  Streamlined Trading Operations Interface for M...   \n",
              "\n",
              "                                        Article_text  \\\n",
              "0  Client: A leading insurance firm worldwide Ind...   \n",
              "1  Client: A leading fintech firm in the USA Indu...   \n",
              "2  Client: A leading tech firm in the USA Industr...   \n",
              "3  Client: A leading tech firm in the USA Industr...   \n",
              "4  Client: A leading fintech firm in the USA Indu...   \n",
              "\n",
              "                                Article_text_cleaned  POSITIVE SCORE  \\\n",
              "0  client: leading insurance firm worldwide indus...            34.0   \n",
              "1  client: leading fintech firm usa industry type...             1.0   \n",
              "2  client: leading tech firm usa industry type: p...             1.0   \n",
              "3  client: leading tech firm usa industry type: p...             1.0   \n",
              "4  client: leading fintech firm usa industry type...             1.0   \n",
              "\n",
              "   NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  AVG SENTENCE LENGTH  \\\n",
              "0             8.0        0.619048            0.078212            40.294118   \n",
              "1             0.0        0.999999            0.043478            20.000000   \n",
              "2             0.0        0.999999            0.055556            19.000000   \n",
              "3             0.0        0.999999            0.045455            21.000000   \n",
              "4             0.0        0.999999            0.043478            20.000000   \n",
              "\n",
              "   Complex Word Count  ...  PERCENTAGE OF COMPLEX WORDS  FOG INDEX  \\\n",
              "0                 249  ...                    36.350365  30.657793   \n",
              "1                   5  ...                    25.000000  18.000000   \n",
              "2                   4  ...                    21.052632  16.021053   \n",
              "3                   5  ...                    23.809524  17.923810   \n",
              "4                   6  ...                    30.000000  20.000000   \n",
              "\n",
              "   total_sentences  the total number of words  \\\n",
              "0               17                        685   \n",
              "1                1                         20   \n",
              "2                1                         19   \n",
              "3                1                         21   \n",
              "4                1                         20   \n",
              "\n",
              "   AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  WORD COUNT  \\\n",
              "0                         40.294118                 249         471   \n",
              "1                         20.000000                   5          16   \n",
              "2                         19.000000                   4          13   \n",
              "3                         21.000000                   5          15   \n",
              "4                         20.000000                   6          16   \n",
              "\n",
              "   SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
              "0               1569                  1         6.093431  \n",
              "1                 41                  0         5.900000  \n",
              "2                 35                  0         5.105263  \n",
              "3                 38                  0         5.333333  \n",
              "4                 42                  0         6.050000  \n",
              "\n",
              "[5 rows x 21 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-eda76ffe-a925-4df2-ba61-623879fab120\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL_ID</th>\n",
              "      <th>Title</th>\n",
              "      <th>Article_text</th>\n",
              "      <th>Article_text_cleaned</th>\n",
              "      <th>POSITIVE SCORE</th>\n",
              "      <th>NEGATIVE SCORE</th>\n",
              "      <th>POLARITY SCORE</th>\n",
              "      <th>SUBJECTIVITY SCORE</th>\n",
              "      <th>AVG SENTENCE LENGTH</th>\n",
              "      <th>Complex Word Count</th>\n",
              "      <th>...</th>\n",
              "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
              "      <th>FOG INDEX</th>\n",
              "      <th>total_sentences</th>\n",
              "      <th>the total number of words</th>\n",
              "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
              "      <th>COMPLEX WORD COUNT</th>\n",
              "      <th>WORD COUNT</th>\n",
              "      <th>SYLLABLE PER WORD</th>\n",
              "      <th>PERSONAL PRONOUNS</th>\n",
              "      <th>AVG WORD LENGTH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bctech2011</td>\n",
              "      <td>ML and AI-based insurance premium model to pre...</td>\n",
              "      <td>Client: A leading insurance firm worldwide Ind...</td>\n",
              "      <td>client: leading insurance firm worldwide indus...</td>\n",
              "      <td>34.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.619048</td>\n",
              "      <td>0.078212</td>\n",
              "      <td>40.294118</td>\n",
              "      <td>249</td>\n",
              "      <td>...</td>\n",
              "      <td>36.350365</td>\n",
              "      <td>30.657793</td>\n",
              "      <td>17</td>\n",
              "      <td>685</td>\n",
              "      <td>40.294118</td>\n",
              "      <td>249</td>\n",
              "      <td>471</td>\n",
              "      <td>1569</td>\n",
              "      <td>1</td>\n",
              "      <td>6.093431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bctech2012</td>\n",
              "      <td>Streamlined Integration: Interactive Brokers A...</td>\n",
              "      <td>Client: A leading fintech firm in the USA Indu...</td>\n",
              "      <td>client: leading fintech firm usa industry type...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>5.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bctech2013</td>\n",
              "      <td>Efficient Data Integration and User-Friendly I...</td>\n",
              "      <td>Client: A leading tech firm in the USA Industr...</td>\n",
              "      <td>client: leading tech firm usa industry type: p...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>...</td>\n",
              "      <td>21.052632</td>\n",
              "      <td>16.021053</td>\n",
              "      <td>1</td>\n",
              "      <td>19</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>5.105263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bctech2014</td>\n",
              "      <td>Effective Management of Social Media Data Extr...</td>\n",
              "      <td>Client: A leading tech firm in the USA Industr...</td>\n",
              "      <td>client: leading tech firm usa industry type: p...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>...</td>\n",
              "      <td>23.809524</td>\n",
              "      <td>17.923810</td>\n",
              "      <td>1</td>\n",
              "      <td>21</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>15</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>5.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bctech2015</td>\n",
              "      <td>Streamlined Trading Operations Interface for M...</td>\n",
              "      <td>Client: A leading fintech firm in the USA Indu...</td>\n",
              "      <td>client: leading fintech firm usa industry type...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>...</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>20</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>6.050000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 21 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-eda76ffe-a925-4df2-ba61-623879fab120')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-eda76ffe-a925-4df2-ba61-623879fab120 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-eda76ffe-a925-4df2-ba61-623879fab120');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d8c2f585-060f-4b46-b5c3-39feb1f3d9c6\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d8c2f585-060f-4b46-b5c3-39feb1f3d9c6')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d8c2f585-060f-4b46-b5c3-39feb1f3d9c6 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuUUBDt8H9IN",
        "outputId": "280e0024-3ff0-4d94-f2ec-1ebfe68bcd66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 147 entries, 0 to 146\n",
            "Data columns (total 21 columns):\n",
            " #   Column                            Non-Null Count  Dtype  \n",
            "---  ------                            --------------  -----  \n",
            " 0   URL_ID                            147 non-null    object \n",
            " 1   Title                             147 non-null    object \n",
            " 2   Article_text                      147 non-null    object \n",
            " 3   Article_text_cleaned              147 non-null    object \n",
            " 4   POSITIVE SCORE                    147 non-null    float64\n",
            " 5   NEGATIVE SCORE                    147 non-null    float64\n",
            " 6   POLARITY SCORE                    147 non-null    float64\n",
            " 7   SUBJECTIVITY SCORE                147 non-null    float64\n",
            " 8   AVG SENTENCE LENGTH               147 non-null    float64\n",
            " 9   Complex Word Count                147 non-null    int64  \n",
            " 10  Total Word Count                  147 non-null    int64  \n",
            " 11  PERCENTAGE OF COMPLEX WORDS       147 non-null    float64\n",
            " 12  FOG INDEX                         147 non-null    float64\n",
            " 13  total_sentences                   147 non-null    int64  \n",
            " 14  the total number of words         147 non-null    int64  \n",
            " 15  AVG NUMBER OF WORDS PER SENTENCE  147 non-null    float64\n",
            " 16  COMPLEX WORD COUNT                147 non-null    int64  \n",
            " 17  WORD COUNT                        147 non-null    int64  \n",
            " 18  SYLLABLE PER WORD                 147 non-null    int64  \n",
            " 19  PERSONAL PRONOUNS                 147 non-null    int64  \n",
            " 20  AVG WORD LENGTH                   147 non-null    float64\n",
            "dtypes: float64(9), int64(8), object(4)\n",
            "memory usage: 24.2+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the Excel file\n",
        "excel_df = pd.read_excel('/content/drive/MyDrive/Blackcoffer/Output Data Structure.xlsx')  # Replace with your actual file path\n",
        "\n",
        "# Rename the existing 'URL' column in df (if it exists)\n",
        "if 'URL' in df.columns:\n",
        "    df = df.rename(columns={'URL': 'Existing_URL'})  # Or any other suitable name\n",
        "\n",
        "# Merge the DataFrames based on the 'URL_ID' column\n",
        "df = pd.merge(df, excel_df[['URL_ID', 'URL']], on='URL_ID', how='left')\n",
        "\n",
        "# Print the updated DataFrame (first few rows)\n",
        "print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0S79XgghICMq",
        "outputId": "2d8d7a2c-7220-457f-a438-b111f333d7bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| URL_ID     | Title                                                                                                                   | Article_text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | Article_text_cleaned                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | POSITIVE SCORE   | NEGATIVE SCORE   | POLARITY SCORE   | SUBJECTIVITY SCORE   | AVG SENTENCE LENGTH   | Complex Word Count   | Total Word Count   | PERCENTAGE OF COMPLEX WORDS   | FOG INDEX   | total_sentences   | the total number of words   | AVG NUMBER OF WORDS PER SENTENCE   | COMPLEX WORD COUNT   | WORD COUNT   | SYLLABLE PER WORD   | PERSONAL PRONOUNS   | AVG WORD LENGTH   | URL                                                                                                                                                      |\n",
            "|:-----------|:------------------------------------------------------------------------------------------------------------------------|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------|:-----------------|:-----------------|:---------------------|:----------------------|:---------------------|:-------------------|:------------------------------|:------------|:------------------|:----------------------------|:-----------------------------------|:---------------------|:-------------|:--------------------|:--------------------|:------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
            "| bctech2011 | ML and AI-based insurance premium model to predict premium to be charged by the insurance company                       | Client: A leading insurance firm worldwide Industry Type: BFSI Products & Services: Insurance Organization Size: 10000+ The insurance industry, particularly in the context of providing coverage to Public Company Directors against Insider Trading public lawsuits, faces a significant challenge in accurately determining insurance premiums. Traditional methods of premium calculation may lack precision, and there is a growing need for more sophisticated and data-driven approaches. The integration of Artificial Intelligence (AI) and Machine Learning (ML) models in predicting insurance premiums for this specialized coverage is essential to enhance accuracy, fairness, and responsiveness in adapting to evolving risk factors. The problem at hand involves developing robust AI and ML models that can effectively analyze a multitude of dynamic variables influencing the risk profile of Public Company Directors. These variables include market conditions, regulatory changes, historical legal precedents, financial performance of the insured company, and individual directorial behaviors. The goal is to create a predictive model that not only accurately assesses the risk associated with potential insider trading public lawsuits but also adapts in real-time to new information, ensuring that the insurance premiums charged by the global insurance firm are reflective of the current risk landscape. Key Challenges: Addressing these challenges will not only improve the accuracy of insurance premium predictions but also contribute to the overall efficiency and effectiveness of the insurance services provided to Public Company Directors by the leading global insurance firm. To develop an ML and AI-based insurance premium prediction model for Public Company Directors in the USA, safeguarding them against insider trading public lawsuits, we propose a comprehensive solution leveraging advanced machine learning techniques. The goal is to create a model that accurately assesses the risk associated with individual directors and adapts to dynamic market conditions. By adopting this ML and AI-based approach, the insurance company can enhance its ability to predict insurance premiums accurately, adapt to changing risk landscapes, and provide tailored coverage for Public Company Directors against insider trading public lawsuits in the dynamic environment of the USA.   Building an ML and AI-based insurance premium prediction model involves the use of various tools and technologies across different stages of development. Here’s a list of tools and technologies that can be employed for creating such a model for a leading insurance firm in the USA, specifically targeting Public Company Directors against insider trading public lawsuits: It’s important to note that the choice of specific tools may vary based on the preferences of the data science team, the complexity of the model, and the existing technology stack of the insurance company. Additionally, compliance with regulatory requirements and industry standards should be considered in the selection of tools and technologies. The deliverables for an ML and AI-based insurance premium model for Public Company Directors in the USA, aiming to predict premiums for protection against insider trading public lawsuits, would encompass various stages of the development and deployment process. Here is a comprehensive list of deliverables: 1.1 Project Proposal: 1.2 Requirements Document: 2.1 Data Collection Report: 2.2 Cleaned and Preprocessed Dataset: 3.1 Feature Selection and Engineering Report: 4.1 Trained ML Models: 4.2 Model Evaluation Report: 5.1 Real-Time Integration Component: 5.2 Scenario Analysis Module: 6.1 Fairness Assessment Report: 6.2 Explainability Module: 7.1 Deployed API: 7.2 User Interface (UI): 7.3 Documentation for Integration: 8.1 Monitoring Dashboard: 8.2 Automated Model Update Pipeline: 9.1 Model Architecture Document: 9.2 Technical User Manual: 10.1 Training Sessions: 10.2 Knowledge Transfer Documentation: 11.1 Regulatory Compliance Report: 11.2 Data Privacy and Security Documentation: 12.1 Support and Maintenance Plan: By delivering these items, the insurance firm can ensure a thorough and transparent development process, facilitating successful integration and utilization of the ML and AI-based insurance premium prediction model. The implementation of an ML and AI-based insurance premium model for Public Company Directors in the USA, specifically tailored to protect them from insider trading public lawsuits, can have significant business impacts for the leading insurance firm. Here are several potential business impacts: By recognizing and leveraging these business impacts, the leading insurance firm can derive significant value from the implementation of an ML and AI-based insurance premium model tailored for Public Company Directors in the USA. | client: leading insurance firm worldwide industry type: bfsi products & services: insurance organization size: 10000+ insurance industry, context providing coverage public company directors insider trading public lawsuits, faces significant challenge accurately determining insurance premiums. traditional methods premium calculation lack precision, growing sophisticated data-driven approaches. integration artificial intelligence (ai) machine learning (ml) models predicting insurance premiums specialized coverage essential enhance accuracy, fairness, responsiveness adapting evolving risk factors. problem hand involves developing robust ai ml models effectively analyze multitude dynamic variables influencing risk profile public company directors. variables include market conditions, regulatory changes, historical legal precedents, financial performance insured company, individual directorial behaviors. goal create predictive model accurately assesses risk potential insider trading public lawsuits adapts real-time information, ensuring insurance premiums charged global insurance firm reflective current risk landscape. key challenges: addressing challenges improve accuracy insurance premium predictions contribute efficiency effectiveness insurance services provided public company directors leading global insurance firm. develop ml ai-based insurance premium prediction model public company directors usa, safeguarding insider trading public lawsuits, propose comprehensive solution leveraging advanced machine learning techniques. goal create model accurately assesses risk individual directors adapts dynamic market conditions. adopting ml ai-based approach, insurance company enhance ability predict insurance premiums accurately, adapt changing risk landscapes, provide tailored coverage public company directors insider trading public lawsuits dynamic environment usa. building ml ai-based insurance premium prediction model involves tools technologies stages development. here’s list tools technologies employed creating model leading insurance firm usa, specifically targeting public company directors insider trading public lawsuits: it’s important note choice specific tools vary based preferences data science team, complexity model, existing technology stack insurance company. additionally, compliance regulatory requirements industry standards considered selection tools technologies. deliverables ml ai-based insurance premium model public company directors usa, aiming predict premiums protection insider trading public lawsuits, encompass stages development deployment process. comprehensive list deliverables: 1.1 project proposal: 1.2 requirements document: 2.1 data collection report: 2.2 cleaned preprocessed dataset: 3.1 feature selection engineering report: 4.1 trained ml models: 4.2 model evaluation report: 5.1 real-time integration component: 5.2 scenario analysis module: 6.1 fairness assessment report: 6.2 explainability module: 7.1 deployed api: 7.2 user interface (ui): 7.3 documentation integration: 8.1 monitoring dashboard: 8.2 automated model update pipeline: 9.1 model architecture document: 9.2 technical user manual: 10.1 training sessions: 10.2 knowledge transfer documentation: 11.1 regulatory compliance report: 11.2 data privacy security documentation: 12.1 support maintenance plan: delivering items, insurance firm ensure transparent development process, facilitating successful integration utilization ml ai-based insurance premium prediction model. implementation ml ai-based insurance premium model public company directors usa, specifically tailored protect insider trading public lawsuits, significant business impacts leading insurance firm. potential business impacts: recognizing leveraging business impacts, leading insurance firm derive significant implementation ml ai-based insurance premium model tailored public company directors usa. | 34               | 8                | 0.619048         | 0.0782123            | 40.2941               | 249                  | 685                | 36.3504                       | 30.6578     | 17                | 685                         | 40.2941                            | 249                  | 471          | 1569                | 1                   | 6.09343           | https://insights.blackcoffer.com/ml-and-ai-based-insurance-premium-model-to-predict-premium-to-be-charged-by-the-insurance-company/                      |\n",
            "| bctech2012 | Streamlined Integration: Interactive Brokers API with Python for Desktop Trading Application                            | Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Banking, Financing Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    | client: leading fintech firm usa industry type: finance products & services: trading, banking, financing organization size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     | 1                | 0                | 0.999999         | 0.0434783            | 20                    | 5                    | 20                 | 25                            | 18          | 1                 | 20                          | 20                                 | 5                    | 16           | 41                  | 0                   | 5.9               | https://insights.blackcoffer.com/streamlined-integration-interactive-brokers-api-with-python-for-desktop-trading-application/                            |\n",
            "| bctech2013 | Efficient Data Integration and User-Friendly Interface Development: Navigating Challenges in Web Application Deployment | Client: A leading tech firm in the USA Industry Type: IT Products & Services: IT Consulting Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          | client: leading tech firm usa industry type: products & services: consulting organization size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | 1                | 0                | 0.999999         | 0.0555556            | 19                    | 4                    | 19                 | 21.0526                       | 16.0211     | 1                 | 19                          | 19                                 | 4                    | 13           | 35                  | 0                   | 5.10526           | https://insights.blackcoffer.com/efficient-data-integration-and-user-friendly-interface-development-navigating-challenges-in-web-application-deployment/ |\n",
            "| bctech2014 | Effective Management of Social Media Data Extraction: Strategies for Authentication, Security, and Reliability          | Client: A leading tech firm in the USA Industry Type: IT Products & Services: Consulting, Product & Services Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         | client: leading tech firm usa industry type: products & services: consulting, product & services organization size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             | 1                | 0                | 0.999999         | 0.0454545            | 21                    | 5                    | 21                 | 23.8095                       | 17.9238     | 1                 | 21                          | 21                                 | 5                    | 15           | 38                  | 0                   | 5.33333           | https://insights.blackcoffer.com/effective-management-of-social-media-data-extraction-strategies-for-authentication-security-and-reliability/            |\n",
            "| bctech2015 | Streamlined Trading Operations Interface for MetaTrader 4: Empowering Efficient Management and Monitoring               | Client: A leading fintech firm in the USA Industry Type: Finance Products & Services: Trading, Investment, Financing Organization Size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 | client: leading fintech firm usa industry type: finance products & services: trading, investment, financing organization size: 100+                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | 1                | 0                | 0.999999         | 0.0434783            | 20                    | 6                    | 20                 | 30                            | 20          | 1                 | 20                          | 20                                 | 6                    | 16           | 42                  | 0                   | 6.05              | https://insights.blackcoffer.com/streamlined-trading-operations-interface-for-metatrader-4-empowering-efficient-management-and-monitoring/               |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['Title','Article_text','total_sentences','the total number of words'],inplace=True)"
      ],
      "metadata": {
        "id": "-YXkD5tlI6S5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop(columns=['Article_text_cleaned','Total Word Count'],inplace=True)"
      ],
      "metadata": {
        "id": "8P-N1fRIMawG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the desired column order\n",
        "new_column_order = ['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE', 'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH', 'COMPLEX WORD COUNT', 'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX', 'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT', 'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH']\n",
        "\n",
        "# Reorder the columns\n",
        "df = df[new_column_order]\n",
        "\n",
        "# Print the updated DataFrame (first few rows)\n",
        "print(df.head().to_markdown(index=False, numalign=\"left\", stralign=\"left\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBIyidsqM6Sl",
        "outputId": "fff2360b-84c3-49c3-b4ad-3543bfaf564d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| URL_ID     | URL                                                                                                                                                      | POSITIVE SCORE   | NEGATIVE SCORE   | POLARITY SCORE   | SUBJECTIVITY SCORE   | AVG SENTENCE LENGTH   | COMPLEX WORD COUNT   | PERCENTAGE OF COMPLEX WORDS   | FOG INDEX   | AVG NUMBER OF WORDS PER SENTENCE   | COMPLEX WORD COUNT   | WORD COUNT   | SYLLABLE PER WORD   | PERSONAL PRONOUNS   | AVG WORD LENGTH   |\n",
            "|:-----------|:---------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------|:-----------------|:-----------------|:---------------------|:----------------------|:---------------------|:------------------------------|:------------|:-----------------------------------|:---------------------|:-------------|:--------------------|:--------------------|:------------------|\n",
            "| bctech2011 | https://insights.blackcoffer.com/ml-and-ai-based-insurance-premium-model-to-predict-premium-to-be-charged-by-the-insurance-company/                      | 34               | 8                | 0.619048         | 0.0782123            | 40.2941               | 249                  | 36.3504                       | 30.6578     | 40.2941                            | 249                  | 471          | 1569                | 1                   | 6.09343           |\n",
            "| bctech2012 | https://insights.blackcoffer.com/streamlined-integration-interactive-brokers-api-with-python-for-desktop-trading-application/                            | 1                | 0                | 0.999999         | 0.0434783            | 20                    | 5                    | 25                            | 18          | 20                                 | 5                    | 16           | 41                  | 0                   | 5.9               |\n",
            "| bctech2013 | https://insights.blackcoffer.com/efficient-data-integration-and-user-friendly-interface-development-navigating-challenges-in-web-application-deployment/ | 1                | 0                | 0.999999         | 0.0555556            | 19                    | 4                    | 21.0526                       | 16.0211     | 19                                 | 4                    | 13           | 35                  | 0                   | 5.10526           |\n",
            "| bctech2014 | https://insights.blackcoffer.com/effective-management-of-social-media-data-extraction-strategies-for-authentication-security-and-reliability/            | 1                | 0                | 0.999999         | 0.0454545            | 21                    | 5                    | 23.8095                       | 17.9238     | 21                                 | 5                    | 15           | 38                  | 0                   | 5.33333           |\n",
            "| bctech2015 | https://insights.blackcoffer.com/streamlined-trading-operations-interface-for-metatrader-4-empowering-efficient-management-and-monitoring/               | 1                | 0                | 0.999999         | 0.0434783            | 20                    | 6                    | 30                            | 20          | 20                                 | 6                    | 16           | 42                  | 0                   | 6.05              |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "z2t3SxhSN5nq",
        "outputId": "9ffb50f0-5dba-4622-a317-67aeed0b3583"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       URL_ID                                                URL  \\\n",
              "0  bctech2011  https://insights.blackcoffer.com/ml-and-ai-bas...   \n",
              "1  bctech2012  https://insights.blackcoffer.com/streamlined-i...   \n",
              "2  bctech2013  https://insights.blackcoffer.com/efficient-dat...   \n",
              "3  bctech2014  https://insights.blackcoffer.com/effective-man...   \n",
              "4  bctech2015  https://insights.blackcoffer.com/streamlined-t...   \n",
              "\n",
              "   POSITIVE SCORE  NEGATIVE SCORE  POLARITY SCORE  SUBJECTIVITY SCORE  \\\n",
              "0            34.0             8.0        0.619048            0.078212   \n",
              "1             1.0             0.0        0.999999            0.043478   \n",
              "2             1.0             0.0        0.999999            0.055556   \n",
              "3             1.0             0.0        0.999999            0.045455   \n",
              "4             1.0             0.0        0.999999            0.043478   \n",
              "\n",
              "   AVG SENTENCE LENGTH  COMPLEX WORD COUNT  PERCENTAGE OF COMPLEX WORDS  \\\n",
              "0            40.294118                 249                    36.350365   \n",
              "1            20.000000                   5                    25.000000   \n",
              "2            19.000000                   4                    21.052632   \n",
              "3            21.000000                   5                    23.809524   \n",
              "4            20.000000                   6                    30.000000   \n",
              "\n",
              "   FOG INDEX  AVG NUMBER OF WORDS PER SENTENCE  COMPLEX WORD COUNT  \\\n",
              "0  30.657793                         40.294118                 249   \n",
              "1  18.000000                         20.000000                   5   \n",
              "2  16.021053                         19.000000                   4   \n",
              "3  17.923810                         21.000000                   5   \n",
              "4  20.000000                         20.000000                   6   \n",
              "\n",
              "   WORD COUNT  SYLLABLE PER WORD  PERSONAL PRONOUNS  AVG WORD LENGTH  \n",
              "0         471               1569                  1         6.093431  \n",
              "1          16                 41                  0         5.900000  \n",
              "2          13                 35                  0         5.105263  \n",
              "3          15                 38                  0         5.333333  \n",
              "4          16                 42                  0         6.050000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-204c92f3-0588-48d4-bd7b-754fcfca8ef1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>URL_ID</th>\n",
              "      <th>URL</th>\n",
              "      <th>POSITIVE SCORE</th>\n",
              "      <th>NEGATIVE SCORE</th>\n",
              "      <th>POLARITY SCORE</th>\n",
              "      <th>SUBJECTIVITY SCORE</th>\n",
              "      <th>AVG SENTENCE LENGTH</th>\n",
              "      <th>COMPLEX WORD COUNT</th>\n",
              "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
              "      <th>FOG INDEX</th>\n",
              "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
              "      <th>COMPLEX WORD COUNT</th>\n",
              "      <th>WORD COUNT</th>\n",
              "      <th>SYLLABLE PER WORD</th>\n",
              "      <th>PERSONAL PRONOUNS</th>\n",
              "      <th>AVG WORD LENGTH</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bctech2011</td>\n",
              "      <td>https://insights.blackcoffer.com/ml-and-ai-bas...</td>\n",
              "      <td>34.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.619048</td>\n",
              "      <td>0.078212</td>\n",
              "      <td>40.294118</td>\n",
              "      <td>249</td>\n",
              "      <td>36.350365</td>\n",
              "      <td>30.657793</td>\n",
              "      <td>40.294118</td>\n",
              "      <td>249</td>\n",
              "      <td>471</td>\n",
              "      <td>1569</td>\n",
              "      <td>1</td>\n",
              "      <td>6.093431</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>bctech2012</td>\n",
              "      <td>https://insights.blackcoffer.com/streamlined-i...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>25.000000</td>\n",
              "      <td>18.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>16</td>\n",
              "      <td>41</td>\n",
              "      <td>0</td>\n",
              "      <td>5.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>bctech2013</td>\n",
              "      <td>https://insights.blackcoffer.com/efficient-dat...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.055556</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>21.052632</td>\n",
              "      <td>16.021053</td>\n",
              "      <td>19.000000</td>\n",
              "      <td>4</td>\n",
              "      <td>13</td>\n",
              "      <td>35</td>\n",
              "      <td>0</td>\n",
              "      <td>5.105263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>bctech2014</td>\n",
              "      <td>https://insights.blackcoffer.com/effective-man...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.045455</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>23.809524</td>\n",
              "      <td>17.923810</td>\n",
              "      <td>21.000000</td>\n",
              "      <td>5</td>\n",
              "      <td>15</td>\n",
              "      <td>38</td>\n",
              "      <td>0</td>\n",
              "      <td>5.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bctech2015</td>\n",
              "      <td>https://insights.blackcoffer.com/streamlined-t...</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.999999</td>\n",
              "      <td>0.043478</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>20.000000</td>\n",
              "      <td>6</td>\n",
              "      <td>16</td>\n",
              "      <td>42</td>\n",
              "      <td>0</td>\n",
              "      <td>6.050000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-204c92f3-0588-48d4-bd7b-754fcfca8ef1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-204c92f3-0588-48d4-bd7b-754fcfca8ef1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-204c92f3-0588-48d4-bd7b-754fcfca8ef1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-3bd8f865-5625-41cf-91a0-74864f8d2149\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-3bd8f865-5625-41cf-91a0-74864f8d2149')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-3bd8f865-5625-41cf-91a0-74864f8d2149 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 147,\n  \"fields\": [\n    {\n      \"column\": \"URL_ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 147,\n        \"samples\": [\n          \"bctech2136\",\n          \"bctech2062\",\n          \"bctech2149\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"URL\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 147,\n        \"samples\": [\n          \"https://insights.blackcoffer.com/lipsync-automation-for-celebrities-and-influencers/\",\n          \"https://insights.blackcoffer.com/an-etl-solution-for-an-internet-publishing-firm/\",\n          \"https://insights.blackcoffer.com/data-etl-local-service-ads-leads-to-bigquery/\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"POSITIVE SCORE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 6.731342230410336,\n        \"min\": 0.0,\n        \"max\": 34.0,\n        \"num_unique_values\": 25,\n        \"samples\": [\n          23.0,\n          5.0,\n          34.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"NEGATIVE SCORE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.309043923778849,\n        \"min\": 0.0,\n        \"max\": 19.0,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          8.0,\n          0.0,\n          2.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"POLARITY SCORE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.4239094855159765,\n        \"min\": -0.8461537810650938,\n        \"max\": 0.9999999473684238,\n        \"num_unique_values\": 71,\n        \"samples\": [\n          0.4285713673469475,\n          0.6190476043083905,\n          -0.4285713673469475\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SUBJECTIVITY SCORE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.023670674768296388,\n        \"min\": 0.0,\n        \"max\": 0.11842105185249308,\n        \"num_unique_values\": 141,\n        \"samples\": [\n          0.03550295836980498,\n          0.017857142538265314,\n          0.0256410255095332\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AVG SENTENCE LENGTH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 22.97655204396729,\n        \"min\": 9.0,\n        \"max\": 180.0,\n        \"num_unique_values\": 135,\n        \"samples\": [\n          20.833333333333332,\n          23.142857142857142,\n          17.333333333333332\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"COMPLEX WORD COUNT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 52,\n        \"min\": 4,\n        \"max\": 296,\n        \"num_unique_values\": 90,\n        \"samples\": [\n          22,\n          156,\n          214\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PERCENTAGE OF COMPLEX WORDS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 7.120585558046633,\n        \"min\": 12.455516014234876,\n        \"max\": 66.66666666666666,\n        \"num_unique_values\": 141,\n        \"samples\": [\n          23.131672597864767,\n          17.712177121771216,\n          17.412935323383085\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FOG INDEX\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.860745517525075,\n        \"min\": 11.226650850138396,\n        \"max\": 80.22222222222223,\n        \"num_unique_values\": 146,\n        \"samples\": [\n          19.98436123348018,\n          21.803921568627455,\n          15.183752417794972\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AVG NUMBER OF WORDS PER SENTENCE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11.848990804804966,\n        \"min\": 8.857142857142858,\n        \"max\": 90.0,\n        \"num_unique_values\": 128,\n        \"samples\": [\n          22.8,\n          16.857142857142858,\n          20.615384615384617\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"COMPLEX WORD COUNT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 52,\n        \"min\": 4,\n        \"max\": 296,\n        \"num_unique_values\": 90,\n        \"samples\": [\n          22,\n          156,\n          214\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"WORD COUNT\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 130,\n        \"min\": 9,\n        \"max\": 900,\n        \"num_unique_values\": 117,\n        \"samples\": [\n          192,\n          19,\n          81\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"SYLLABLE PER WORD\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 416,\n        \"min\": 35,\n        \"max\": 2732,\n        \"num_unique_values\": 140,\n        \"samples\": [\n          874,\n          828,\n          290\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"PERSONAL PRONOUNS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3,\n        \"min\": 0,\n        \"max\": 16,\n        \"num_unique_values\": 16,\n        \"samples\": [\n          1,\n          0,\n          8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AVG WORD LENGTH\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.92035954997072,\n        \"min\": 4.629032258064516,\n        \"max\": 26.791666666666668,\n        \"num_unique_values\": 146,\n        \"samples\": [\n          5.463276836158192,\n          5.535294117647059,\n          5.467948717948718\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJv8wCpJOSq3",
        "outputId": "d3be5712-b0b5-4277-d597-e45547147b67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 147 entries, 0 to 146\n",
            "Data columns (total 16 columns):\n",
            " #   Column                            Non-Null Count  Dtype  \n",
            "---  ------                            --------------  -----  \n",
            " 0   URL_ID                            147 non-null    object \n",
            " 1   URL                               147 non-null    object \n",
            " 2   POSITIVE SCORE                    147 non-null    float64\n",
            " 3   NEGATIVE SCORE                    147 non-null    float64\n",
            " 4   POLARITY SCORE                    147 non-null    float64\n",
            " 5   SUBJECTIVITY SCORE                147 non-null    float64\n",
            " 6   AVG SENTENCE LENGTH               147 non-null    float64\n",
            " 7   COMPLEX WORD COUNT                147 non-null    int64  \n",
            " 8   PERCENTAGE OF COMPLEX WORDS       147 non-null    float64\n",
            " 9   FOG INDEX                         147 non-null    float64\n",
            " 10  AVG NUMBER OF WORDS PER SENTENCE  147 non-null    float64\n",
            " 11  COMPLEX WORD COUNT                147 non-null    int64  \n",
            " 12  WORD COUNT                        147 non-null    int64  \n",
            " 13  SYLLABLE PER WORD                 147 non-null    int64  \n",
            " 14  PERSONAL PRONOUNS                 147 non-null    int64  \n",
            " 15  AVG WORD LENGTH                   147 non-null    float64\n",
            "dtypes: float64(9), int64(5), object(2)\n",
            "memory usage: 18.5+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify the output file path\n",
        "output_file_path = '/content/drive/MyDrive/Blackcoffer/Output Data Structure.csv'\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"DataFrame saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocnVbhCEOTsC",
        "outputId": "4e71fab6-2eee-4bc7-cb12-6e36c1371620"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame saved to /content/drive/MyDrive/Blackcoffer/Output Data Structure.csv\n"
          ]
        }
      ]
    }
  ]
}